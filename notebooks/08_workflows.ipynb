{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "**Orchestrating Complex Multi-Agent Systems**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this comprehensive tutorial on **workflows** in the Strands framework! This notebook demonstrates how to orchestrate complex multi-agent systems, build intelligent pipelines, and create sophisticated AI applications. By the end of this 10-minute tutorial, you'll master the art of agent orchestration.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Build multi-agent workflows\n",
    "- Implement sequential and parallel pipelines\n",
    "- Create conditional workflows\n",
    "- Handle state management between agents\n",
    "- Build complex orchestration patterns\n",
    "- Implement error handling and retries\n",
    "\n",
    "### üîÑ Why Workflows Matter\n",
    "\n",
    "Workflows enable you to:\n",
    "- **Divide complex tasks** into manageable steps\n",
    "- **Leverage specialized agents** for specific roles\n",
    "- **Build scalable systems** with modular components\n",
    "- **Implement business logic** through orchestration\n",
    "- **Create maintainable** AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Installing Required Packages\n",
    "\n",
    "### Overview\n",
    "Let's install the necessary packages for building workflows.\n",
    "\n",
    "### üìö Packages We'll Install\n",
    "- **strands-agents**: Core framework with workflow support\n",
    "- **strands-agents-tools**: Additional workflow tools\n",
    "- **asyncio**: For async workflow execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install strands-agents strands-agents-tools strands-agents-builder -q\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"   Ready to build workflows! üîÑ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Step 2: Setting Up AWS Authentication\n",
    "\n",
    "### Overview\n",
    "We'll configure AWS Bedrock for our workflow agents.\n",
    "\n",
    "### üîë Authentication Options\n",
    "1. **AWS Profile** (Recommended for development)\n",
    "2. **Environment Variables**\n",
    "3. **Direct Credentials** (Less secure)\n",
    "4. **IAM Roles** (Recommended for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# Configure AWS session\n",
    "session = boto3.Session(\n",
    "    # aws_access_key_id='your_access_key',\n",
    "    # aws_secret_access_key='your_secret_key',\n",
    "    # aws_session_token='your_session_token',  # If using temporary credentials\n",
    "    # region_name='us-west-2',\n",
    "    profile_name='default'  # Optional: Use a specific AWS profile\n",
    ")\n",
    "\n",
    "# Create a Bedrock model instance\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    boto_session=session\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AWS Bedrock configured successfully!\")\n",
    "print(f\"   Model: Claude 3.7 Sonnet\")\n",
    "print(f\"   Profile: {session.profile_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 3: Building Basic Sequential Workflows\n",
    "\n",
    "### Sequential Processing\n",
    "Let's start with simple sequential workflows where agents process tasks one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow state\n",
    "@dataclass\n",
    "class WorkflowState:\n",
    "    \"\"\"Shared state between workflow steps\"\"\"\n",
    "    input_text: str\n",
    "    processed_text: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    analysis: Optional[Dict[str, Any]] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "# Create specialized agents\n",
    "text_processor = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a text processing specialist.\n",
    "    Clean and format text for clarity and readability.\n",
    "    Fix grammar and spelling errors while preserving meaning.\"\"\"\n",
    ")\n",
    "\n",
    "summarizer = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a summarization expert.\n",
    "    Create concise, accurate summaries that capture key points.\n",
    "    Use bullet points for clarity.\"\"\"\n",
    ")\n",
    "\n",
    "analyzer = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a text analysis expert.\n",
    "    Analyze sentiment, tone, and key themes.\n",
    "    Provide structured insights.\"\"\"\n",
    ")\n",
    "\n",
    "# Sequential workflow\n",
    "class SequentialWorkflow:\n",
    "    \"\"\"Sequential text processing workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: Agent, summarizer: Agent, analyzer: Agent):\n",
    "        self.processor = processor\n",
    "        self.summarizer = summarizer\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def run(self, text: str) -> WorkflowState:\n",
    "        \"\"\"Execute sequential workflow\"\"\"\n",
    "        print(\"üîÑ Starting Sequential Workflow...\")\n",
    "        state = WorkflowState(input_text=text)\n",
    "        \n",
    "        # Step 1: Process text\n",
    "        print(\"\\nüìù Step 1: Processing text...\")\n",
    "        state.processed_text = str(self.processor(f\"Process this text: {text}\"))\n",
    "        state.metadata['processing_complete'] = True\n",
    "        \n",
    "        # Step 2: Summarize\n",
    "        print(\"\\nüìã Step 2: Summarizing...\")\n",
    "        state.summary = str(self.summarizer(f\"Summarize: {state.processed_text}\"))\n",
    "        state.metadata['summary_complete'] = True\n",
    "        \n",
    "        # Step 3: Analyze\n",
    "        print(\"\\nüîç Step 3: Analyzing...\")\n",
    "        analysis_response = self.analyzer(\n",
    "            f\"Analyze this text for sentiment, tone, and themes: {state.processed_text}\"\n",
    "        )\n",
    "        state.analysis = {\"raw_analysis\": str(analysis_response)}\n",
    "        state.metadata['analysis_complete'] = True\n",
    "        \n",
    "        print(\"\\n‚úÖ Workflow complete!\")\n",
    "        return state\n",
    "\n",
    "# Create and test workflow\n",
    "workflow = SequentialWorkflow(text_processor, summarizer, analyzer)\n",
    "\n",
    "# Test with sample text\n",
    "sample_text = \"\"\"Artificial intelligence is transforming industries across the globe. \n",
    "From healthcare to finance, AI applications are improving efficiency and creating new opportunities. \n",
    "However, we must also consider ethical implications and ensure responsible development.\"\"\"\n",
    "\n",
    "result = workflow.run(sample_text)\n",
    "\n",
    "print(\"\\nüìä Workflow Results:\")\n",
    "print(f\"Summary: {result.summary}\")\n",
    "print(f\"\\nMetadata: {result.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 4: Implementing Parallel Workflows\n",
    "\n",
    "### Parallel Processing\n",
    "Let's build workflows that execute multiple agents in parallel for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelWorkflow:\n",
    "    \"\"\"Parallel execution workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create specialized agents\n",
    "        self.sentiment_analyzer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Analyze text sentiment. Return: positive, negative, or neutral.\"\n",
    "        )\n",
    "        \n",
    "        self.keyword_extractor = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Extract key terms and concepts from text. Return as a list.\"\n",
    "        )\n",
    "        \n",
    "        self.language_detector = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Detect the language of the text and provide confidence score.\"\n",
    "        )\n",
    "    \n",
    "    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze sentiment asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.sentiment_analyzer(f\"Analyze sentiment: {text}\")\n",
    "        return {\n",
    "            \"sentiment\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def extract_keywords(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract keywords asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.keyword_extractor(f\"Extract keywords: {text}\")\n",
    "        return {\n",
    "            \"keywords\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def detect_language(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect language asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.language_detector(f\"Detect language: {text}\")\n",
    "        return {\n",
    "            \"language\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def run(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute parallel workflow\"\"\"\n",
    "        print(\"‚ö° Starting Parallel Workflow...\")\n",
    "        overall_start = time.time()\n",
    "        \n",
    "        # Execute all analyses in parallel\n",
    "        tasks = [\n",
    "            self.analyze_sentiment(text),\n",
    "            self.extract_keywords(text),\n",
    "            self.detect_language(text)\n",
    "        ]\n",
    "        \n",
    "        print(\"üîÑ Executing 3 agents in parallel...\")\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        combined_results = {\n",
    "            \"sentiment\": results[0],\n",
    "            \"keywords\": results[1],\n",
    "            \"language\": results[2],\n",
    "            \"total_duration\": time.time() - overall_start\n",
    "        }\n",
    "        \n",
    "        print(\"\\n‚úÖ Parallel execution complete!\")\n",
    "        return combined_results\n",
    "\n",
    "# Create and test parallel workflow\n",
    "parallel_workflow = ParallelWorkflow()\n",
    "\n",
    "# Run parallel workflow\n",
    "parallel_results = await parallel_workflow.run(sample_text)\n",
    "\n",
    "print(\"\\nüìä Parallel Workflow Results:\")\n",
    "print(f\"Total Duration: {parallel_results['total_duration']:.2f}s\")\n",
    "print(f\"\\nSentiment Analysis: {parallel_results['sentiment']['sentiment']}\")\n",
    "print(f\"  Duration: {parallel_results['sentiment']['duration']:.2f}s\")\n",
    "print(f\"\\nKeywords: {parallel_results['keywords']['keywords']}\")\n",
    "print(f\"  Duration: {parallel_results['keywords']['duration']:.2f}s\")\n",
    "print(f\"\\nLanguage: {parallel_results['language']['language']}\")\n",
    "print(f\"  Duration: {parallel_results['language']['duration']:.2f}s\")\n",
    "\n",
    "# Compare with sequential execution time\n",
    "sequential_time = sum([\n",
    "    parallel_results['sentiment']['duration'],\n",
    "    parallel_results['keywords']['duration'],\n",
    "    parallel_results['language']['duration']\n",
    "])\n",
    "print(f\"\\n‚ö° Speed improvement: {sequential_time / parallel_results['total_duration']:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Step 5: Building Conditional Workflows\n",
    "\n",
    "### Dynamic Routing\n",
    "Let's create workflows that make decisions and route to different agents based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalWorkflow:\n",
    "    \"\"\"Workflow with conditional branching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Router agent\n",
    "        self.router = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"Classify content into categories:\n",
    "            - TECHNICAL: Programming, technology, engineering\n",
    "            - BUSINESS: Finance, marketing, management\n",
    "            - CREATIVE: Art, writing, design\n",
    "            Return only the category name.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Specialized handlers\n",
    "        self.technical_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a technical expert.\n",
    "            Provide detailed technical analysis and code examples when relevant.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.business_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a business analyst.\n",
    "            Focus on ROI, market impact, and strategic implications.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.creative_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a creative consultant.\n",
    "            Emphasize aesthetics, user experience, and innovation.\"\"\"\n",
    "        )\n",
    "    \n",
    "    def route_content(self, content: str) -> str:\n",
    "        \"\"\"Determine content category\"\"\"\n",
    "        category = str(self.router(f\"Classify this content: {content}\"))\n",
    "        category = category.strip().upper()\n",
    "        \n",
    "        # Ensure valid category\n",
    "        if category not in [\"TECHNICAL\", \"BUSINESS\", \"CREATIVE\"]:\n",
    "            print(f\"‚ö†Ô∏è  Unknown category '{category}', defaulting to BUSINESS\")\n",
    "            category = \"BUSINESS\"\n",
    "        \n",
    "        return category\n",
    "    \n",
    "    def process_content(self, content: str, task: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process content based on its category\"\"\"\n",
    "        print(\"üå≥ Starting Conditional Workflow...\")\n",
    "        \n",
    "        # Step 1: Route content\n",
    "        print(\"\\nüîÄ Routing content...\")\n",
    "        category = self.route_content(content)\n",
    "        print(f\"   Category: {category}\")\n",
    "        \n",
    "        # Step 2: Process with appropriate handler\n",
    "        print(f\"\\nüìù Processing with {category} handler...\")\n",
    "        \n",
    "        if category == \"TECHNICAL\":\n",
    "            response = self.technical_handler(f\"{task}: {content}\")\n",
    "        elif category == \"BUSINESS\":\n",
    "            response = self.business_handler(f\"{task}: {content}\")\n",
    "        else:  # CREATIVE\n",
    "            response = self.creative_handler(f\"{task}: {content}\")\n",
    "        \n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"response\": str(response),\n",
    "            \"handler_used\": f\"{category.lower()}_handler\"\n",
    "        }\n",
    "\n",
    "# Create conditional workflow\n",
    "conditional_workflow = ConditionalWorkflow()\n",
    "\n",
    "# Test with different content types\n",
    "test_contents = [\n",
    "    \"How to implement a binary search algorithm in Python\",\n",
    "    \"Strategies for increasing quarterly revenue by 20%\",\n",
    "    \"Design principles for creating engaging user interfaces\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Conditional Workflow\\n\")\n",
    "\n",
    "for content in test_contents:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Content: {content[:50]}...\")\n",
    "    \n",
    "    result = conditional_workflow.process_content(\n",
    "        content, \n",
    "        \"Provide expert analysis\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Category: {result['category']}\")\n",
    "    print(f\"üìä Response: {result['response'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Implementing Workflow Orchestration\n",
    "\n",
    "### Complex Orchestration\n",
    "Let's build a sophisticated workflow orchestrator that combines multiple patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowOrchestrator:\n",
    "    \"\"\"Advanced workflow orchestration system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workflows = {}\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def register_workflow(self, name: str, workflow: Any):\n",
    "        \"\"\"Register a workflow\"\"\"\n",
    "        self.workflows[name] = workflow\n",
    "        print(f\"‚úÖ Registered workflow: {name}\")\n",
    "    \n",
    "    async def execute_workflow(self, name: str, input_data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a named workflow\"\"\"\n",
    "        if name not in self.workflows:\n",
    "            raise ValueError(f\"Workflow '{name}' not found\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Execute workflow\n",
    "            workflow = self.workflows[name]\n",
    "            \n",
    "            # Handle both sync and async workflows\n",
    "            if asyncio.iscoroutinefunction(workflow.run):\n",
    "                result = await workflow.run(input_data)\n",
    "            else:\n",
    "                result = workflow.run(input_data)\n",
    "            \n",
    "            execution_record = {\n",
    "                \"workflow\": name,\n",
    "                \"status\": \"success\",\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\"error\": str(e)}\n",
    "            execution_record = {\n",
    "                \"workflow\": name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "        \n",
    "        self.execution_history.append(execution_record)\n",
    "        return result\n",
    "    \n",
    "    def get_execution_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get workflow execution statistics\"\"\"\n",
    "        if not self.execution_history:\n",
    "            return {\"message\": \"No executions recorded\"}\n",
    "        \n",
    "        total = len(self.execution_history)\n",
    "        successful = sum(1 for e in self.execution_history if e[\"status\"] == \"success\")\n",
    "        failed = total - successful\n",
    "        \n",
    "        avg_duration = sum(e[\"duration\"] for e in self.execution_history) / total\n",
    "        \n",
    "        return {\n",
    "            \"total_executions\": total,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"success_rate\": (successful / total) * 100,\n",
    "            \"avg_duration\": avg_duration\n",
    "        }\n",
    "\n",
    "# Complex multi-stage workflow\n",
    "class ResearchWorkflow:\n",
    "    \"\"\"Multi-stage research workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.researcher = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Research the topic and provide comprehensive information.\"\n",
    "        )\n",
    "        \n",
    "        self.fact_checker = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Verify facts and identify any inaccuracies or concerns.\"\n",
    "        )\n",
    "        \n",
    "        self.report_writer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Write a professional report with clear sections and conclusions.\"\n",
    "        )\n",
    "    \n",
    "    async def run(self, topic: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute research workflow\"\"\"\n",
    "        print(f\"üî¨ Starting Research Workflow for: {topic}\")\n",
    "        \n",
    "        # Stage 1: Research\n",
    "        print(\"\\nüìö Stage 1: Researching...\")\n",
    "        research_data = str(self.researcher(f\"Research this topic: {topic}\"))\n",
    "        \n",
    "        # Stage 2: Fact check (parallel with report drafting)\n",
    "        print(\"\\nüîç Stage 2: Fact checking and drafting report...\")\n",
    "        \n",
    "        fact_check_task = asyncio.create_task(\n",
    "            asyncio.to_thread(\n",
    "                self.fact_checker,\n",
    "                f\"Fact check this research: {research_data[:500]}...\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        report_task = asyncio.create_task(\n",
    "            asyncio.to_thread(\n",
    "                self.report_writer,\n",
    "                f\"Write a report on {topic} based on: {research_data}\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Wait for both tasks\n",
    "        fact_check_results, report = await asyncio.gather(\n",
    "            fact_check_task, report_task\n",
    "        )\n",
    "        \n",
    "        # Stage 3: Finalize\n",
    "        print(\"\\n‚úçÔ∏è Stage 3: Finalizing report...\")\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"research\": research_data[:500] + \"...\",\n",
    "            \"fact_check\": str(fact_check_results),\n",
    "            \"report\": str(report),\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "\n",
    "# Create orchestrator and register workflows\n",
    "orchestrator = WorkflowOrchestrator()\n",
    "orchestrator.register_workflow(\"sequential\", workflow)\n",
    "orchestrator.register_workflow(\"parallel\", parallel_workflow)\n",
    "orchestrator.register_workflow(\"conditional\", conditional_workflow)\n",
    "orchestrator.register_workflow(\"research\", ResearchWorkflow())\n",
    "\n",
    "# Execute research workflow\n",
    "research_result = await orchestrator.execute_workflow(\n",
    "    \"research\", \n",
    "    \"The impact of quantum computing on cybersecurity\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Research Workflow Results:\")\n",
    "print(f\"Status: {research_result.get('status', 'unknown')}\")\n",
    "print(f\"\\nFact Check Results: {research_result.get('fact_check', 'N/A')[:200]}...\")\n",
    "\n",
    "# Show execution stats\n",
    "print(\"\\nüìà Workflow Execution Statistics:\")\n",
    "stats = orchestrator.get_execution_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Step 7: Error Handling and Retry Logic\n",
    "\n",
    "### Resilient Workflows\n",
    "Let's implement robust error handling and retry mechanisms for our workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResilientWorkflow:\n",
    "    \"\"\"Workflow with error handling and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        \n",
    "        # Create agents with potential for failures\n",
    "        self.data_fetcher = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Fetch and validate data. Sometimes simulate failures for testing.\"\n",
    "        )\n",
    "        \n",
    "        self.data_processor = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Process data and handle edge cases gracefully.\"\n",
    "        )\n",
    "    \n",
    "    async def fetch_with_retry(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fetch data with retry logic\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                print(f\"\\nüîÑ Attempt {attempt + 1}/{self.max_retries}: Fetching data...\")\n",
    "                \n",
    "                # Simulate potential failure\n",
    "                if attempt == 0 and \"fail\" in query.lower():\n",
    "                    raise Exception(\"Simulated network error\")\n",
    "                \n",
    "                result = self.data_fetcher(f\"Fetch data for: {query}\")\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"data\": str(result),\n",
    "                    \"attempts\": attempt + 1\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                print(f\"   ‚ùå Error: {str(e)}\")\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"   ‚è≥ Retrying in {self.retry_delay}s...\")\n",
    "                    await asyncio.sleep(self.retry_delay)\n",
    "                    # Exponential backoff\n",
    "                    self.retry_delay *= 2\n",
    "        \n",
    "        # All retries failed\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"attempts\": self.max_retries\n",
    "        }\n",
    "    \n",
    "    async def run(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute resilient workflow\"\"\"\n",
    "        print(\"üõ°Ô∏è Starting Resilient Workflow...\")\n",
    "        workflow_result = {\n",
    "            \"query\": query,\n",
    "            \"stages\": {}\n",
    "        }\n",
    "        \n",
    "        # Stage 1: Fetch data with retry\n",
    "        fetch_result = await self.fetch_with_retry(query)\n",
    "        workflow_result[\"stages\"][\"fetch\"] = fetch_result\n",
    "        \n",
    "        if not fetch_result[\"success\"]:\n",
    "            workflow_result[\"status\"] = \"failed\"\n",
    "            workflow_result[\"error\"] = \"Failed to fetch data after all retries\"\n",
    "            return workflow_result\n",
    "        \n",
    "        # Stage 2: Process data\n",
    "        try:\n",
    "            print(\"\\nüìä Processing fetched data...\")\n",
    "            processed = self.data_processor(\n",
    "                f\"Process this data: {fetch_result['data']}\"\n",
    "            )\n",
    "            \n",
    "            workflow_result[\"stages\"][\"process\"] = {\n",
    "                \"success\": True,\n",
    "                \"result\": str(processed)\n",
    "            }\n",
    "            workflow_result[\"status\"] = \"completed\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            workflow_result[\"stages\"][\"process\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            workflow_result[\"status\"] = \"partial_failure\"\n",
    "        \n",
    "        return workflow_result\n",
    "\n",
    "# Test resilient workflow\n",
    "resilient_workflow = ResilientWorkflow(max_retries=3, retry_delay=0.5)\n",
    "\n",
    "# Test with simulated failure\n",
    "print(\"üß™ Testing with simulated failure...\")\n",
    "failure_result = await resilient_workflow.run(\"Fail on first attempt then succeed\")\n",
    "\n",
    "print(\"\\nüìä Resilient Workflow Results:\")\n",
    "print(f\"Status: {failure_result.get('status', 'unknown')}\")\n",
    "print(f\"Fetch attempts: {failure_result['stages']['fetch']['attempts']}\")\n",
    "print(f\"Fetch success: {failure_result['stages']['fetch']['success']}\")\n",
    "\n",
    "# Test with successful case\n",
    "print(\"\\n\\nüß™ Testing with successful case...\")\n",
    "success_result = await resilient_workflow.run(\"Process customer data\")\n",
    "print(f\"\\nStatus: {success_result.get('status', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: State Management in Workflows\n",
    "\n",
    "### Managing Shared State\n",
    "Let's implement sophisticated state management for complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "class WorkflowStatus(Enum):\n",
    "    \"\"\"Workflow execution status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "\n",
    "class StatefulWorkflow:\n",
    "    \"\"\"Workflow with comprehensive state management\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow_id: str):\n",
    "        self.workflow_id = workflow_id\n",
    "        self.state = {\n",
    "            \"id\": workflow_id,\n",
    "            \"status\": WorkflowStatus.PENDING,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"steps_completed\": [],\n",
    "            \"current_step\": None,\n",
    "            \"data\": {},\n",
    "            \"errors\": [],\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        # Create agents\n",
    "        self.validator = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Validate input data and identify any issues.\"\n",
    "        )\n",
    "        \n",
    "        self.transformer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Transform data into the required format.\"\n",
    "        )\n",
    "        \n",
    "        self.analyzer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Analyze transformed data and provide insights.\"\n",
    "        )\n",
    "    \n",
    "    def update_state(self, updates: Dict[str, Any]):\n",
    "        \"\"\"Update workflow state\"\"\"\n",
    "        self.state.update(updates)\n",
    "        self.state[\"last_updated\"] = datetime.now().isoformat()\n",
    "    \n",
    "    def add_step_result(self, step_name: str, result: Any):\n",
    "        \"\"\"Add step result to state\"\"\"\n",
    "        self.state[\"steps_completed\"].append({\n",
    "            \"step\": step_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"result\": result\n",
    "        })\n",
    "        self.state[\"data\"][step_name] = result\n",
    "    \n",
    "    async def execute(self, input_data: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute stateful workflow\"\"\"\n",
    "        print(f\"üìä Starting Stateful Workflow: {self.workflow_id}\")\n",
    "        self.update_state({\n",
    "            \"status\": WorkflowStatus.RUNNING,\n",
    "            \"started_at\": datetime.now().isoformat(),\n",
    "            \"input\": input_data\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Validation\n",
    "            self.state[\"current_step\"] = \"validation\"\n",
    "            print(\"\\nüîç Step 1: Validating input...\")\n",
    "            \n",
    "            validation_result = self.validator(f\"Validate: {input_data}\")\n",
    "            self.add_step_result(\"validation\", str(validation_result))\n",
    "            \n",
    "            # Step 2: Transformation\n",
    "            self.state[\"current_step\"] = \"transformation\"\n",
    "            print(\"\\nüîÑ Step 2: Transforming data...\")\n",
    "            \n",
    "            transform_result = self.transformer(\n",
    "                f\"Transform based on validation: {validation_result}\"\n",
    "            )\n",
    "            self.add_step_result(\"transformation\", str(transform_result))\n",
    "            \n",
    "            # Step 3: Analysis\n",
    "            self.state[\"current_step\"] = \"analysis\"\n",
    "            print(\"\\nüìà Step 3: Analyzing data...\")\n",
    "            \n",
    "            analysis_result = self.analyzer(\n",
    "                f\"Analyze: {transform_result}\"\n",
    "            )\n",
    "            self.add_step_result(\"analysis\", str(analysis_result))\n",
    "            \n",
    "            # Workflow completed\n",
    "            self.update_state({\n",
    "                \"status\": WorkflowStatus.COMPLETED,\n",
    "                \"completed_at\": datetime.now().isoformat(),\n",
    "                \"current_step\": None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.state[\"errors\"].append({\n",
    "                \"step\": self.state[\"current_step\"],\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            self.update_state({\n",
    "                \"status\": WorkflowStatus.FAILED,\n",
    "                \"failed_at\": datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        return self.get_state_summary()\n",
    "    \n",
    "    def get_state_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get workflow state summary\"\"\"\n",
    "        return {\n",
    "            \"id\": self.state[\"id\"],\n",
    "            \"status\": self.state[\"status\"].value,\n",
    "            \"steps_completed\": len(self.state[\"steps_completed\"]),\n",
    "            \"current_step\": self.state[\"current_step\"],\n",
    "            \"has_errors\": len(self.state[\"errors\"]) > 0,\n",
    "            \"data_keys\": list(self.state[\"data\"].keys()),\n",
    "            \"execution_time\": self._calculate_execution_time()\n",
    "        }\n",
    "    \n",
    "    def _calculate_execution_time(self) -> Optional[float]:\n",
    "        \"\"\"Calculate total execution time\"\"\"\n",
    "        if \"started_at\" not in self.state:\n",
    "            return None\n",
    "        \n",
    "        start = datetime.fromisoformat(self.state[\"started_at\"])\n",
    "        \n",
    "        if \"completed_at\" in self.state:\n",
    "            end = datetime.fromisoformat(self.state[\"completed_at\"])\n",
    "        elif \"failed_at\" in self.state:\n",
    "            end = datetime.fromisoformat(self.state[\"failed_at\"])\n",
    "        else:\n",
    "            end = datetime.now()\n",
    "        \n",
    "        return (end - start).total_seconds()\n",
    "\n",
    "# Test stateful workflow\n",
    "stateful_workflow = StatefulWorkflow(\"WF-001\")\n",
    "\n",
    "# Execute workflow\n",
    "result = await stateful_workflow.execute(\n",
    "    \"Analyze customer feedback data for Q1 2024\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Workflow State Summary:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Show detailed state\n",
    "print(\"\\nüìã Completed Steps:\")\n",
    "for step in stateful_workflow.state[\"steps_completed\"]:\n",
    "    print(f\"   - {step['step']} at {step['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Production-Ready Workflow Patterns\n",
    "\n",
    "### Best Practices\n",
    "Let's explore production-ready workflow patterns and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ PRODUCTION WORKFLOW BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = {\n",
    "    \"üèóÔ∏è Architecture\": [\n",
    "        \"Use single-responsibility agents\",\n",
    "        \"Implement clear interfaces between steps\",\n",
    "        \"Design for modularity and reusability\",\n",
    "        \"Keep workflows testable\",\n",
    "        \"Version your workflow definitions\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Parallelize independent steps\",\n",
    "        \"Implement caching for expensive operations\",\n",
    "        \"Use streaming for large data\",\n",
    "        \"Monitor and optimize bottlenecks\",\n",
    "        \"Set appropriate timeouts\"\n",
    "    ],\n",
    "    \"üõ°Ô∏è Reliability\": [\n",
    "        \"Implement comprehensive error handling\",\n",
    "        \"Add retry logic with exponential backoff\",\n",
    "        \"Use circuit breakers for external services\",\n",
    "        \"Implement graceful degradation\",\n",
    "        \"Add health checks\"\n",
    "    ],\n",
    "    \"üìä Observability\": [\n",
    "        \"Log all state transitions\",\n",
    "        \"Track execution metrics\",\n",
    "        \"Implement distributed tracing\",\n",
    "        \"Monitor resource usage\",\n",
    "        \"Create meaningful dashboards\"\n",
    "    ],\n",
    "    \"üîÑ State Management\": [\n",
    "        \"Use immutable state where possible\",\n",
    "        \"Implement state persistence\",\n",
    "        \"Handle partial failures gracefully\",\n",
    "        \"Support workflow resumption\",\n",
    "        \"Maintain audit trails\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "# Example production configuration\n",
    "print(\"\\n\\nüìã Example Production Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_config = {\n",
    "    \"workflow\": {\n",
    "        \"max_concurrent_workflows\": 10,\n",
    "        \"default_timeout_seconds\": 300,\n",
    "        \"retry_policy\": {\n",
    "            \"max_attempts\": 3,\n",
    "            \"initial_delay\": 1,\n",
    "            \"max_delay\": 60,\n",
    "            \"exponential_base\": 2\n",
    "        },\n",
    "        \"state_storage\": {\n",
    "            \"type\": \"redis\",\n",
    "            \"ttl_seconds\": 86400\n",
    "        }\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"metrics_enabled\": True,\n",
    "        \"tracing_enabled\": True,\n",
    "        \"log_level\": \"INFO\"\n",
    "    },\n",
    "    \"scaling\": {\n",
    "        \"auto_scaling_enabled\": True,\n",
    "        \"min_workers\": 2,\n",
    "        \"max_workers\": 20,\n",
    "        \"scale_up_threshold\": 0.8,\n",
    "        \"scale_down_threshold\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "In this tutorial, you've mastered:\n",
    "- ‚úÖ Building sequential workflows\n",
    "- ‚úÖ Implementing parallel execution\n",
    "- ‚úÖ Creating conditional workflows\n",
    "- ‚úÖ Orchestrating complex multi-agent systems\n",
    "- ‚úÖ Implementing error handling and retries\n",
    "- ‚úÖ Managing workflow state\n",
    "- ‚úÖ Production-ready patterns\n",
    "\n",
    "### üîÑ The Power of Workflows\n",
    "\n",
    "You now have the skills to:\n",
    "- **Build complex AI systems** with multiple specialized agents\n",
    "- **Orchestrate sophisticated processes** with conditional logic\n",
    "- **Scale efficiently** with parallel execution\n",
    "- **Handle failures gracefully** with retry mechanisms\n",
    "- **Maintain state** across complex operations\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **Modular Design**: Break complex tasks into specialized agents\n",
    "2. **Parallel When Possible**: Maximize efficiency with concurrent execution\n",
    "3. **Handle Failures**: Always implement error handling and retries\n",
    "4. **Track State**: Maintain comprehensive state for debugging and recovery\n",
    "5. **Monitor Everything**: Observability is crucial for production workflows\n",
    "\n",
    "### üîÆ Advanced Patterns\n",
    "\n",
    "Consider exploring:\n",
    "- **Saga Pattern**: For distributed transactions\n",
    "- **Event-Driven Workflows**: Using message queues\n",
    "- **Human-in-the-Loop**: Workflows with manual approval steps\n",
    "- **Dynamic Workflows**: Self-modifying based on results\n",
    "- **Workflow Composition**: Nesting workflows within workflows\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Strands Documentation](https://strandsagents.com/0.1.x/)\n",
    "- [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "- [Apache Airflow](https://airflow.apache.org/)\n",
    "- [Temporal.io](https://temporal.io/)\n",
    "\n",
    "### üåü Next Steps\n",
    "\n",
    "You're ready to:\n",
    "1. Design complex multi-agent systems\n",
    "2. Build production-grade workflow orchestration\n",
    "3. Implement sophisticated business processes\n",
    "4. Create scalable AI applications\n",
    "5. Lead workflow architecture initiatives\n",
    "\n",
    "### üöÄ Final Thoughts\n",
    "\n",
    "Workflows transform individual AI agents into powerful, orchestrated systems. With the patterns and practices you've learned, you can build applications that handle complex, real-world processes reliably and efficiently.\n",
    "\n",
    "Remember: Great AI applications are built one workflow at a time!\n",
    "\n",
    "Happy orchestrating! üîÑü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
