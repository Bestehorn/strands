{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "**Orchestrating Complex Multi-Agent Systems**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this comprehensive tutorial on **workflows** in the Strands framework! This notebook demonstrates how to orchestrate complex multi-agent systems, build intelligent pipelines, and create sophisticated AI applications. By the end of this 10-minute tutorial, you'll master the art of agent orchestration.\n",
    "\n",
    "### 🎯 What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Build multi-agent workflows\n",
    "- Implement sequential and parallel pipelines\n",
    "- Create conditional workflows\n",
    "- Handle state management between agents\n",
    "- Build complex orchestration patterns\n",
    "- Implement error handling and retries\n",
    "\n",
    "### 🔄 Why Workflows Matter\n",
    "\n",
    "Workflows enable you to:\n",
    "- **Divide complex tasks** into manageable steps\n",
    "- **Leverage specialized agents** for specific roles\n",
    "- **Build scalable systems** with modular components\n",
    "- **Implement business logic** through orchestration\n",
    "- **Create maintainable** AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Step 1: Installing Required Packages\n",
    "\n",
    "### Overview\n",
    "Let's install the necessary packages for building workflows.\n",
    "\n",
    "### 📚 Packages We'll Install\n",
    "- **strands-agents**: Core framework with workflow support\n",
    "- **strands-agents-tools**: Additional workflow tools\n",
    "- **asyncio**: For async workflow execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install strands-agents strands-agents-tools strands-agents-builder -q\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")\n",
    "print(\"   Ready to build workflows! 🔄\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔐 Step 2: Setting Up AWS Authentication\n",
    "\n",
    "### Overview\n",
    "We'll configure AWS Bedrock for our workflow agents.\n",
    "\n",
    "### 🔑 Authentication Options\n",
    "1. **AWS Profile** (Recommended for development)\n",
    "2. **Environment Variables**\n",
    "3. **Direct Credentials** (Less secure)\n",
    "4. **IAM Roles** (Recommended for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# Configure AWS session\n",
    "session = boto3.Session(\n",
    "    # aws_access_key_id='your_access_key',\n",
    "    # aws_secret_access_key='your_secret_key',\n",
    "    # aws_session_token='your_session_token',  # If using temporary credentials\n",
    "    # region_name='us-west-2',\n",
    "    profile_name='default'  # Optional: Use a specific AWS profile\n",
    ")\n",
    "\n",
    "# Create a Bedrock model instance\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    boto_session=session\n",
    ")\n",
    "\n",
    "print(\"✅ AWS Bedrock configured successfully!\")\n",
    "print(f\"   Model: Claude 3.7 Sonnet\")\n",
    "print(f\"   Profile: {session.profile_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Step 3: Building Basic Sequential Workflows\n",
    "\n",
    "### Sequential Processing\n",
    "Let's start with simple sequential workflows where agents process tasks one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow state\n",
    "@dataclass\n",
    "class WorkflowState:\n",
    "    \"\"\"Shared state between workflow steps\"\"\"\n",
    "    input_text: str\n",
    "    processed_text: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    analysis: Optional[Dict[str, Any]] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "# Create specialized agents\n",
    "text_processor = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a text processing specialist.\n",
    "    Clean and format text for clarity and readability.\n",
    "    Fix grammar and spelling errors while preserving meaning.\"\"\"\n",
    ")\n",
    "\n",
    "summarizer = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a summarization expert.\n",
    "    Create concise, accurate summaries that capture key points.\n",
    "    Use bullet points for clarity.\"\"\"\n",
    ")\n",
    "\n",
    "analyzer = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a text analysis expert.\n",
    "    Analyze sentiment, tone, and key themes.\n",
    "    Provide structured insights.\"\"\"\n",
    ")\n",
    "\n",
    "# Sequential workflow\n",
    "class SequentialWorkflow:\n",
    "    \"\"\"Sequential text processing workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: Agent, summarizer: Agent, analyzer: Agent):\n",
    "        self.processor = processor\n",
    "        self.summarizer = summarizer\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def run(self, text: str) -> WorkflowState:\n",
    "        \"\"\"Execute sequential workflow\"\"\"\n",
    "        print(\"🔄 Starting Sequential Workflow...\")\n",
    "        state = WorkflowState(input_text=text)\n",
    "        \n",
    "        # Step 1: Process text\n",
    "        print(\"\\n📝 Step 1: Processing text...\")\n",
    "        state.processed_text = str(self.processor(f\"Process this text: {text}\"))\n",
    "        state.metadata['processing_complete'] = True\n",
    "        \n",
    "        # Step 2: Summarize\n",
    "        print(\"\\n📋 Step 2: Summarizing...\")\n",
    "        state.summary = str(self.summarizer(f\"Summarize: {state.processed_text}\"))\n",
    "        state.metadata['summary_complete'] = True\n",
    "        \n",
    "        # Step 3: Analyze\n",
    "        print(\"\\n🔍 Step 3: Analyzing...\")\n",
    "        analysis_response = self.analyzer(\n",
    "            f\"Analyze this text for sentiment, tone, and themes: {state.processed_text}\"\n",
    "        )\n",
    "        state.analysis = {\"raw_analysis\": str(analysis_response)}\n",
    "        state.metadata['analysis_complete'] = True\n",
    "        \n",
    "        print(\"\\n✅ Workflow complete!\")\n",
    "        return state\n",
    "\n",
    "# Create and test workflow\n",
    "workflow = SequentialWorkflow(text_processor, summarizer, analyzer)\n",
    "\n",
    "# Test with sample text\n",
    "sample_text = \"\"\"Artificial intelligence is transforming industries across the globe. \n",
    "From healthcare to finance, AI applications are improving efficiency and creating new opportunities. \n",
    "However, we must also consider ethical implications and ensure responsible development.\"\"\"\n",
    "\n",
    "result = workflow.run(sample_text)\n",
    "\n",
    "print(\"\\n📊 Workflow Results:\")\n",
    "print(f\"Summary: {result.summary}\")\n",
    "print(f\"\\nMetadata: {result.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Step 4: Implementing Parallel Workflows\n",
    "\n",
    "### Parallel Processing\n",
    "Let's build workflows that execute multiple agents in parallel for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelWorkflow:\n",
    "    \"\"\"Parallel execution workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create specialized agents\n",
    "        self.sentiment_analyzer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Analyze text sentiment. Return: positive, negative, or neutral.\"\n",
    "        )\n",
    "        \n",
    "        self.keyword_extractor = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Extract key terms and concepts from text. Return as a list.\"\n",
    "        )\n",
    "        \n",
    "        self.language_detector = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Detect the language of the text and provide confidence score.\"\n",
    "        )\n",
    "    \n",
    "    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze sentiment asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.sentiment_analyzer(f\"Analyze sentiment: {text}\")\n",
    "        return {\n",
    "            \"sentiment\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def extract_keywords(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract keywords asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.keyword_extractor(f\"Extract keywords: {text}\")\n",
    "        return {\n",
    "            \"keywords\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def detect_language(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect language asynchronously\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = self.language_detector(f\"Detect language: {text}\")\n",
    "        return {\n",
    "            \"language\": str(result),\n",
    "            \"duration\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    async def run(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute parallel workflow\"\"\"\n",
    "        print(\"⚡ Starting Parallel Workflow...\")\n",
    "        overall_start = time.time()\n",
    "        \n",
    "        # Execute all analyses in parallel\n",
    "        tasks = [\n",
    "            self.analyze_sentiment(text),\n",
    "            self.extract_keywords(text),\n",
    "            self.detect_language(text)\n",
    "        ]\n",
    "        \n",
    "        print(\"🔄 Executing 3 agents in parallel...\")\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        combined_results = {\n",
    "            \"sentiment\": results[0],\n",
    "            \"keywords\": results[1],\n",
    "            \"language\": results[2],\n",
    "            \"total_duration\": time.time() - overall_start\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✅ Parallel execution complete!\")\n",
    "        return combined_results\n",
    "\n",
    "# Create and test parallel workflow\n",
    "parallel_workflow = ParallelWorkflow()\n",
    "\n",
    "# Run parallel workflow\n",
    "parallel_results = await parallel_workflow.run(sample_text)\n",
    "\n",
    "print(\"\\n📊 Parallel Workflow Results:\")\n",
    "print(f\"Total Duration: {parallel_results['total_duration']:.2f}s\")\n",
    "print(f\"\\nSentiment Analysis: {parallel_results['sentiment']['sentiment']}\")\n",
    "print(f\"  Duration: {parallel_results['sentiment']['duration']:.2f}s\")\n",
    "print(f\"\\nKeywords: {parallel_results['keywords']['keywords']}\")\n",
    "print(f\"  Duration: {parallel_results['keywords']['duration']:.2f}s\")\n",
    "print(f\"\\nLanguage: {parallel_results['language']['language']}\")\n",
    "print(f\"  Duration: {parallel_results['language']['duration']:.2f}s\")\n",
    "\n",
    "# Compare with sequential execution time\n",
    "sequential_time = sum([\n",
    "    parallel_results['sentiment']['duration'],\n",
    "    parallel_results['keywords']['duration'],\n",
    "    parallel_results['language']['duration']\n",
    "])\n",
    "print(f\"\\n⚡ Speed improvement: {sequential_time / parallel_results['total_duration']:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌳 Step 5: Building Conditional Workflows\n",
    "\n",
    "### Dynamic Routing\n",
    "Let's create workflows that make decisions and route to different agents based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalWorkflow:\n",
    "    \"\"\"Workflow with conditional branching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Router agent\n",
    "        self.router = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"Classify content into categories:\n",
    "            - TECHNICAL: Programming, technology, engineering\n",
    "            - BUSINESS: Finance, marketing, management\n",
    "            - CREATIVE: Art, writing, design\n",
    "            Return only the category name.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Specialized handlers\n",
    "        self.technical_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a technical expert.\n",
    "            Provide detailed technical analysis and code examples when relevant.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.business_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a business analyst.\n",
    "            Focus on ROI, market impact, and strategic implications.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.creative_handler = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"\"\"You are a creative consultant.\n",
    "            Emphasize aesthetics, user experience, and innovation.\"\"\"\n",
    "        )\n",
    "    \n",
    "    def route_content(self, content: str) -> str:\n",
    "        \"\"\"Determine content category\"\"\"\n",
    "        category = str(self.router(f\"Classify this content: {content}\"))\n",
    "        category = category.strip().upper()\n",
    "        \n",
    "        # Ensure valid category\n",
    "        if category not in [\"TECHNICAL\", \"BUSINESS\", \"CREATIVE\"]:\n",
    "            print(f\"⚠️  Unknown category '{category}', defaulting to BUSINESS\")\n",
    "            category = \"BUSINESS\"\n",
    "        \n",
    "        return category\n",
    "    \n",
    "    def process_content(self, content: str, task: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process content based on its category\"\"\"\n",
    "        print(\"🌳 Starting Conditional Workflow...\")\n",
    "        \n",
    "        # Step 1: Route content\n",
    "        print(\"\\n🔀 Routing content...\")\n",
    "        category = self.route_content(content)\n",
    "        print(f\"   Category: {category}\")\n",
    "        \n",
    "        # Step 2: Process with appropriate handler\n",
    "        print(f\"\\n📝 Processing with {category} handler...\")\n",
    "        \n",
    "        if category == \"TECHNICAL\":\n",
    "            response = self.technical_handler(f\"{task}: {content}\")\n",
    "        elif category == \"BUSINESS\":\n",
    "            response = self.business_handler(f\"{task}: {content}\")\n",
    "        else:  # CREATIVE\n",
    "            response = self.creative_handler(f\"{task}: {content}\")\n",
    "        \n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"response\": str(response),\n",
    "            \"handler_used\": f\"{category.lower()}_handler\"\n",
    "        }\n",
    "\n",
    "# Create conditional workflow\n",
    "conditional_workflow = ConditionalWorkflow()\n",
    "\n",
    "# Test with different content types\n",
    "test_contents = [\n",
    "    \"How to implement a binary search algorithm in Python\",\n",
    "    \"Strategies for increasing quarterly revenue by 20%\",\n",
    "    \"Design principles for creating engaging user interfaces\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Conditional Workflow\\n\")\n",
    "\n",
    "for content in test_contents:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Content: {content[:50]}...\")\n",
    "    \n",
    "    result = conditional_workflow.process_content(\n",
    "        content, \n",
    "        \"Provide expert analysis\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🏷️  Category: {result['category']}\")\n",
    "    print(f\"📊 Response: {result['response'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Step 6: Implementing Workflow Orchestration\n",
    "\n",
    "### Complex Orchestration\n",
    "Let's build a sophisticated workflow orchestrator that combines multiple patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowOrchestrator:\n",
    "    \"\"\"Advanced workflow orchestration system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workflows = {}\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def register_workflow(self, name: str, workflow: Any):\n",
    "        \"\"\"Register a workflow\"\"\"\n",
    "        self.workflows[name] = workflow\n",
    "        print(f\"✅ Registered workflow: {name}\")\n",
    "    \n",
    "    async def execute_workflow(self, name: str, input_data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a named workflow\"\"\"\n",
    "        if name not in self.workflows:\n",
    "            raise ValueError(f\"Workflow '{name}' not found\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Execute workflow\n",
    "            workflow = self.workflows[name]\n",
    "            \n",
    "            # Handle both sync and async workflows\n",
    "            if asyncio.iscoroutinefunction(workflow.run):\n",
    "                result = await workflow.run(input_data)\n",
    "            else:\n",
    "                result = workflow.run(input_data)\n",
    "            \n",
    "            execution_record = {\n",
    "                \"workflow\": name,\n",
    "                \"status\": \"success\",\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\"error\": str(e)}\n",
    "            execution_record = {\n",
    "                \"workflow\": name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "        \n",
    "        self.execution_history.append(execution_record)\n",
    "        return result\n",
    "    \n",
    "    def get_execution_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get workflow execution statistics\"\"\"\n",
    "        if not self.execution_history:\n",
    "            return {\"message\": \"No executions recorded\"}\n",
    "        \n",
    "        total = len(self.execution_history)\n",
    "        successful = sum(1 for e in self.execution_history if e[\"status\"] == \"success\")\n",
    "        failed = total - successful\n",
    "        \n",
    "        avg_duration = sum(e[\"duration\"] for e in self.execution_history) / total\n",
    "        \n",
    "        return {\n",
    "            \"total_executions\": total,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"success_rate\": (successful / total) * 100,\n",
    "            \"avg_duration\": avg_duration\n",
    "        }\n",
    "\n",
    "# Complex multi-stage workflow\n",
    "class ResearchWorkflow:\n",
    "    \"\"\"Multi-stage research workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.researcher = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Research the topic and provide comprehensive information.\"\n",
    "        )\n",
    "        \n",
    "        self.fact_checker = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Verify facts and identify any inaccuracies or concerns.\"\n",
    "        )\n",
    "        \n",
    "        self.report_writer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Write a professional report with clear sections and conclusions.\"\n",
    "        )\n",
    "    \n",
    "    async def run(self, topic: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute research workflow\"\"\"\n",
    "        print(f\"🔬 Starting Research Workflow for: {topic}\")\n",
    "        \n",
    "        # Stage 1: Research\n",
    "        print(\"\\n📚 Stage 1: Researching...\")\n",
    "        research_data = str(self.researcher(f\"Research this topic: {topic}\"))\n",
    "        \n",
    "        # Stage 2: Fact check (parallel with report drafting)\n",
    "        print(\"\\n🔍 Stage 2: Fact checking and drafting report...\")\n",
    "        \n",
    "        fact_check_task = asyncio.create_task(\n",
    "            asyncio.to_thread(\n",
    "                self.fact_checker,\n",
    "                f\"Fact check this research: {research_data[:500]}...\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        report_task = asyncio.create_task(\n",
    "            asyncio.to_thread(\n",
    "                self.report_writer,\n",
    "                f\"Write a report on {topic} based on: {research_data}\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Wait for both tasks\n",
    "        fact_check_results, report = await asyncio.gather(\n",
    "            fact_check_task, report_task\n",
    "        )\n",
    "        \n",
    "        # Stage 3: Finalize\n",
    "        print(\"\\n✍️ Stage 3: Finalizing report...\")\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"research\": research_data[:500] + \"...\",\n",
    "            \"fact_check\": str(fact_check_results),\n",
    "            \"report\": str(report),\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "\n",
    "# Create orchestrator and register workflows\n",
    "orchestrator = WorkflowOrchestrator()\n",
    "orchestrator.register_workflow(\"sequential\", workflow)\n",
    "orchestrator.register_workflow(\"parallel\", parallel_workflow)\n",
    "orchestrator.register_workflow(\"conditional\", conditional_workflow)\n",
    "orchestrator.register_workflow(\"research\", ResearchWorkflow())\n",
    "\n",
    "# Execute research workflow\n",
    "research_result = await orchestrator.execute_workflow(\n",
    "    \"research\", \n",
    "    \"The impact of quantum computing on cybersecurity\"\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Research Workflow Results:\")\n",
    "print(f\"Status: {research_result.get('status', 'unknown')}\")\n",
    "print(f\"\\nFact Check Results: {research_result.get('fact_check', 'N/A')[:200]}...\")\n",
    "\n",
    "# Show execution stats\n",
    "print(\"\\n📈 Workflow Execution Statistics:\")\n",
    "stats = orchestrator.get_execution_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛡️ Step 7: Error Handling and Retry Logic\n",
    "\n",
    "### Resilient Workflows\n",
    "Let's implement robust error handling and retry mechanisms for our workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResilientWorkflow:\n",
    "    \"\"\"Workflow with error handling and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        \n",
    "        # Create agents with potential for failures\n",
    "        self.data_fetcher = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Fetch and validate data. Sometimes simulate failures for testing.\"\n",
    "        )\n",
    "        \n",
    "        self.data_processor = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Process data and handle edge cases gracefully.\"\n",
    "        )\n",
    "    \n",
    "    async def fetch_with_retry(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fetch data with retry logic\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                print(f\"\\n🔄 Attempt {attempt + 1}/{self.max_retries}: Fetching data...\")\n",
    "                \n",
    "                # Simulate potential failure\n",
    "                if attempt == 0 and \"fail\" in query.lower():\n",
    "                    raise Exception(\"Simulated network error\")\n",
    "                \n",
    "                result = self.data_fetcher(f\"Fetch data for: {query}\")\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"data\": str(result),\n",
    "                    \"attempts\": attempt + 1\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                print(f\"   ❌ Error: {str(e)}\")\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"   ⏳ Retrying in {self.retry_delay}s...\")\n",
    "                    await asyncio.sleep(self.retry_delay)\n",
    "                    # Exponential backoff\n",
    "                    self.retry_delay *= 2\n",
    "        \n",
    "        # All retries failed\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"attempts\": self.max_retries\n",
    "        }\n",
    "    \n",
    "    async def run(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute resilient workflow\"\"\"\n",
    "        print(\"🛡️ Starting Resilient Workflow...\")\n",
    "        workflow_result = {\n",
    "            \"query\": query,\n",
    "            \"stages\": {}\n",
    "        }\n",
    "        \n",
    "        # Stage 1: Fetch data with retry\n",
    "        fetch_result = await self.fetch_with_retry(query)\n",
    "        workflow_result[\"stages\"][\"fetch\"] = fetch_result\n",
    "        \n",
    "        if not fetch_result[\"success\"]:\n",
    "            workflow_result[\"status\"] = \"failed\"\n",
    "            workflow_result[\"error\"] = \"Failed to fetch data after all retries\"\n",
    "            return workflow_result\n",
    "        \n",
    "        # Stage 2: Process data\n",
    "        try:\n",
    "            print(\"\\n📊 Processing fetched data...\")\n",
    "            processed = self.data_processor(\n",
    "                f\"Process this data: {fetch_result['data']}\"\n",
    "            )\n",
    "            \n",
    "            workflow_result[\"stages\"][\"process\"] = {\n",
    "                \"success\": True,\n",
    "                \"result\": str(processed)\n",
    "            }\n",
    "            workflow_result[\"status\"] = \"completed\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            workflow_result[\"stages\"][\"process\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            workflow_result[\"status\"] = \"partial_failure\"\n",
    "        \n",
    "        return workflow_result\n",
    "\n",
    "# Test resilient workflow\n",
    "resilient_workflow = ResilientWorkflow(max_retries=3, retry_delay=0.5)\n",
    "\n",
    "# Test with simulated failure\n",
    "print(\"🧪 Testing with simulated failure...\")\n",
    "failure_result = await resilient_workflow.run(\"Fail on first attempt then succeed\")\n",
    "\n",
    "print(\"\\n📊 Resilient Workflow Results:\")\n",
    "print(f\"Status: {failure_result.get('status', 'unknown')}\")\n",
    "print(f\"Fetch attempts: {failure_result['stages']['fetch']['attempts']}\")\n",
    "print(f\"Fetch success: {failure_result['stages']['fetch']['success']}\")\n",
    "\n",
    "# Test with successful case\n",
    "print(\"\\n\\n🧪 Testing with successful case...\")\n",
    "success_result = await resilient_workflow.run(\"Process customer data\")\n",
    "print(f\"\\nStatus: {success_result.get('status', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 8: State Management in Workflows\n",
    "\n",
    "### Managing Shared State\n",
    "Let's implement sophisticated state management for complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "class WorkflowStatus(Enum):\n",
    "    \"\"\"Workflow execution status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "\n",
    "class StatefulWorkflow:\n",
    "    \"\"\"Workflow with comprehensive state management\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow_id: str):\n",
    "        self.workflow_id = workflow_id\n",
    "        self.state = {\n",
    "            \"id\": workflow_id,\n",
    "            \"status\": WorkflowStatus.PENDING,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"steps_completed\": [],\n",
    "            \"current_step\": None,\n",
    "            \"data\": {},\n",
    "            \"errors\": [],\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        # Create agents\n",
    "        self.validator = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Validate input data and identify any issues.\"\n",
    "        )\n",
    "        \n",
    "        self.transformer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Transform data into the required format.\"\n",
    "        )\n",
    "        \n",
    "        self.analyzer = Agent(\n",
    "            model=bedrock_model,\n",
    "            system_prompt=\"Analyze transformed data and provide insights.\"\n",
    "        )\n",
    "    \n",
    "    def update_state(self, updates: Dict[str, Any]):\n",
    "        \"\"\"Update workflow state\"\"\"\n",
    "        self.state.update(updates)\n",
    "        self.state[\"last_updated\"] = datetime.now().isoformat()\n",
    "    \n",
    "    def add_step_result(self, step_name: str, result: Any):\n",
    "        \"\"\"Add step result to state\"\"\"\n",
    "        self.state[\"steps_completed\"].append({\n",
    "            \"step\": step_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"result\": result\n",
    "        })\n",
    "        self.state[\"data\"][step_name] = result\n",
    "    \n",
    "    async def execute(self, input_data: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute stateful workflow\"\"\"\n",
    "        print(f\"📊 Starting Stateful Workflow: {self.workflow_id}\")\n",
    "        self.update_state({\n",
    "            \"status\": WorkflowStatus.RUNNING,\n",
    "            \"started_at\": datetime.now().isoformat(),\n",
    "            \"input\": input_data\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Validation\n",
    "            self.state[\"current_step\"] = \"validation\"\n",
    "            print(\"\\n🔍 Step 1: Validating input...\")\n",
    "            \n",
    "            validation_result = self.validator(f\"Validate: {input_data}\")\n",
    "            self.add_step_result(\"validation\", str(validation_result))\n",
    "            \n",
    "            # Step 2: Transformation\n",
    "            self.state[\"current_step\"] = \"transformation\"\n",
    "            print(\"\\n🔄 Step 2: Transforming data...\")\n",
    "            \n",
    "            transform_result = self.transformer(\n",
    "                f\"Transform based on validation: {validation_result}\"\n",
    "            )\n",
    "            self.add_step_result(\"transformation\", str(transform_result))\n",
    "            \n",
    "            # Step 3: Analysis\n",
    "            self.state[\"current_step\"] = \"analysis\"\n",
    "            print(\"\\n📈 Step 3: Analyzing data...\")\n",
    "            \n",
    "            analysis_result = self.analyzer(\n",
    "                f\"Analyze: {transform_result}\"\n",
    "            )\n",
    "            self.add_step_result(\"analysis\", str(analysis_result))\n",
    "            \n",
    "            # Workflow completed\n",
    "            self.update_state({\n",
    "                \"status\": WorkflowStatus.COMPLETED,\n",
    "                \"completed_at\": datetime.now().isoformat(),\n",
    "                \"current_step\": None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.state[\"errors\"].append({\n",
    "                \"step\": self.state[\"current_step\"],\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            self.update_state({\n",
    "                \"status\": WorkflowStatus.FAILED,\n",
    "                \"failed_at\": datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        return self.get_state_summary()\n",
    "    \n",
    "    def get_state_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get workflow state summary\"\"\"\n",
    "        return {\n",
    "            \"id\": self.state[\"id\"],\n",
    "            \"status\": self.state[\"status\"].value,\n",
    "            \"steps_completed\": len(self.state[\"steps_completed\"]),\n",
    "            \"current_step\": self.state[\"current_step\"],\n",
    "            \"has_errors\": len(self.state[\"errors\"]) > 0,\n",
    "            \"data_keys\": list(self.state[\"data\"].keys()),\n",
    "            \"execution_time\": self._calculate_execution_time()\n",
    "        }\n",
    "    \n",
    "    def _calculate_execution_time(self) -> Optional[float]:\n",
    "        \"\"\"Calculate total execution time\"\"\"\n",
    "        if \"started_at\" not in self.state:\n",
    "            return None\n",
    "        \n",
    "        start = datetime.fromisoformat(self.state[\"started_at\"])\n",
    "        \n",
    "        if \"completed_at\" in self.state:\n",
    "            end = datetime.fromisoformat(self.state[\"completed_at\"])\n",
    "        elif \"failed_at\" in self.state:\n",
    "            end = datetime.fromisoformat(self.state[\"failed_at\"])\n",
    "        else:\n",
    "            end = datetime.now()\n",
    "        \n",
    "        return (end - start).total_seconds()\n",
    "\n",
    "# Test stateful workflow\n",
    "stateful_workflow = StatefulWorkflow(\"WF-001\")\n",
    "\n",
    "# Execute workflow\n",
    "result = await stateful_workflow.execute(\n",
    "    \"Analyze customer feedback data for Q1 2024\"\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Workflow State Summary:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Show detailed state\n",
    "print(\"\\n📋 Completed Steps:\")\n",
    "for step in stateful_workflow.state[\"steps_completed\"]:\n",
    "    print(f\"   - {step['step']} at {step['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Step 9: Production-Ready Workflow Patterns\n",
    "\n",
    "### Best Practices\n",
    "Let's explore production-ready workflow patterns and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 PRODUCTION WORKFLOW BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = {\n",
    "    \"🏗️ Architecture\": [\n",
    "        \"Use single-responsibility agents\",\n",
    "        \"Implement clear interfaces between steps\",\n",
    "        \"Design for modularity and reusability\",\n",
    "        \"Keep workflows testable\",\n",
    "        \"Version your workflow definitions\"\n",
    "    ],\n",
    "    \"⚡ Performance\": [\n",
    "        \"Parallelize independent steps\",\n",
    "        \"Implement caching for expensive operations\",\n",
    "        \"Use streaming for large data\",\n",
    "        \"Monitor and optimize bottlenecks\",\n",
    "        \"Set appropriate timeouts\"\n",
    "    ],\n",
    "    \"🛡️ Reliability\": [\n",
    "        \"Implement comprehensive error handling\",\n",
    "        \"Add retry logic with exponential backoff\",\n",
    "        \"Use circuit breakers for external services\",\n",
    "        \"Implement graceful degradation\",\n",
    "        \"Add health checks\"\n",
    "    ],\n",
    "    \"📊 Observability\": [\n",
    "        \"Log all state transitions\",\n",
    "        \"Track execution metrics\",\n",
    "        \"Implement distributed tracing\",\n",
    "        \"Monitor resource usage\",\n",
    "        \"Create meaningful dashboards\"\n",
    "    ],\n",
    "    \"🔄 State Management\": [\n",
    "        \"Use immutable state where possible\",\n",
    "        \"Implement state persistence\",\n",
    "        \"Handle partial failures gracefully\",\n",
    "        \"Support workflow resumption\",\n",
    "        \"Maintain audit trails\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for practice in practices:\n",
    "        print(f\"   • {practice}\")\n",
    "\n",
    "# Example production configuration\n",
    "print(\"\\n\\n📋 Example Production Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_config = {\n",
    "    \"workflow\": {\n",
    "        \"max_concurrent_workflows\": 10,\n",
    "        \"default_timeout_seconds\": 300,\n",
    "        \"retry_policy\": {\n",
    "            \"max_attempts\": 3,\n",
    "            \"initial_delay\": 1,\n",
    "            \"max_delay\": 60,\n",
    "            \"exponential_base\": 2\n",
    "        },\n",
    "        \"state_storage\": {\n",
    "            \"type\": \"redis\",\n",
    "            \"ttl_seconds\": 86400\n",
    "        }\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"metrics_enabled\": True,\n",
    "        \"tracing_enabled\": True,\n",
    "        \"log_level\": \"INFO\"\n",
    "    },\n",
    "    \"scaling\": {\n",
    "        \"auto_scaling_enabled\": True,\n",
    "        \"min_workers\": 2,\n",
    "        \"max_workers\": 20,\n",
    "        \"scale_up_threshold\": 0.8,\n",
    "        \"scale_down_threshold\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "### 🏆 What You've Accomplished\n",
    "In this tutorial, you've mastered:\n",
    "- ✅ Building sequential workflows\n",
    "- ✅ Implementing parallel execution\n",
    "- ✅ Creating conditional workflows\n",
    "- ✅ Orchestrating complex multi-agent systems\n",
    "- ✅ Implementing error handling and retries\n",
    "- ✅ Managing workflow state\n",
    "- ✅ Production-ready patterns\n",
    "\n",
    "### 🔄 The Power of Workflows\n",
    "\n",
    "You now have the skills to:\n",
    "- **Build complex AI systems** with multiple specialized agents\n",
    "- **Orchestrate sophisticated processes** with conditional logic\n",
    "- **Scale efficiently** with parallel execution\n",
    "- **Handle failures gracefully** with retry mechanisms\n",
    "- **Maintain state** across complex operations\n",
    "\n",
    "### 💡 Key Takeaways\n",
    "\n",
    "1. **Modular Design**: Break complex tasks into specialized agents\n",
    "2. **Parallel When Possible**: Maximize efficiency with concurrent execution\n",
    "3. **Handle Failures**: Always implement error handling and retries\n",
    "4. **Track State**: Maintain comprehensive state for debugging and recovery\n",
    "5. **Monitor Everything**: Observability is crucial for production workflows\n",
    "\n",
    "### 🔮 Advanced Patterns\n",
    "\n",
    "Consider exploring:\n",
    "- **Saga Pattern**: For distributed transactions\n",
    "- **Event-Driven Workflows**: Using message queues\n",
    "- **Human-in-the-Loop**: Workflows with manual approval steps\n",
    "- **Dynamic Workflows**: Self-modifying based on results\n",
    "- **Workflow Composition**: Nesting workflows within workflows\n",
    "\n",
    "### 📚 Resources\n",
    "\n",
    "- [Strands Documentation](https://strandsagents.com/0.1.x/)\n",
    "- [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "- [Apache Airflow](https://airflow.apache.org/)\n",
    "- [Temporal.io](https://temporal.io/)\n",
    "\n",
    "### 🌟 Next Steps\n",
    "\n",
    "You're ready to:\n",
    "1. Design complex multi-agent systems\n",
    "2. Build production-grade workflow orchestration\n",
    "3. Implement sophisticated business processes\n",
    "4. Create scalable AI applications\n",
    "5. Lead workflow architecture initiatives\n",
    "\n",
    "### 🚀 Final Thoughts\n",
    "\n",
    "Workflows transform individual AI agents into powerful, orchestrated systems. With the patterns and practices you've learned, you can build applications that handle complex, real-world processes reliably and efficiently.\n",
    "\n",
    "Remember: Great AI applications are built one workflow at a time!\n",
    "\n",
    "Happy orchestrating! 🔄🤖✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
