{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Agentic RAG\n",
    "\n",
    "**Professional Document Processing with AI Agents**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the advanced world of **Retrieval-Augmented Generation**! This notebook builds upon the basics to show you how to create production-ready RAG systems that can handle real-world documents like PDFs, Word files, and more. By the end of this 10-minute tutorial, you'll have a sophisticated document processing pipeline.\n",
    "\n",
    "### ğŸ¯ What You'll Learn\n",
    "\n",
    "In this advanced tutorial, you will:\n",
    "- Process multiple document formats (PDF, DOCX, TXT, MD)\n",
    "- Implement smart text chunking strategies\n",
    "- Use ChromaDB for persistent storage\n",
    "- Add metadata filtering and advanced search\n",
    "- Build a document analysis agent\n",
    "- Optimize performance for production use\n",
    "\n",
    "### ğŸ“‹ Prerequisites\n",
    "\n",
    "This tutorial assumes you've completed the \"Agentic RAG Basics\" notebook and understand:\n",
    "- Basic RAG concepts\n",
    "- Vector embeddings\n",
    "- Tool creation for agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Installing Advanced Dependencies\n",
    "\n",
    "### Additional Packages\n",
    "We'll need extra libraries for document processing and persistent storage.\n",
    "\n",
    "### ğŸ“š New Dependencies\n",
    "- **chromadb**: Persistent vector database with metadata filtering\n",
    "- **PyPDF2**: PDF text extraction\n",
    "- **python-docx**: Word document processing\n",
    "- **tiktoken**: Advanced text tokenization for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install chromadb PyPDF2 python-docx tiktoken -q\n",
    "\n",
    "# Also ensure we have the basics from the previous notebook\n",
    "%pip install sentence-transformers strands-agents reportlab -q\n",
    "\n",
    "print(\"âœ… All advanced packages installed successfully!\")\n",
    "print(\"   Ready for professional document processing! ğŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 2: Setting Up ChromaDB\n",
    "\n",
    "### Persistent Vector Storage\n",
    "Unlike FAISS (from the basic tutorial), ChromaDB provides:\n",
    "- **Persistence**: Your knowledge base survives restarts\n",
    "- **Metadata Filtering**: Search by document type, date, etc.\n",
    "- **Collections**: Organize documents by topic\n",
    "- **No Server Required**: Runs entirely local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a persistent ChromaDB client\n",
    "# This stores data in ./chroma_db directory\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_db\",\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use the same embedding model as before\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create or get a collection for our documents\n",
    "collection_name = \"strands_documents\"\n",
    "try:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"description\": \"Advanced RAG document collection\"}\n",
    "    )\n",
    "    print(f\"âœ… Created new collection: {collection_name}\")\n",
    "except:\n",
    "    collection = chroma_client.get_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(f\"âœ… Using existing collection: {collection_name}\")\n",
    "    print(f\"   Current documents: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Setting Up AWS Bedrock\n",
    "\n",
    "### Reusing Our Hybrid Approach\n",
    "Just like in the basic tutorial, we'll use AWS Bedrock for the LLM while keeping our documents local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "# Configure AWS session\n",
    "session = boto3.Session(\n",
    "    profile_name='default'  # Use your AWS profile name\n",
    ")\n",
    "\n",
    "# Create a Bedrock model instance\n",
    "try:\n",
    "    bedrock_model = BedrockModel(\n",
    "        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        boto_session=session\n",
    "    )\n",
    "    print(\"âœ… AWS Bedrock configured successfully!\")\n",
    "    print(\"   Model: Claude 3.7 Sonnet\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error configuring Bedrock: {e}\")\n",
    "    print(\"   Please check your AWS credentials and Bedrock access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 3: Advanced Document Processing\n",
    "\n",
    "### Multi-Format Document Handler\n",
    "We'll create a sophisticated document processor that can:\n",
    "- Extract text from PDFs (including multi-page)\n",
    "- Read Word documents with formatting\n",
    "- Process markdown and plain text\n",
    "- Handle encoding issues gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from docx import Document as DocxDocument\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "import hashlib\n",
    "\n",
    "class AdvancedDocumentProcessor:\n",
    "    \"\"\"Process various document types with advanced features.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text and metadata from PDF files.\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\n",
    "            \"source_type\": \"pdf\",\n",
    "            \"page_count\": 0,\n",
    "            \"file_path\": pdf_path\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                metadata[\"page_count\"] = len(pdf_reader.pages)\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += f\"\\n\\n[Page {page_num + 1}]\\n{page_text}\"\n",
    "                \n",
    "                # Try to get document info\n",
    "                if pdf_reader.metadata:\n",
    "                    metadata[\"title\"] = getattr(pdf_reader.metadata, 'title', 'Unknown')\n",
    "                    metadata[\"author\"] = getattr(pdf_reader.metadata, 'author', 'Unknown')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF: {e}\")\n",
    "            text = f\"Error extracting text from PDF: {str(e)}\"\n",
    "            \n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_docx(docx_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text and metadata from Word documents.\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\n",
    "            \"source_type\": \"docx\",\n",
    "            \"paragraph_count\": 0,\n",
    "            \"file_path\": docx_path\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            doc = DocxDocument(docx_path)\n",
    "            \n",
    "            # Extract paragraphs\n",
    "            paragraphs = []\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    paragraphs.append(para.text)\n",
    "            \n",
    "            text = \"\\n\\n\".join(paragraphs)\n",
    "            metadata[\"paragraph_count\"] = len(paragraphs)\n",
    "            \n",
    "            # Extract tables if any\n",
    "            if doc.tables:\n",
    "                metadata[\"table_count\"] = len(doc.tables)\n",
    "                text += \"\\n\\n[Tables found in document]\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DOCX: {e}\")\n",
    "            text = f\"Error extracting text from DOCX: {str(e)}\"\n",
    "            \n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text from any supported file type.\"\"\"\n",
    "        path = Path(file_path)\n",
    "        \n",
    "        if not path.exists():\n",
    "            return \"\", {\"error\": \"File not found\"}\n",
    "        \n",
    "        # Determine file type and process accordingly\n",
    "        suffix = path.suffix.lower()\n",
    "        \n",
    "        if suffix == '.pdf':\n",
    "            return AdvancedDocumentProcessor.extract_text_from_pdf(file_path)\n",
    "        elif suffix in ['.docx', '.doc']:\n",
    "            return AdvancedDocumentProcessor.extract_text_from_docx(file_path)\n",
    "        elif suffix in ['.txt', '.md', '.markdown']:\n",
    "            try:\n",
    "                text = path.read_text(encoding='utf-8')\n",
    "                metadata = {\n",
    "                    \"source_type\": \"text\",\n",
    "                    \"file_path\": file_path,\n",
    "                    \"char_count\": len(text)\n",
    "                }\n",
    "                return text, metadata\n",
    "            except Exception as e:\n",
    "                return f\"Error reading file: {str(e)}\", {\"error\": str(e)}\n",
    "        else:\n",
    "            return \"\", {\"error\": f\"Unsupported file type: {suffix}\"}\n",
    "\n",
    "# Test the processor\n",
    "processor = AdvancedDocumentProcessor()\n",
    "print(\"ğŸ“„ Document processor ready!\")\n",
    "print(\"   Supported formats: PDF, DOCX, TXT, MD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 4: Smart Text Chunking\n",
    "\n",
    "### Why Chunking Matters\n",
    "Large documents need to be split into smaller, meaningful pieces:\n",
    "- **Context Preservation**: Each chunk should be self-contained\n",
    "- **Optimal Size**: Not too small (loses context) or too large (poor retrieval)\n",
    "- **Overlap**: Ensures information at boundaries isn't lost\n",
    "\n",
    "### Advanced Chunking Strategy\n",
    "We'll implement semantic chunking that respects:\n",
    "- Paragraph boundaries\n",
    "- Sentence completion\n",
    "- Maximum token limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from typing import List\n",
    "\n",
    "class SmartTextChunker:\n",
    "    \"\"\"Intelligent text chunking with overlap and boundary respect.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50,\n",
    "                 encoding_name: str = \"cl100k_base\"):\n",
    "        \"\"\"Initialize the chunker with configurable parameters.\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def split_text_into_chunks(self, text: str) -> List[Dict[str, any]]:\n",
    "        \"\"\"Split text into semantic chunks with metadata.\"\"\"\n",
    "        # First, split by paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = self.count_tokens(para)\n",
    "            \n",
    "            # If paragraph is too large, split by sentences\n",
    "            if para_tokens > self.chunk_size:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "                for sentence in sentences:\n",
    "                    sentence_tokens = self.count_tokens(sentence)\n",
    "                    \n",
    "                    if current_tokens + sentence_tokens > self.chunk_size:\n",
    "                        # Save current chunk\n",
    "                        if current_chunk:\n",
    "                            chunks.appen
