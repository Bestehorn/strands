{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming and Real-Time Responses\n",
    "\n",
    "**Building Interactive AI Agents with Live Streaming Capabilities**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this comprehensive tutorial on **streaming responses** in the Strands framework! This notebook demonstrates how to build AI agents that provide real-time, streaming responses for enhanced user experiences. By the end of this 10-minute tutorial, you'll master the art of creating responsive, interactive AI applications.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Understand streaming vs. traditional responses\n",
    "- Implement real-time streaming agents\n",
    "- Build interactive chat interfaces\n",
    "- Handle streaming events and errors\n",
    "- Optimize for user experience\n",
    "- Create progress-aware applications\n",
    "\n",
    "### ‚ö° Why Streaming Matters\n",
    "\n",
    "Streaming responses provide:\n",
    "- **Instant feedback** - Users see progress immediately\n",
    "- **Better UX** - No waiting for complete responses\n",
    "- **Lower latency** - First tokens arrive quickly\n",
    "- **Interruptibility** - Users can stop generation\n",
    "- **Progress tracking** - Monitor response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Installing Required Packages\n",
    "\n",
    "### Overview\n",
    "Let's install the necessary packages for streaming functionality.\n",
    "\n",
    "### üìö Packages We'll Install\n",
    "- **strands-agents**: Core framework with streaming support\n",
    "- **asyncio**: For asynchronous streaming\n",
    "- **IPython.display**: For real-time notebook updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install strands-agents strands-agents-tools strands-agents-builder -q\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"   Ready to build streaming agents! ‚ö°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Step 2: Setting Up AWS Authentication\n",
    "\n",
    "### Overview\n",
    "We'll configure AWS Bedrock with streaming support enabled.\n",
    "\n",
    "### üîë Authentication Options\n",
    "1. **AWS Profile** (Recommended for development)\n",
    "2. **Environment Variables**\n",
    "3. **Direct Credentials** (Less secure)\n",
    "4. **IAM Roles** (Recommended for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "import asyncio\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configure AWS session\n",
    "session = boto3.Session(\n",
    "    # aws_access_key_id='your_access_key',\n",
    "    # aws_secret_access_key='your_secret_key',\n",
    "    # aws_session_token='your_session_token',  # If using temporary credentials\n",
    "    # region_name='us-west-2',\n",
    "    profile_name='default'  # Optional: Use a specific AWS profile\n",
    ")\n",
    "\n",
    "# Create a Bedrock model instance with streaming support\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    boto_session=session\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AWS Bedrock configured successfully!\")\n",
    "print(f\"   Model: Claude 3.7 Sonnet\")\n",
    "print(f\"   Profile: {session.profile_name}\")\n",
    "print(\"   Streaming: Enabled ‚ö°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Step 3: Understanding Streaming vs. Traditional Responses\n",
    "\n",
    "### The Difference\n",
    "Let's compare traditional and streaming responses to understand the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional response (all at once)\n",
    "traditional_agent = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"You are a helpful assistant. Be concise.\"\n",
    ")\n",
    "\n",
    "print(\"üîÑ Traditional Response (wait for complete response):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "response = traditional_agent(\"Explain the benefits of streaming in 3 points\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(response)\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"   ‚ùå User waited for entire response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 4: Creating a Streaming Agent\n",
    "\n",
    "### Real-Time Responses\n",
    "Now let's create an agent that streams responses token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming agent\n",
    "streaming_agent = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a helpful streaming assistant.\n",
    "    Provide clear, structured responses that work well with streaming.\n",
    "    Use bullet points and clear sections when appropriate.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚ö° Streaming Agent created!\")\n",
    "print(\"   Type: Real-time streaming\")\n",
    "print(\"   Benefits: Instant feedback, better UX\")\n",
    "\n",
    "# Function to handle streaming\n",
    "async def stream_response(agent, prompt):\n",
    "    \"\"\"Stream response from agent token by token\"\"\"\n",
    "    print(\"\\nüåä Streaming Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    collected_response = \"\"\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    \n",
    "    # Stream the response\n",
    "    async for chunk in agent.astream(prompt):\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time()\n",
    "        \n",
    "        # In a real application, you'd update the UI here\n",
    "        collected_response += chunk\n",
    "        print(chunk, end='', flush=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n\\n‚è±Ô∏è  Time to first token: {first_token_time - start_time:.2f} seconds\")\n",
    "    print(f\"   Total time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"   ‚úÖ User saw progress immediately!\")\n",
    "    \n",
    "    return collected_response\n",
    "\n",
    "# Test streaming\n",
    "await stream_response(streaming_agent, \"Explain the benefits of streaming in 3 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 5: Building an Interactive Chat Interface\n",
    "\n",
    "### Live Chat Experience\n",
    "Let's create a chat interface that updates in real-time as the agent responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatInterface:\n",
    "    \"\"\"Interactive chat interface with streaming responses\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    async def chat(self, user_input):\n",
    "        \"\"\"Handle a chat interaction with streaming\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Display user message\n",
    "        display(HTML(f\"<div style='background-color:#e3f2fd; padding:10px; margin:5px; border-radius:5px;'>\"\n",
    "                    f\"<strong>You:</strong> {user_input}</div>\"))\n",
    "        \n",
    "        # Create placeholder for assistant response\n",
    "        response_html = \"<div style='background-color:#f5f5f5; padding:10px; margin:5px; border-radius:5px;'>\"\n",
    "        response_html += \"<strong>Assistant:</strong> <span id='response'></span>\"\n",
    "        response_html += \"<span id='cursor' style='animation: blink 1s infinite;'>‚ñä</span></div>\"\n",
    "        response_html += \"<style>@keyframes blink { 0% {opacity: 1;} 50% {opacity: 0;} 100% {opacity: 1;} }</style>\"\n",
    "        \n",
    "        display(HTML(response_html))\n",
    "        \n",
    "        # Stream the response\n",
    "        collected_response = \"\"\n",
    "        async for chunk in self.agent.astream(user_input):\n",
    "            collected_response += chunk\n",
    "            # In a real web app, you'd update the DOM here\n",
    "            # For notebook, we'll just print\n",
    "            print(chunk, end='', flush=True)\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": collected_response})\n",
    "        \n",
    "        return collected_response\n",
    "\n",
    "# Create chat interface\n",
    "chat_interface = StreamingChatInterface(streaming_agent)\n",
    "\n",
    "print(\"üí¨ Interactive Chat Interface Ready!\")\n",
    "print(\"   Features: Real-time updates, typing indicator, conversation history\")\n",
    "\n",
    "# Test the chat interface\n",
    "await chat_interface.chat(\"What are the key advantages of streaming AI responses?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: Progress Tracking and Metrics\n",
    "\n",
    "### Monitoring Stream Performance\n",
    "Let's implement progress tracking to monitor streaming performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingMetrics:\n",
    "    \"\"\"Track streaming performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.start_time = None\n",
    "        self.first_token_time = None\n",
    "        self.end_time = None\n",
    "        self.token_count = 0\n",
    "        self.chunk_times = []\n",
    "    \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def record_chunk(self):\n",
    "        current_time = time.time()\n",
    "        if self.first_token_time is None:\n",
    "            self.first_token_time = current_time\n",
    "        self.chunk_times.append(current_time)\n",
    "        self.token_count += 1\n",
    "    \n",
    "    def end(self):\n",
    "        self.end_time = time.time()\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.start_time or not self.end_time:\n",
    "            return None\n",
    "        \n",
    "        total_time = self.end_time - self.start_time\n",
    "        time_to_first_token = self.first_token_time - self.start_time if self.first_token_time else 0\n",
    "        \n",
    "        # Calculate inter-token delays\n",
    "        inter_token_delays = []\n",
    "        for i in range(1, len(self.chunk_times)):\n",
    "            delay = self.chunk_times[i] - self.chunk_times[i-1]\n",
    "            inter_token_delays.append(delay)\n",
    "        \n",
    "        avg_delay = sum(inter_token_delays) / len(inter_token_delays) if inter_token_delays else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_time\": total_time,\n",
    "            \"time_to_first_token\": time_to_first_token,\n",
    "            \"token_count\": self.token_count,\n",
    "            \"tokens_per_second\": self.token_count / total_time if total_time > 0 else 0,\n",
    "            \"avg_inter_token_delay\": avg_delay\n",
    "        }\n",
    "\n",
    "# Test with metrics\n",
    "async def stream_with_metrics(agent, prompt):\n",
    "    \"\"\"Stream response with detailed metrics\"\"\"\n",
    "    metrics = StreamingMetrics()\n",
    "    \n",
    "    print(f\"üìä Streaming with Metrics: '{prompt}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics.start()\n",
    "    response = \"\"\n",
    "    \n",
    "    async for chunk in agent.astream(prompt):\n",
    "        metrics.record_chunk()\n",
    "        response += chunk\n",
    "        print(chunk, end='', flush=True)\n",
    "    \n",
    "    metrics.end()\n",
    "    \n",
    "    # Display metrics\n",
    "    results = metrics.get_metrics()\n",
    "    print(\"\\n\\nüìà Streaming Metrics:\")\n",
    "    print(f\"   ‚è±Ô∏è  Time to first token: {results['time_to_first_token']:.3f}s\")\n",
    "    print(f\"   ‚è±Ô∏è  Total time: {results['total_time']:.2f}s\")\n",
    "    print(f\"   üìù Tokens streamed: {results['token_count']}\")\n",
    "    print(f\"   ‚ö° Tokens per second: {results['tokens_per_second']:.1f}\")\n",
    "    print(f\"   üìä Avg inter-token delay: {results['avg_inter_token_delay']:.3f}s\")\n",
    "    \n",
    "    return response, results\n",
    "\n",
    "# Run test with metrics\n",
    "response, metrics = await stream_with_metrics(\n",
    "    streaming_agent, \n",
    "    \"Write a short story about a robot learning to paint (3 paragraphs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 7: Handling Streaming with Tools\n",
    "\n",
    "### Streaming Tool-Equipped Agents\n",
    "Let's explore how streaming works with tool-calling agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import tool\n",
    "import random\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def generate_plot_point():\n",
    "    \"\"\"Generate a random plot point for a story\"\"\"\n",
    "    plot_points = [\n",
    "        \"discovers a hidden treasure map\",\n",
    "        \"meets a mysterious stranger\",\n",
    "        \"finds an ancient artifact\",\n",
    "        \"receives an urgent message\",\n",
    "        \"witnesses something impossible\"\n",
    "    ]\n",
    "    return random.choice(plot_points)\n",
    "\n",
    "@tool\n",
    "def generate_character_name():\n",
    "    \"\"\"Generate a random character name\"\"\"\n",
    "    first_names = [\"Alex\", \"Jordan\", \"Sam\", \"Casey\", \"Morgan\"]\n",
    "    last_names = [\"Stone\", \"Rivers\", \"Sky\", \"Winter\", \"Fox\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "# Create streaming agent with tools\n",
    "streaming_tool_agent = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a creative writing assistant with tools.\n",
    "    Use the available tools to enhance your stories with dynamic elements.\n",
    "    Stream your responses for a better reading experience.\"\"\",\n",
    "    tools=[generate_plot_point, generate_character_name]\n",
    ")\n",
    "\n",
    "print(\"üõ†Ô∏è Streaming Tool Agent created!\")\n",
    "print(\"   Tools: generate_plot_point, generate_character_name\")\n",
    "print(\"   Capability: Real-time story generation with dynamic elements\")\n",
    "\n",
    "# Test streaming with tools\n",
    "print(\"\\nüìñ Generating Interactive Story...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "async for chunk in streaming_tool_agent.astream(\n",
    "    \"Write a short adventure story. Use tools to generate the main character's name and a key plot point.\"\n",
    "):\n",
    "    print(chunk, end='', flush=True)\n",
    "    # Small delay to simulate realistic streaming\n",
    "    await asyncio.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Step 8: Error Handling in Streaming\n",
    "\n",
    "### Graceful Error Management\n",
    "Streaming requires special error handling considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeStreamingAgent:\n",
    "    \"\"\"Streaming agent with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    async def safe_stream(self, prompt, max_retries=3):\n",
    "        \"\"\"Stream with error handling and retries\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"\\nüîÑ Streaming (Attempt {attempt + 1}/{max_retries})...\")\n",
    "                \n",
    "                response = \"\"\n",
    "                timeout_counter = 0\n",
    "                \n",
    "                async for chunk in self.agent.astream(prompt):\n",
    "                    # Simulate potential timeout check\n",
    "                    timeout_counter += 1\n",
    "                    if timeout_counter > 1000:  # Safety limit\n",
    "                        raise TimeoutError(\"Stream timeout\")\n",
    "                    \n",
    "                    response += chunk\n",
    "                    print(chunk, end='', flush=True)\n",
    "                \n",
    "                print(\"\\n‚úÖ Stream completed successfully!\")\n",
    "                return response\n",
    "                \n",
    "            except TimeoutError as e:\n",
    "                print(f\"\\n‚è±Ô∏è Timeout error: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(\"   Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                else:\n",
    "                    print(\"   ‚ùå Max retries reached\")\n",
    "                    return None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Streaming error: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(\"   Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                else:\n",
    "                    print(\"   ‚ùå Max retries reached\")\n",
    "                    return None\n",
    "    \n",
    "    async def stream_with_fallback(self, prompt):\n",
    "        \"\"\"Stream with fallback to non-streaming\"\"\"\n",
    "        try:\n",
    "            # Try streaming first\n",
    "            return await self.safe_stream(prompt)\n",
    "        except:\n",
    "            print(\"\\n‚ö†Ô∏è Streaming failed, falling back to traditional response...\")\n",
    "            # Fallback to non-streaming\n",
    "            return self.agent(prompt)\n",
    "\n",
    "# Create safe streaming wrapper\n",
    "safe_agent = SafeStreamingAgent(streaming_agent)\n",
    "\n",
    "# Test error handling\n",
    "response = await safe_agent.safe_stream(\"Explain error handling in streaming systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 9: Building a Real-Time Dashboard\n",
    "\n",
    "### Live Monitoring Interface\n",
    "Let's create a dashboard that shows streaming progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingDashboard:\n",
    "    \"\"\"Real-time dashboard for streaming responses\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sessions = []\n",
    "    \n",
    "    async def monitor_stream(self, agent, prompt, session_name):\n",
    "        \"\"\"Monitor a streaming session with live updates\"\"\"\n",
    "        session = {\n",
    "            \"name\": session_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"start_time\": time.time(),\n",
    "            \"chunks\": [],\n",
    "            \"status\": \"streaming\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä STREAMING DASHBOARD - {session_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìù Prompt: {prompt}\")\n",
    "        print(\"\\nüîÑ Status: STREAMING\")\n",
    "        print(\"\\nüìú Response:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        response = \"\"\n",
    "        chunk_count = 0\n",
    "        \n",
    "        try:\n",
    "            async for chunk in agent.astream(prompt):\n",
    "                chunk_count += 1\n",
    "                response += chunk\n",
    "                session[\"chunks\"].append({\n",
    "                    \"text\": chunk,\n",
    "                    \"time\": time.time() - session[\"start_time\"]\n",
    "                })\n",
    "                \n",
    "                # Display chunk\n",
    "                print(chunk, end='', flush=True)\n",
    "                \n",
    "                # Update progress bar (simulated)\n",
    "                if chunk_count % 10 == 0:\n",
    "                    elapsed = time.time() - session[\"start_time\"]\n",
    "                    print(f\"\\n[{elapsed:.1f}s - {chunk_count} chunks]\", end='')\n",
    "            \n",
    "            session[\"status\"] = \"completed\"\n",
    "            session[\"end_time\"] = time.time()\n",
    "            session[\"response\"] = response\n",
    "            \n",
    "        except Exception as e:\n",
    "            session[\"status\"] = f\"error: {e}\"\n",
    "            session[\"end_time\"] = time.time()\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(f\"\\n‚úÖ Status: {session['status'].upper()}\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {session['end_time'] - session['start_time']:.2f}s\")\n",
    "        print(f\"üì¶ Chunks received: {len(session['chunks'])}\")\n",
    "        print(f\"üìè Response length: {len(response)} characters\")\n",
    "        \n",
    "        self.sessions.append(session)\n",
    "        return session\n",
    "    \n",
    "    def get_analytics(self):\n",
    "        \"\"\"Get streaming analytics across all sessions\"\"\"\n",
    "        if not self.sessions:\n",
    "            return \"No sessions recorded\"\n",
    "        \n",
    "        total_sessions = len(self.sessions)\n",
    "        successful = sum(1 for s in self.sessions if s[\"status\"] == \"completed\")\n",
    "        avg_duration = sum(s.get(\"end_time\", 0) - s[\"start_time\"] for s in self.sessions) / total_sessions\n",
    "        \n",
    "        print(\"\\nüìà STREAMING ANALYTICS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä Total sessions: {total_sessions}\")\n",
    "        print(f\"‚úÖ Successful: {successful} ({successful/total_sessions*100:.1f}%)\")\n",
    "        print(f\"‚è±Ô∏è  Avg duration: {avg_duration:.2f}s\")\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = StreamingDashboard()\n",
    "\n",
    "# Test dashboard with a session\n",
    "session = await dashboard.monitor_stream(\n",
    "    streaming_agent,\n",
    "    \"Explain the benefits of real-time streaming dashboards\",\n",
    "    \"Dashboard Demo\"\n",
    ")\n",
    "\n",
    "# Show analytics\n",
    "dashboard.get_analytics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 10: Production Best Practices\n",
    "\n",
    "### Building Production-Ready Streaming\n",
    "Let's explore best practices for deploying streaming agents in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ PRODUCTION BEST PRACTICES FOR STREAMING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best practices guide\n",
    "best_practices = {\n",
    "    \"‚ö° Performance Optimization\": [\n",
    "        \"Implement connection pooling for API calls\",\n",
    "        \"Use WebSockets for real-time communication\",\n",
    "        \"Enable HTTP/2 for multiplexing\",\n",
    "        \"Implement client-side buffering\",\n",
    "        \"Consider CDN for global distribution\"\n",
    "    ],\n",
    "    \"üõ°Ô∏è Error Handling & Resilience\": [\n",
    "        \"Implement exponential backoff for retries\",\n",
    "        \"Add circuit breakers for failing services\",\n",
    "        \"Provide graceful degradation options\",\n",
    "        \"Log errors comprehensively\",\n",
    "        \"Monitor stream health metrics\"\n",
    "    ],\n",
    "    \"üîê Security Considerations\": [\n",
    "        \"Implement rate limiting per user\",\n",
    "        \"Add authentication for streaming endpoints\",\n",
    "        \"Validate and sanitize all inputs\",\n",
    "        \"Use HTTPS for all connections\",\n",
    "        \"Implement timeout limits\"\n",
    "    ],\n",
    "    \"üìä Monitoring & Analytics\": [\n",
    "        \"Track time to first byte (TTFB)\",\n",
    "        \"Monitor streaming completion rates\",\n",
    "        \"Log token generation speed\",\n",
    "        \"Track user engagement metrics\",\n",
    "        \"Set up alerting for anomalies\"\n",
    "    ],\n",
    "    \"üéØ User Experience\": [\n",
    "        \"Show typing indicators during streaming\",\n",
    "        \"Implement smooth text rendering\",\n",
    "        \"Add progress indicators for long responses\",\n",
    "        \"Enable stream interruption/cancellation\",\n",
    "        \"Provide fallback for poor connections\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "# Example production configuration\n",
    "print(\"\\n\\nüìã Example Production Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_config = {\n",
    "    \"streaming\": {\n",
    "        \"enabled\": True,\n",
    "        \"timeout_seconds\": 30,\n",
    "        \"max_retries\": 3,\n",
    "        \"retry_delay_seconds\": 1,\n",
    "        \"chunk_size\": \"token\",\n",
    "        \"buffer_size\": 1024\n",
    "    },\n",
    "    \"rate_limiting\": {\n",
    "        \"requests_per_minute\": 60,\n",
    "        \"concurrent_streams\": 5,\n",
    "        \"max_tokens_per_stream\": 4096\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"metrics_enabled\": True,\n",
    "        \"health_check_interval\": 60\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "In this tutorial, you've mastered:\n",
    "- ‚úÖ Understanding streaming vs. traditional responses\n",
    "- ‚úÖ Implementing real-time streaming agents\n",
    "- ‚úÖ Building interactive chat interfaces\n",
    "- ‚úÖ Creating progress tracking systems\n",
    "- ‚úÖ Handling streaming with tools\n",
    "- ‚úÖ Implementing error handling and resilience\n",
    "- ‚úÖ Building real-time dashboards\n",
    "- ‚úÖ Production best practices\n",
    "\n",
    "### ‚ö° The Power of Streaming\n",
    "\n",
    "You now have the skills to build:\n",
    "- **Interactive chat applications** with real-time responses\n",
    "- **Progress-aware interfaces** that keep users engaged\n",
    "- **Resilient streaming systems** that handle errors gracefully\n",
    "- **Production-ready deployments** with monitoring and analytics\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **User Experience First**: Streaming dramatically improves perceived performance\n",
    "2. **Error Handling is Critical**: Always implement fallbacks and retries\n",
    "3. **Monitor Everything**: Track metrics to optimize performance\n",
    "4. **Tools + Streaming**: Combine for powerful interactive applications\n",
    "\n",
    "### üîÆ Advanced Techniques\n",
    "\n",
    "Consider exploring:\n",
    "- **WebSocket Integration**: For bidirectional streaming\n",
    "- **Server-Sent Events (SSE)**: For unidirectional streaming\n",
    "- **gRPC Streaming**: For high-performance applications\n",
    "- **Reactive Streams**: For backpressure handling\n",
    "- **Stream Processing**: For real-time analytics\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Strands Documentation](https://strandsagents.com/0.1.x/)\n",
    "- [AWS Bedrock Streaming Guide](https://docs.aws.amazon.com/bedrock/)\n",
    "- [AsyncIO Documentation](https://docs.python.org/3/library/asyncio.html)\n",
    "- [Real-time Web Technologies](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)\n",
    "\n",
    "### üåü Next Steps\n",
    "\n",
    "You're ready to:\n",
    "1. Build chat applications with streaming responses\n",
    "2. Create real-time dashboards and monitoring\n",
    "3. Implement streaming in production applications\n",
    "4. Optimize performance for global scale\n",
    "5. Combine streaming with other advanced features\n",
    "\n",
    "### üöÄ Final Thoughts\n",
    "\n",
    "Streaming transforms AI applications from static Q&A systems into dynamic, interactive experiences. With the techniques you've learned, you can create applications that feel instantaneous and engaging.\n",
    "\n",
    "Remember: Great user experiences are built one stream at a time!\n",
    "\n",
    "Happy streaming! ‚ö°ü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
