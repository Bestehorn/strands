{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ollama Configuration\n",
    "\n",
    "**Optimizing Your Local AI Setup**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the advanced guide for **Ollama configuration**! This notebook dives deep into optimizing your local AI setup, understanding model quantization, hardware acceleration, and advanced features. By the end of this 10-minute tutorial, you'll be able to fine-tune your local AI environment for maximum performance.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this advanced tutorial, you will:\n",
    "- Understand model sizes and quantization levels\n",
    "- Configure GPU acceleration for faster inference\n",
    "- Create custom modelfiles for specialized behavior\n",
    "- Optimize memory usage and performance\n",
    "- Troubleshoot common issues\n",
    "- Build production-ready local AI systems\n",
    "\n",
    "### üìã Prerequisites\n",
    "\n",
    "This notebook assumes you've completed **Video 4** and have:\n",
    "- Ollama installed and running\n",
    "- Basic understanding of local models\n",
    "- At least one model downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 1: Installation Verification\n",
    "\n",
    "### Re-checking Your Setup\n",
    "Let's verify that Ollama is properly installed and provide troubleshooting guidance if needed.\n",
    "\n",
    "#### üìù Haven't installed Ollama yet?\n",
    "If you haven't installed Ollama yet, please refer to **Video 4** notebook (`04_local_models_ollama.ipynb`) which contains detailed installation instructions for:\n",
    "- ü™ü Windows (installer download and setup)\n",
    "- üçé macOS (Homebrew or direct download)\n",
    "- üêß Linux (one-line install script)\n",
    "\n",
    "The installation typically takes just 2-3 minutes and is a one-time setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import psutil  # For system monitoring\n",
    "\n",
    "def check_ollama_installation():\n",
    "    \"\"\"Check Ollama installation and provide troubleshooting guidance.\"\"\"\n",
    "    try:\n",
    "        # Try to run ollama version command\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, \n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Ollama is installed!\")\n",
    "            print(f\"   Version: {result.stdout.strip()}\")\n",
    "            \n",
    "            # Check if Ollama service is running\n",
    "            try:\n",
    "                list_result = subprocess.run(['ollama', 'list'], \n",
    "                                           capture_output=True, text=True,\n",
    "                                           shell=(platform.system() == 'Windows'))\n",
    "                if list_result.returncode == 0:\n",
    "                    print(\"‚úÖ Ollama service is running!\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Ollama is installed but service might not be running.\")\n",
    "                    print(\"   Try: 'ollama serve' in a separate terminal\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è  Could not verify Ollama service status\")\n",
    "                \n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(\"Ollama command failed\")\n",
    "            \n",
    "    except (subprocess.CalledProcessError, FileNotFoundError, Exception) as e:\n",
    "        print(\"‚ùå Ollama is not properly installed or configured.\")\n",
    "        print(f\"\\nüîß Troubleshooting Guide:\\n\")\n",
    "        \n",
    "        system = platform.system()\n",
    "        \n",
    "        print(\"1. Common Issues:\")\n",
    "        print(\"   ‚Ä¢ PATH not updated after installation\")\n",
    "        print(\"   ‚Ä¢ Service not started automatically\")\n",
    "        print(\"   ‚Ä¢ Firewall blocking Ollama\")\n",
    "        \n",
    "        if system == \"Windows\":\n",
    "            print(\"\\n2. Windows-specific fixes:\")\n",
    "            print(\"   ‚Ä¢ Restart your terminal/VS Code\")\n",
    "            print(\"   ‚Ä¢ Check Windows Services for 'Ollama'\")\n",
    "            print(\"   ‚Ä¢ Run as Administrator if needed\")\n",
    "            \n",
    "        elif system == \"Darwin\":\n",
    "            print(\"\\n2. macOS-specific fixes:\")\n",
    "            print(\"   ‚Ä¢ Check if Ollama app is running\")\n",
    "            print(\"   ‚Ä¢ Try: 'brew services restart ollama'\")\n",
    "            print(\"   ‚Ä¢ Check Activity Monitor for 'ollama'\")\n",
    "            \n",
    "        elif system == \"Linux\":\n",
    "            print(\"\\n2. Linux-specific fixes:\")\n",
    "            print(\"   ‚Ä¢ Check systemd: 'sudo systemctl status ollama'\")\n",
    "            print(\"   ‚Ä¢ Start service: 'sudo systemctl start ollama'\")\n",
    "            print(\"   ‚Ä¢ Check logs: 'journalctl -u ollama'\")\n",
    "        \n",
    "        print(\"\\n3. If still not working:\")\n",
    "        print(\"   ‚Ä¢ Reinstall Ollama from https://ollama.com\")\n",
    "        print(\"   ‚Ä¢ Check GitHub issues: https://github.com/ollama/ollama/issues\")\n",
    "        return False\n",
    "\n",
    "# Check installation\n",
    "ollama_ok = check_ollama_installation()\n",
    "\n",
    "if ollama_ok:\n",
    "    print(\"\\nüöÄ Great! Let's dive into advanced configuration!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please resolve the installation issues before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Understanding Model Quantization\n",
    "\n",
    "### What is Quantization?\n",
    "Quantization reduces model precision to save memory and increase speed. Different quantization levels offer different trade-offs:\n",
    "\n",
    "- **Q8_0**: 8-bit quantization (minimal quality loss)\n",
    "- **Q5_K_M**: 5-bit quantization (good balance)\n",
    "- **Q4_0**: 4-bit quantization (faster, some quality loss)\n",
    "- **Q2_K**: 2-bit quantization (very fast, noticeable quality loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison\n",
    "print(\"üìä MODEL QUANTIZATION GUIDE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model size data (approximate)\n",
    "model_sizes = [\n",
    "    {\"name\": \"Llama 3.1 70B\", \"fp16\": 140, \"q8\": 75, \"q5\": 48, \"q4\": 39, \"q2\": 24},\n",
    "    {\"name\": \"Llama 3.1 8B\", \"fp16\": 16, \"q8\": 8.5, \"q5\": 5.5, \"q4\": 4.5, \"q2\": 3},\n",
    "    {\"name\": \"Mistral 7B\", \"fp16\": 14, \"q8\": 7.5, \"q5\": 4.8, \"q4\": 3.9, \"q2\": 2.5},\n",
    "    {\"name\": \"Phi-3 3.8B\", \"fp16\": 7.6, \"q8\": 4, \"q5\": 2.6, \"q4\": 2.1, \"q2\": 1.3}\n",
    "]\n",
    "\n",
    "print(\"\\nüìè Model Sizes (GB) by Quantization Level:\\n\")\n",
    "print(f\"{'Model':<15} {'FP16':<8} {'Q8':<8} {'Q5':<8} {'Q4':<8} {'Q2':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for model in model_sizes:\n",
    "    print(f\"{model['name']:<15} {model['fp16']:<8} {model['q8']:<8} \"\n",
    "          f\"{model['q5']:<8} {model['q4']:<8} {model['q2']:<8}\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"   ‚Ä¢ Q8: Best quality, use if you have RAM\")\n",
    "print(\"   ‚Ä¢ Q5: Great balance for most users\")\n",
    "print(\"   ‚Ä¢ Q4: Good for limited RAM systems\")\n",
    "print(\"   ‚Ä¢ Q2: Only for very constrained systems\")\n",
    "\n",
    "# Download specific quantization\n",
    "print(\"\\nüì• To download specific quantizations:\")\n",
    "print(\"   ollama pull llama3.1:8b-instruct-q4_0\")\n",
    "print(\"   ollama pull mistral:7b-instruct-q5_K_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: GPU Acceleration Setup\n",
    "\n",
    "### Maximizing Performance with GPU\n",
    "Ollama automatically uses GPU if available, but let's check and optimize your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_support():\n",
    "    \"\"\"Check GPU support and provide optimization tips.\"\"\"\n",
    "    print(\"üéÆ GPU ACCELERATION CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    system = platform.system()\n",
    "    \n",
    "    # Check for NVIDIA GPU\n",
    "    try:\n",
    "        nvidia_result = subprocess.run(['nvidia-smi'], \n",
    "                                     capture_output=True, text=True,\n",
    "                                     shell=(system == 'Windows'))\n",
    "        \n",
    "        if nvidia_result.returncode == 0:\n",
    "            print(\"‚úÖ NVIDIA GPU detected!\")\n",
    "            print(\"\\nüìä GPU Information:\")\n",
    "            # Parse basic info from nvidia-smi\n",
    "            lines = nvidia_result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'NVIDIA' in line and 'Driver' in line:\n",
    "                    print(f\"   {line.strip()}\")\n",
    "            \n",
    "            print(\"\\n‚ö° GPU Optimization Tips:\")\n",
    "            print(\"   ‚Ä¢ Ollama will automatically use your GPU\")\n",
    "            print(\"   ‚Ä¢ For multiple GPUs: Set CUDA_VISIBLE_DEVICES\")\n",
    "            print(\"   ‚Ä¢ Monitor GPU usage with 'nvidia-smi -l 1'\")\n",
    "            return True\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check for AMD GPU (ROCm)\n",
    "    try:\n",
    "        if system == \"Linux\":\n",
    "            rocm_result = subprocess.run(['rocm-smi'], \n",
    "                                       capture_output=True, text=True)\n",
    "            if rocm_result.returncode == 0:\n",
    "                print(\"‚úÖ AMD GPU detected (ROCm)!\")\n",
    "                print(\"   Ollama supports AMD GPUs on Linux\")\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check for Apple Silicon\n",
    "    if system == \"Darwin\":\n",
    "        try:\n",
    "            # Check if running on Apple Silicon\n",
    "            arch_result = subprocess.run(['uname', '-m'], \n",
    "                                       capture_output=True, text=True)\n",
    "            if 'arm64' in arch_result.stdout:\n",
    "                print(\"‚úÖ Apple Silicon detected!\")\n",
    "                print(\"   ‚Ä¢ Metal Performance Shaders enabled\")\n",
    "                print(\"   ‚Ä¢ Unified memory architecture benefits\")\n",
    "                print(\"   ‚Ä¢ Excellent performance for local models\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"‚ùå No GPU acceleration detected\")\n",
    "    print(\"\\nüí° CPU-Only Optimization:\")\n",
    "    print(\"   ‚Ä¢ Use smaller models (3B-7B parameters)\")\n",
    "    print(\"   ‚Ä¢ Use Q4 or Q5 quantization\")\n",
    "    print(\"   ‚Ä¢ Close other applications to free RAM\")\n",
    "    print(\"   ‚Ä¢ Consider cloud models for heavy tasks\")\n",
    "    return False\n",
    "\n",
    "# Check GPU support\n",
    "has_gpu = check_gpu_support()\n",
    "\n",
    "# Memory recommendations\n",
    "print(\"\\nüíæ MEMORY RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMinimum RAM for smooth operation:\")\n",
    "print(\"   ‚Ä¢ 3B models: 8GB RAM (4GB with Q4)\")\n",
    "print(\"   ‚Ä¢ 7B models: 16GB RAM (8GB with Q4)\")\n",
    "print(\"   ‚Ä¢ 13B models: 32GB RAM (16GB with Q4)\")\n",
    "print(\"   ‚Ä¢ 70B models: 64GB+ RAM (32GB with Q2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 4: Custom Modelfiles\n",
    "\n",
    "### Creating Specialized Models\n",
    "Modelfiles let you create custom versions of models with specific parameters, prompts, and behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Modelfile creation\n",
    "print(\"üîß CUSTOM MODELFILE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a custom coding assistant modelfile\n",
    "coding_modelfile = \"\"\"\n",
    "# Coding Assistant Modelfile\n",
    "FROM mistral\n",
    "\n",
    "# Set custom parameters\n",
    "PARAMETER temperature 0.2\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "# System prompt for coding\n",
    "SYSTEM \"\"\"You are an expert programmer with deep knowledge of multiple programming languages. \n",
    "You provide clean, efficient, and well-commented code. \n",
    "Always explain your code and suggest best practices.\"\"\"\n",
    "\n",
    "# Template for conversations\n",
    "TEMPLATE \"\"\"{{ .System }}\n",
    "User: {{ .Prompt }}\n",
    "Assistant: \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Save modelfile\n",
    "with open('coding_assistant.modelfile', 'w') as f:\n",
    "    f.write(coding_modelfile)\n",
    "\n",
    "print(\"üìù Created: coding_assistant.modelfile\")\n",
    "print(\"\\nTo create this model:\")\n",
    "print(\"   ollama create coding-assistant -f coding_assistant.modelfile\")\n",
    "\n",
    "# Creative writing modelfile\n",
    "creative_modelfile = \"\"\"\n",
    "# Creative Writing Assistant\n",
    "FROM llama3.2\n",
    "\n",
    "# Creative parameters\n",
    "PARAMETER temperature 0.8\n",
    "PARAMETER top_p 0.95\n",
    "PARAMETER repeat_penalty 1.0\n",
    "\n",
    "SYSTEM \"\"\"You are a creative writing assistant with a vivid imagination. \n",
    "You help craft engaging stories, develop characters, and create immersive worlds. \n",
    "Be descriptive, creative, and original in your responses.\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Fast response modelfile\n",
    "fast_modelfile = \"\"\"\n",
    "# Fast Response Assistant\n",
    "FROM phi3:mini\n",
    "\n",
    "# Speed-optimized parameters\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER num_predict 150\n",
    "PARAMETER stop \"User:\"\n",
    "PARAMETER stop \"\\n\\n\"\n",
    "\n",
    "SYSTEM \"\"\"You are a quick-response assistant. \n",
    "Provide concise, direct answers without unnecessary elaboration.\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìö Key Modelfile Parameters:\")\n",
    "print(\"   ‚Ä¢ temperature: Controls randomness (0.0-1.0)\")\n",
    "print(\"   ‚Ä¢ top_p: Nucleus sampling threshold\")\n",
    "print(\"   ‚Ä¢ repeat_penalty: Reduces repetition\")\n",
    "print(\"   ‚Ä¢ num_predict: Max tokens to generate\")\n",
    "print(\"   ‚Ä¢ num_ctx: Context window size\")\n",
    "print(\"   ‚Ä¢ num_gpu: Number of layers on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Step 5: Performance Tuning\n",
    "\n",
    "### Optimizing Ollama for Your System\n",
    "Let's explore environment variables and settings that can dramatically improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéõÔ∏è PERFORMANCE TUNING GUIDE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Environment variables for optimization\n",
    "env_vars = [\n",
    "    {\n",
    "        \"var\": \"OLLAMA_NUM_PARALLEL\",\n",
    "        \"default\": \"1\",\n",
    "        \"desc\": \"Number of parallel requests\",\n",
    "        \"tip\": \"Set to 2-4 for multi-user scenarios\"\n",
    "    },\n",
    "    {\n",
    "        \"var\": \"OLLAMA_MAX_LOADED_MODELS\",\n",
    "        \"default\": \"1\",\n",
    "        \"desc\": \"Max models kept in memory\",\n",
    "        \"tip\": \"Increase if switching between models frequently\"\n",
    "    },\n",
    "    {\n",
    "        \"var\": \"OLLAMA_KEEP_ALIVE\",\n",
    "        \"default\": \"5m\",\n",
    "        \"desc\": \"How long to keep models loaded\",\n",
    "        \"tip\": \"Set to '-1' to keep models always loaded\"\n",
    "    },\n",
    "    {\n",
    "        \"var\": \"OLLAMA_HOST\",\n",
    "        \"default\": \"127.0.0.1:11434\",\n",
    "        \"desc\": \"API endpoint\",\n",
    "        \"tip\": \"Change for network access\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüîß Environment Variables:\\n\")\n",
    "for var in env_vars:\n",
    "    print(f\"üìå {var['var']}\")\n",
    "    print(f\"   Default: {var['default']}\")\n",
    "    print(f\"   Purpose: {var['desc']}\")\n",
    "    print(f\"   üí° Tip: {var['tip']}\")\n",
    "    print()\n",
    "\n",
    "# Platform-specific setup\n",
    "system = platform.system()\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Platform-Specific Setup:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if system == \"Windows\":\n",
    "    print(\"\\nü™ü Windows:\")\n",
    "    print(\"   Set environment variables in System Properties\")\n",
    "    print(\"   Or use PowerShell:\")\n",
    "    print(\"   $env:OLLAMA_NUM_PARALLEL = '2'\")\n",
    "    \n",
    "elif system == \"Darwin\":\n",
    "    print(\"\\nüçé macOS:\")\n",
    "    print(\"   Add to ~/.zshrc or ~/.bash_profile:\")\n",
    "    print(\"   export OLLAMA_NUM_PARALLEL=2\")\n",
    "    print(\"   export OLLAMA_KEEP_ALIVE=-1\")\n",
    "    \n",
    "elif system == \"Linux\":\n",
    "    print(\"\\nüêß Linux:\")\n",
    "    print(\"   Edit /etc/systemd/system/ollama.service:\")\n",
    "    print(\"   Environment='OLLAMA_NUM_PARALLEL=2'\")\n",
    "    print(\"   Then: sudo systemctl daemon-reload\")\n",
    "\n",
    "# Memory optimization tips\n",
    "print(\"\\nüíæ Memory Optimization:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\n1. Model Loading Strategy:\")\n",
    "print(\"   ‚Ä¢ Single model: Keep loaded for speed\")\n",
    "print(\"   ‚Ä¢ Multiple models: Use shorter keep_alive\")\n",
    "print(\"   ‚Ä¢ Limited RAM: Load on demand only\")\n",
    "\n",
    "print(\"\\n2. Context Window Tuning:\")\n",
    "print(\"   ‚Ä¢ Default: 2048 tokens\")\n",
    "print(\"   ‚Ä¢ Reduce for faster responses\")\n",
    "print(\"   ‚Ä¢ Increase for longer conversations\")\n",
    "print(\"   ‚Ä¢ Set in modelfile: PARAMETER num_ctx 4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ Step 6: Benchmarking Your Setup\n",
    "\n",
    "### Measuring Performance\n",
    "Let's create a simple benchmark to test your system's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import OllamaModel\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "def benchmark_model(model_name, num_runs=3):\n",
    "    \"\"\"Benchmark a model's performance.\"\"\"\n",
    "    print(f\"\\nüèÉ Benchmarking {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create agent\n",
    "        agent = Agent(\n",
    "            model=OllamaModel(model_id=model_name),\n",
    "            system_prompt=\"You are a benchmark assistant. Be concise.\"\n",
    "        )\n",
    "        \n",
    "        # Test prompts\n",
    "        prompts = [\n",
    "            \"What is 2+2?\",\n",
    "            \"Write a haiku about AI.\",\n",
    "            \"Explain quantum computing in one sentence.\"\n",
    "        ]\n",
    "        \n",
    "        times = []\n",
    "        tokens_per_second = []\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"   Test {i+1}: {prompt[:30]}...\")\n",
    "            \n",
    "            start = time.time()\n",
    "            response = agent(prompt)\n",
    "            end = time.time()\n",
    "            \n",
    "            duration = end - start\n",
    "            times.append(duration)\n",
    "            \n",
    "            # Estimate tokens (rough)\n",
    "            response_length = len(str(response).split())\n",
    "            tps = response_length / duration\n",
    "            tokens_per_second.append(tps)\n",
    "            \n",
    "            print(f\"      Time: {duration:.2f}s, ~{tps:.1f} tokens/sec\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_time = statistics.mean(times)\n",
    "        avg_tps = statistics.mean(tokens_per_second)\n",
    "        \n",
    "        print(f\"\\n   üìä Results for {model_name}:\")\n",
    "        print(f\"      Average response time: {avg_time:.2f}s\")\n",
    "        print(f\"      Average tokens/second: {avg_tps:.1f}\")\n",
    "        \n",
    "        return avg_time, avg_tps\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error benchmarking {model_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"üèÉ SYSTEM BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get system info\n",
    "try:\n",
    "    import psutil\n",
    "    print(\"\\nüíª System Information:\")\n",
    "    print(f\"   CPU: {psutil.cpu_count()} cores\")\n",
    "    print(f\"   RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    print(f\"   Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "except:\n",
    "    print(\"   Install psutil for system info: pip install psutil\")\n",
    "\n",
    "# List available models for benchmarking\n",
    "print(\"\\nüìã Available models for benchmarking:\")\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], \n",
    "                          capture_output=True, text=True,\n",
    "                          shell=(platform.system() == 'Windows'))\n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Benchmark the smallest model available\n",
    "        print(\"\\nüöÄ Starting benchmark (this may take a minute)...\")\n",
    "        \n",
    "        # Try to benchmark phi3:mini as it's fast\n",
    "        benchmark_model(\"phi3:mini\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Benchmark error: {e}\")\n",
    "\n",
    "print(\"\\nüí° Performance Tips Based on Results:\")\n",
    "print(\"   ‚Ä¢ <1s response: Excellent for real-time apps\")\n",
    "print(\"   ‚Ä¢ 1-3s response: Good for interactive use\")\n",
    "print(\"   ‚Ä¢ >3s response: Consider smaller model or GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî® Step 7: Advanced Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions\n",
    "Let's diagnose and fix common Ollama problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_ollama():\n",
    "    \"\"\"Comprehensive Ollama diagnostics.\"\"\"\n",
    "    print(\"üî® OLLAMA DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues_found = []\n",
    "    \n",
    "    # 1. Check Ollama version\n",
    "    print(\"\\n1Ô∏è‚É£ Checking Ollama version...\")\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True,\n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ‚úÖ {result.stdout.strip()}\")\n",
    "        else:\n",
    "            issues_found.append(\"Ollama not in PATH\")\n",
    "            print(\"   ‚ùå Ollama not found in PATH\")\n",
    "    except:\n",
    "        issues_found.append(\"Cannot execute ollama command\")\n",
    "        print(\"   ‚ùå Cannot execute ollama command\")\n",
    "    \n",
    "    # 2. Check service status\n",
    "    print(\"\\n2Ô∏è‚É£ Checking Ollama service...\")\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"   ‚úÖ Ollama service is running\")\n",
    "        else:\n",
    "            issues_found.append(\"Ollama service not responding\")\n",
    "            print(\"   ‚ùå Ollama service not responding\")\n",
    "    except:\n",
    "        issues_found.append(\"Cannot connect to Ollama API\")\n",
    "        print(\"   ‚ùå Cannot connect to Ollama API (http://localhost:11434)\")\n",
    "    \n",
    "    # 3. Check models\n",
    "    print(\"\\n3Ô∏è‚É£ Checking installed models...\")\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True,\n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        if result.returncode == 0:\n",
    "            models = result.stdout.strip().split('\\n')\n",
    "            if len(models) > 1:  # Header + at least one model\n",
    "                print(f\"   ‚úÖ {len(models)-1} models installed\")\n",
    "            else:\n",
    "                issues_found.append(\"No models installed\")\n",
    "                print(\"   ‚ùå No models installed\")\n",
    "    except:\n",
    "        issues_found.append(\"Cannot list models\")\n",
    "        print(\"   ‚ùå Cannot list models\")\n",
    "    \n",
    "    # 4. Check disk space\n",
    "    print(\"\\n4Ô∏è‚É£ Checking disk space...\")\n",
    "    try:\n",
    "        import shutil\n",
    "        total, used, free = shutil.disk_usage(\"/\")\n",
    "        free_gb = free / (1024**3)\n",
    "        if free_gb > 10:\n",
    "            print(f\"   ‚úÖ {free_gb:.1f} GB free\")\n",
    "        else:\n",
    "            issues_found.append(f\"Low disk space: {free_gb:.1f} GB\")\n",
    "            print(f\"   ‚ö†Ô∏è  Low disk space: {free_gb:.1f} GB\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  Could not check disk space\")\n",
    "    \n",
    "    # Provide solutions\n",
    "    if issues_found:\n",
    "        print(\"\\n\\nüîß SOLUTIONS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for issue in issues_found:\n",
    "            print(f\"\\n‚ùå Issue: {issue}\")\n",
    "            \n",
    "            if \"PATH\" in issue:\n",
    "                print(\"   Solution: Restart your terminal/IDE after installation\")\n",
    "            elif \"service\" in issue or \"API\" in issue:\n",
    "                print(\"   Solution: Start Ollama service\")\n",
    "                if platform.system() == \"Windows\":\n",
    "                    print(\"   - Check Windows Services\")\n",
    "                elif platform.system() == \"Darwin\":\n",
    "                    print(\"   - Open Ollama from Applications\")\n",
    "                else:\n",
    "                    print(\"   - Run: sudo systemctl start ollama\")\n",
    "            elif \"No models\" in issue:\n",
    "                print(\"   Solution: Download a model\")\n",
    "                print(\"   - Run: ollama pull llama3.2\")\n",
    "            elif \"disk space\" in issue:\n",
    "                print(\"   Solution: Free up disk space\")\n",
    "                print(\"   - Models need 3-10 GB each\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All diagnostics passed! Ollama is ready to use.\")\n",
    "    \n",
    "    return len(issues_found) == 0\n",
    "\n",
    "# Run diagnostics\n",
    "ollama_healthy = diagnose_ollama()\n",
    "\n",
    "if not ollama_healthy:\n",
    "    print(\"\\nüí° Fix the issues above before proceeding with advanced configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 8: Network Configuration\n",
    "\n",
    "### Sharing Your Local AI\n",
    "Want to access Ollama from other devices or share with teammates? Let's configure network access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê NETWORK CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîß To enable network access:\")\n",
    "print(\"\\n1. Set OLLAMA_HOST environment variable:\")\n",
    "print(\"   OLLAMA_HOST=0.0.0.0:11434\")\n",
    "\n",
    "print(\"\\n2. Restart Ollama service\")\n",
    "\n",
    "print(\"\\n3. Access from other devices:\")\n",
    "print(\"   http://YOUR_IP_ADDRESS:11434\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Security Considerations:\")\n",
    "print(\"   ‚Ä¢ Only expose on trusted networks\")\n",
    "print(\"   ‚Ä¢ Consider using a reverse proxy\")\n",
    "print(\"   ‚Ä¢ Implement authentication if needed\")\n",
    "print(\"   ‚Ä¢ Use firewall rules to restrict access\")\n",
    "\n",
    "# Get local IP address\n",
    "try:\n",
    "    import socket\n",
    "    hostname = socket.gethostname()\n",
    "    local_ip = socket.gethostbyname(hostname)\n",
    "    print(f\"\\nüíª Your local IP: {local_ip}\")\n",
    "    print(f\"   After configuration, access at: http://{local_ip}:11434\")\n",
    "except:\n",
    "    print(\"\\nüíª Could not determine local IP address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 9: Production Best Practices\n",
    "\n",
    "### Building Reliable Local AI Systems\n",
    "Here are essential practices for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ PRODUCTION BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_tips = {\n",
    "    \"üîí Security\": [\n",
    "        \"Keep Ollama updated regularly\",\n",
    "        \"Use firewall rules for network access\",\n",
    "        \"Monitor access logs\",\n",
    "        \"Run with minimal privileges\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Use GPU acceleration when available\",\n",
    "        \"Choose appropriate quantization levels\",\n",
    "        \"Monitor memory usage\",\n",
    "        \"Implement request queuing\"\n",
    "    ],\n",
    "    \"üõ°Ô∏è Reliability\": [\n",
    "        \"Set up automatic restarts\",\n",
    "        \"Monitor service health\",\n",
    "        \"Implement graceful error handling\",\n",
    "        \"Use model fallbacks\"\n",
    "    ],\n",
    "    \"üìä Monitoring\": [\n",
    "        \"Track response times\",\n",
    "        \"Monitor token usage\",\n",
    "        \"Log errors and warnings\",\n",
    "        \"Set up alerts for issues\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tips in production_tips.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for tip in tips:\n",
    "        print(f\"   ‚Ä¢ {tip}\")\n",
    "\n",
    "print(\"\\n\\nüìù Example Production Setup:\")\n",
    "print(\"=\"*40)\n",
    "print(\"\\n1. SystemD service with auto-restart\")\n",
    "print(\"2. Nginx reverse proxy with SSL\")\n",
    "print(\"3. Prometheus metrics collection\")\n",
    "print(\"4. Grafana dashboard for monitoring\")\n",
    "print(\"5. Automated model updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Mastered\n",
    "In this advanced tutorial, you've learned:\n",
    "- ‚úÖ Model quantization and optimization\n",
    "- ‚úÖ GPU acceleration configuration\n",
    "- ‚úÖ Custom modelfile creation\n",
    "- ‚úÖ Performance tuning techniques\n",
    "- ‚úÖ Advanced troubleshooting\n",
    "- ‚úÖ Production deployment practices\n",
    "\n",
    "### üöÄ Your Local AI Arsenal\n",
    "\n",
    "You now have the skills to:\n",
    "- Optimize models for your hardware\n",
    "- Create specialized AI assistants\n",
    "- Diagnose and fix common issues\n",
    "- Deploy production-ready local AI\n",
    "\n",
    "### üí° Advanced Tips\n",
    "\n",
    "1. **Experiment with Quantization**: Find the sweet spot between quality and speed\n",
    "2. **Create Model Libraries**: Build a collection of specialized models\n",
    "3. **Automate Everything**: Scripts for model updates and health checks\n",
    "4. **Join the Community**: Share your modelfiles and learn from others\n",
    "\n",
    "### üìö Further Resources\n",
    "\n",
    "- [Ollama Documentation](https://ollama.com/docs)\n",
    "- [Ollama GitHub](https://github.com/ollama/ollama)\n",
    "- [Model Library](https://ollama.com/library)\n",
    "- [Community Discord](https://discord.gg/ollama)\n",
    "\n",
    "### üåü Final Thoughts\n",
    "\n",
    "With these advanced skills, you're ready to build sophisticated local AI systems that are:\n",
    "- **Fast**: Optimized for your hardware\n",
    "- **Private**: Your data stays yours\n",
    "- **Reliable**: Production-ready configurations\n",
    "- **Flexible**: Custom models for any use case\n",
    "\n",
    "Keep experimenting, and remember: the best AI is the one that works for YOUR needs!\n",
    "\n",
    "Happy building with your supercharged local AI setup! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
