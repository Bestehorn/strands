{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Providers: Running AI Locally with Ollama\n",
    "\n",
    "**Free, Private, and Powerful: Your Agents on Your Machine**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the world of **local AI models**! This notebook demonstrates how to run powerful AI agents entirely on your own machine using Ollama and the Strands Agents SDK. By the end of this 10-minute tutorial, you'll be able to create agents that run completely offline, cost nothing after setup, and keep your data 100% private.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this hands-on tutorial, you will:\n",
    "- Set up Ollama for local model execution\n",
    "- Create Strands agents using local models\n",
    "- Compare different open-source models\n",
    "- Understand when to use local vs cloud models\n",
    "- Build privacy-first AI applications\n",
    "- Save money on AI development\n",
    "\n",
    "### üè† Why Run Models Locally?\n",
    "\n",
    "Running AI models locally offers several advantages:\n",
    "- **üí∞ Cost**: Zero API fees after initial setup\n",
    "- **üîí Privacy**: Your data never leaves your machine\n",
    "- **üåê Offline**: Works without internet connection\n",
    "- **üõ†Ô∏è Control**: Full control over model \n",
    "\n",
    "Disadvantage:\n",
    "- **‚ö° Speed**: LLMs require highly-optimized hardware, normal PCs will be very slow\n",
    "- **üí∞ Cost**: For the cost of decent GPU-based hardware, you can create **trillions of tokens**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Pre-Setup: Installing Ollama and Required Packages\n",
    "\n",
    "### üöÄ Installing Ollama (One-Time Setup)\n",
    "\n",
    "Before we begin, you need to install Ollama on your system. This is a one-time setup that enables local AI model execution. Choose your platform below:\n",
    "\n",
    "#### ü™ü Windows Installation\n",
    "1. **Download the Installer**\n",
    "   - Visit [ollama.com](https://ollama.com)\n",
    "   - Click \"Download for Windows\"\n",
    "   - Save the `.exe` installer\n",
    "\n",
    "2. **Run the Installer**\n",
    "   - Double-click the downloaded file\n",
    "   - Follow the installation wizard\n",
    "   - Ollama will install as a Windows service\n",
    "\n",
    "3. **Verify Installation**\n",
    "   - Open a new Command Prompt or PowerShell\n",
    "   - Type: `ollama --version`\n",
    "   - You should see the version number\n",
    "\n",
    "#### üçé macOS Installation\n",
    "\n",
    "**Option 1: Using Homebrew (Recommended)**\n",
    "```bash\n",
    "# If you have Homebrew installed:\n",
    "brew install ollama\n",
    "\n",
    "# Start Ollama:\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "**Option 2: Direct Download**\n",
    "1. Visit [ollama.com](https://ollama.com)\n",
    "2. Click \"Download for macOS\"\n",
    "3. Open the downloaded `.dmg` file\n",
    "4. Drag Ollama to your Applications folder\n",
    "5. Launch Ollama from Applications\n",
    "6. You'll see the Ollama icon in your menu bar\n",
    "\n",
    "#### üêß Linux Installation\n",
    "\n",
    "**One-Line Install (Recommended)**\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "This script will:\n",
    "- Download the latest Ollama binary\n",
    "- Install it to `/usr/local/bin`\n",
    "- Set up systemd service (on supported systems)\n",
    "- Start the Ollama service\n",
    "\n",
    "**Manual Installation**\n",
    "```bash\n",
    "# Download the binary\n",
    "sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama\n",
    "\n",
    "# Make it executable\n",
    "sudo chmod +x /usr/local/bin/ollama\n",
    "\n",
    "# Start Ollama\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### ‚è±Ô∏è Installation Time\n",
    "- Download: 1-2 minutes (depending on internet speed)\n",
    "- Installation: 1-2 minutes\n",
    "- First model download: 3-5 minutes\n",
    "\n",
    "**Note**: This setup time is NOT included in our 10-minute tutorial!\n",
    "\n",
    "### üêç Installing Python Dependencies\n",
    "\n",
    "Now let's install the Strands SDK with Ollama support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Strands SDK installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install Strands with Ollama support\n",
    "%pip install strands-agents -q\n",
    "%pip install strands-agents[ollama] -q\n",
    "\n",
    "print(\"‚úÖ Strands SDK installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 1: Checking Ollama Installation\n",
    "\n",
    "### Smart Installation Checker\n",
    "This cell will check if Ollama is installed on your system and provide platform-specific installation instructions if needed.\n",
    "\n",
    "### ‚è±Ô∏è Time Note\n",
    "If you need to install Ollama, this is a one-time setup that doesn't count toward our 10-minute tutorial time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Great! Ollama is installed!\n",
      "   Version: ollama version is 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def check_ollama_installation():\n",
    "    \"\"\"Check if Ollama is installed and provide installation instructions if not.\"\"\"\n",
    "    try:\n",
    "        # Try to run ollama version command\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, \n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Great! Ollama is installed!\")\n",
    "            print(f\"   Version: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(\"Ollama command failed\")\n",
    "            \n",
    "    except (subprocess.CalledProcessError, FileNotFoundError, Exception):\n",
    "        print(\"‚ùå Ollama is not installed on your system.\")\n",
    "        print(\"\\nüìã Installation Instructions:\\n\")\n",
    "        \n",
    "        system = platform.system()\n",
    "        \n",
    "        if system == \"Darwin\":  # macOS\n",
    "            print(\"üçé macOS detected. You have two options:\\n\")\n",
    "            print(\"Option 1 - Using Homebrew (recommended):\")\n",
    "            print(\"   brew install ollama\\n\")\n",
    "            print(\"Option 2 - Direct download:\")\n",
    "            print(\"   1. Visit https://ollama.com\")\n",
    "            print(\"   2. Download the macOS installer\")\n",
    "            print(\"   3. Run the installer\")\n",
    "            print(\"   4. Start Ollama from your Applications folder\")\n",
    "            \n",
    "        elif system == \"Linux\":\n",
    "            print(\"üêß Linux detected. Install with this command:\\n\")\n",
    "            print(\"   curl -fsSL https://ollama.com/install.sh | sh\\n\")\n",
    "            print(\"After installation, Ollama will run as a service.\")\n",
    "            \n",
    "        elif system == \"Windows\":\n",
    "            print(\"ü™ü Windows detected. Installation steps:\\n\")\n",
    "            print(\"   1. Visit https://ollama.com\")\n",
    "            print(\"   2. Download the Windows installer\")\n",
    "            print(\"   3. Run the downloaded .exe file\")\n",
    "            print(\"   4. Follow the installation wizard\")\n",
    "            print(\"   5. Ollama will run as a Windows service\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unknown system: {system}\")\n",
    "            print(\"   Visit https://ollama.com for installation instructions\")\n",
    "        \n",
    "        print(\"\\n‚è∏Ô∏è  After installing Ollama, restart this notebook and continue!\")\n",
    "        return False\n",
    "\n",
    "# Check installation\n",
    "ollama_installed = check_ollama_installation()\n",
    "\n",
    "if not ollama_installed:\n",
    "    print(\"\\n‚ö° Quick tip: Installation usually takes just 2-3 minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Downloading AI Models\n",
    "\n",
    "### Smart Model Management\n",
    "We'll download three popular models for our demonstrations. Don't worry - if you already have them, we won't download them again!\n",
    "\n",
    "### üìä Models We'll Use:\n",
    "1. **Llama 3.2 (3B)** - Latest from Meta, great all-around model\n",
    "2. **Mistral (7B)** - Excellent for coding and technical tasks\n",
    "3. **Phi-3 Mini (3.8B)** - Microsoft's efficient model, great for quick responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setting up AI models for local execution...\n",
      "\n",
      "‚úÖ llama3.2 is already installed!\n",
      "\n",
      "‚úÖ mistral is already installed!\n",
      "\n",
      "‚úÖ phi3:mini is already installed!\n",
      "\n",
      "üéâ All models are ready! Let's create some agents!\n"
     ]
    }
   ],
   "source": [
    "def check_and_pull_model(model_name):\n",
    "    \"\"\"Check if a model is installed, pull if not.\"\"\"\n",
    "    try:\n",
    "        # Get list of installed models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True,\n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Error checking models: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "        installed_models = result.stdout.lower()\n",
    "        \n",
    "        # Check if model is already installed\n",
    "        if model_name.lower() in installed_models:\n",
    "            print(f\"‚úÖ {model_name} is already installed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"üì• Downloading {model_name}... (this may take a few minutes)\")\n",
    "            \n",
    "            # Pull the model\n",
    "            result = subprocess.run(['ollama', 'pull', model_name],\n",
    "                                  shell=(platform.system() == 'Windows'))\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ {model_name} downloaded successfully!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to download {model_name}\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Models to use in this tutorial\n",
    "models_to_install = [\n",
    "    \"llama3.2\",     # Meta's latest, 3B parameters\n",
    "    \"mistral\",      # Great for coding, 7B parameters\n",
    "    \"phi3:mini\"     # Microsoft's efficient model, 3.8B parameters\n",
    "]\n",
    "\n",
    "print(\"üöÄ Setting up AI models for local execution...\\n\")\n",
    "\n",
    "# Check and install each model\n",
    "all_models_ready = True\n",
    "for model in models_to_install:\n",
    "    if not check_and_pull_model(model):\n",
    "        all_models_ready = False\n",
    "    print()  # Add spacing\n",
    "\n",
    "if all_models_ready:\n",
    "    print(\"üéâ All models are ready! Let's create some agents!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some models couldn't be installed. You can continue with the available ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Optional Step 3: Viewing Available Models\n",
    "\n",
    "Let's see what models are available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available Local Models:\n",
      "==================================================\n",
      "NAME               ID              SIZE      MODIFIED    \n",
      "phi3:mini          4f2222927938    2.2 GB    6 weeks ago    \n",
      "mistral:latest     3944fe81ec14    4.1 GB    6 weeks ago    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    6 weeks ago    \n",
      "\n",
      "\n",
      "üí° Tip: Model sizes shown are compressed. They'll use ~2x RAM when running.\n"
     ]
    }
   ],
   "source": [
    "# List all available models\n",
    "print(\"üìã Available Local Models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], \n",
    "                          capture_output=True, text=True,\n",
    "                          shell=(platform.system() == 'Windows'))\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå Could not list models\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüí° Tip: Model sizes shown are compressed. They'll use ~2x RAM when running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Creating Your First Local Agent\n",
    "\n",
    "### From Cloud to Local\n",
    "Creating an agent with Ollama is just as easy as with cloud providers.\n",
    "\n",
    "### üîÑ Provider Comparison\n",
    "```python\n",
    "# Cloud (AWS Bedrock)\n",
    "from strands.models import BedrockModel\n",
    "model = BedrockModel(model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "\n",
    "# Local (Ollama)\n",
    "from strands.models import OllamaModel\n",
    "model = OllamaModel(host=\"http://localhost:11434\", model_id=\"llama3.2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Your first local AI agent is ready!\n",
      "   Model: Llama 3.2 (3B parameters)\n",
      "   Location: Running entirely on your machine\n"
     ]
    }
   ],
   "source": [
    "from strands import Agent\n",
    "from strands.models.ollama import OllamaModel\n",
    "\n",
    "ollama_host = \"http://localhost:11434\"\n",
    "\n",
    "# Create a local agent with Llama 3.2\n",
    "local_agent = Agent(\n",
    "    model=OllamaModel(\n",
    "        host=ollama_host,\n",
    "        model_id=\"llama3.2\"\n",
    "    ),\n",
    "    system_prompt=\"You are a helpful assistant running locally. Be concise and friendly.\"\n",
    ")\n",
    "\n",
    "print(\"üéâ Your first local AI agent is ready!\")\n",
    "print(\"   Model: Llama 3.2 (3B parameters)\")\n",
    "print(\"   Location: Running entirely on your machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 5: Your First Local Conversation\n",
    "\n",
    "Let's test our local agent! Notice how it responds just like cloud-based models, but everything happens on your machine. Note that the first invocation can take a long time (even minutes) and depending on your hardware, subsequent invocations still may take 20 seconds or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ You: Give me one sentence with advantages of running LLMs locally.\n",
      "\n",
      "ü§ñ Local Llama 3.2 Agent:\n",
      "--------------------------------------------------\n",
      "Running Large Language Models (LLMs) locally can provide faster inference speeds, reduced dependence on internet connectivity, and improved data privacy without sacrificing the accuracy and functionality of these powerful models.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è  Response time: 52.24 seconds\n",
      "üí° Note: First response may be slower as the model loads into memory.\n"
     ]
    }
   ],
   "source": [
    "# Test the local agent\n",
    "import time\n",
    "\n",
    "question = \"Give me one sentence with advantages of running LLMs locally.\"\n",
    "\n",
    "print(f\"üë§ You: {question}\")\n",
    "print(\"\\nü§ñ Local Llama 3.2 Agent:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "response = local_agent(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n‚è±Ô∏è  Response time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"üí° Note: First response may be slower as the model loads into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Model Switching - Same Code, Different Models\n",
    "\n",
    "### The Power of Strands\n",
    "One of the best features of Strands is how easy it is to switch between models. Let's create agents with different local models and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created agent with llama3.2\n",
      "   Meta's Llama 3.2 - Great all-around model\n",
      "‚úÖ Created agent with mistral\n",
      "   Mistral 7B - Excellent for technical tasks\n",
      "‚úÖ Created agent with phi3:mini\n",
      "   Microsoft Phi-3 - Fast and efficient\n",
      "\n",
      "üéØ Successfully created 3 different local agents!\n"
     ]
    }
   ],
   "source": [
    "# Create agents with different models\n",
    "models = {\n",
    "    \"llama3.2\": \"Meta's Llama 3.2 - Great all-around model\",\n",
    "    \"mistral\": \"Mistral 7B - Excellent for technical tasks\",\n",
    "    \"phi3:mini\": \"Microsoft Phi-3 - Fast and efficient\"\n",
    "}\n",
    "\n",
    "agents = {}\n",
    "for model_id, description in models.items():\n",
    "    try:\n",
    "        agents[model_id] = Agent(\n",
    "            model=OllamaModel(host=ollama_host, model_id=model_id),\n",
    "            system_prompt=\"You are a helpful AI assistant. Be concise.\"\n",
    "        )\n",
    "        print(f\"‚úÖ Created agent with {model_id}\")\n",
    "        print(f\"   {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not create agent with {model_id}: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Successfully created {len(agents)} different local agents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 7: Model Comparison - Side by Side\n",
    "\n",
    "### Real-World Testing\n",
    "Let's ask the same question to different models and compare their responses. This helps you choose the right model for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Test Question: Write a Python HelloWorld program.\n",
      "================================================================================\n",
      "\n",
      "ü§ñ LLAMA3.2 Response:\n",
      "----------------------------------------\n",
      "Here is a simple \"Hello, World!\" program in Python:\n",
      "\n",
      "```python\n",
      "# hello.py\n",
      "\n",
      "def main():\n",
      "    print(\"Hello, World!\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "To run this program, save it to a file called `hello.py` and execute it with Python: \n",
      "\n",
      "```bash\n",
      "python hello.py\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "Hello, World!\n",
      "```Here is a simple \"Hello, World!\" program in Python:\n",
      "\n",
      "```python\n",
      "# hello.py\n",
      "\n",
      "def main():\n",
      "    print(\"Hello, World!\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "To run this program, save it to a file called `hello.py` and execute it with Python: \n",
      "\n",
      "```bash\n",
      "python hello.py\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "Hello, World!\n",
      "```\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Time: 203.59s\n",
      "üìè Length: 315 characters\n",
      "----------------------------------------\n",
      "\n",
      "ü§ñ MISTRAL Response:\n",
      "----------------------------------------\n",
      " Here is a simple \"Hello, World!\" program in Python:\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "When you run this code, it will print the text \"Hello, World!\" to the console. This is often used as a starting point for learning a new programming language. Here is a simple \"Hello, World!\" program in Python:\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "When you run this code, it will print the text \"Hello, World!\" to the console. This is often used as a starting point for learning a new programming language.\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Time: 127.51s\n",
      "üìè Length: 251 characters\n",
      "----------------------------------------\n",
      "\n",
      "ü§ñ PHI3:MINI Response:\n",
      "----------------------------------------\n",
      "```python\n",
      "\n",
      "print(\"Hello, World!\")\n",
      "\n",
      "``````python\n",
      "\n",
      "print(\"Hello, World!\")\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Time: 102.47s\n",
      "üìè Length: 39 characters\n",
      "----------------------------------------\n",
      "\n",
      "üìä Performance Summary:\n",
      "   llama3.2: 203.59s response time\n",
      "   mistral: 127.51s response time\n",
      "   phi3:mini: 102.47s response time\n"
     ]
    }
   ],
   "source": [
    "# Compare models with the same question\n",
    "test_question = \"Write a Python HelloWorld program.\"\n",
    "\n",
    "print(f\"üî¨ Test Question: {test_question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, agent in agents.items():\n",
    "    print(f\"\\nü§ñ {model_name.upper()} Response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = agent(test_question)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            \"time\": end_time - start_time,\n",
    "            \"length\": len(str(response))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Time: {results[model_name]['time']:.2f}s\")\n",
    "        print(f\"üìè Length: {results[model_name]['length']} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"   {model}: {metrics['time']:.2f}s response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Step 10: Cost Analysis - Local vs Cloud\n",
    "\n",
    "With LLMs in the cloud you pay per use, i.e., how many tokens do you put into and get out of the LLM. For small models (still bigger than 3B used above), this can cost **$0.15 for 1 million tokens** or even less (see [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)). Now there are more powerful, larger models that can charge up to $15 per million tokens, but the models we have been running here and that can be run on commodity hardware is not capable of running these extremely large models. So to keep this an apples-to-apples comparison, let's look at the cheaper cloud models with $0.15 for 1 million tokens.  \n",
    "\n",
    "How much is a million tokens? About 750k words, which equals 2500 single spaced pages or 10 PhD dissertations or the entire works of Shakespeare. In other words: **A lot of text for 15 cents**.\n",
    "\n",
    "When you run your model locally, all tokens cost $0. But you pay for a.) the (potentially beefy) hardware, b.) the power to run the hardware and c.) potentially software licenses including those for GPUs. Even if you spend just $100 on a PC, your power and licenses, you'd need to generate more than 666 million tokens. That's more than 1.6 million single-spaces pages of text. So in a nutshell, from a cost perspective it almost never makes sense to run this on your own hardware, but there may be other reasons as discussed above.\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "In just 10 minutes (excluding setup), you've:\n",
    "- ‚úÖ Set up Ollama for local AI execution\n",
    "- ‚úÖ Downloaded and configured multiple AI models\n",
    "- ‚úÖ Created agents with different local models\n",
    "- ‚úÖ Compared performance and capabilities\n",
    "- ‚úÖ Built privacy-first AI applications\n",
    "- ‚úÖ Learned when to use local vs cloud models\n",
    "\n",
    "### üöÄ Your Journey Continues\n",
    "\n",
    "You now have the power to:\n",
    "- Build AI applications with zero API costs\n",
    "- Keep sensitive data completely private\n",
    "- Develop offline-capable AI systems\n",
    "- Switch seamlessly between local and cloud models\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "Ready to dive deeper? Check out:\n",
    "1. **Video 4.5**: Advanced Ollama Configuration\n",
    "2. **Video 5**: Understanding the Agent Loop\n",
    "3. **Video 6**: Streaming and Real-Time Responses\n",
    "\n",
    "### üí° Remember\n",
    "\n",
    "With Ollama and Strands, you have:\n",
    "- **Freedom**: No API rate limits or costs\n",
    "- **Privacy**: Your data stays yours\n",
    "- **Flexibility**: Same code works everywhere\n",
    "- **Power**: State-of-the-art models on your machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
