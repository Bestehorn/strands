{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Providers: Running AI Locally with Ollama\n",
    "\n",
    "**Free, Private, and Powerful: Your Agents on Your Machine**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the world of **local AI models**! This notebook demonstrates how to run powerful AI agents entirely on your own machine using Ollama and the Strands Agents SDK. By the end of this 10-minute tutorial, you'll be able to create agents that run completely offline, cost nothing after setup, and keep your data 100% private.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this hands-on tutorial, you will:\n",
    "- Set up Ollama for local model execution\n",
    "- Create Strands agents using local models\n",
    "- Compare different open-source models\n",
    "- Understand when to use local vs cloud models\n",
    "- Build privacy-first AI applications\n",
    "- Save money on AI development\n",
    "\n",
    "### üè† Why Run Models Locally?\n",
    "\n",
    "Running AI models locally offers several advantages:\n",
    "- **üí∞ Cost**: Zero API fees after initial setup\n",
    "- **üîí Privacy**: Your data never leaves your machine\n",
    "- **‚ö° Speed**: No network latency for API calls\n",
    "- **üåê Offline**: Works without internet connection\n",
    "- **üõ†Ô∏è Control**: Full control over model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Pre-Setup: Installing Ollama and Required Packages\n",
    "\n",
    "### üöÄ Installing Ollama (One-Time Setup)\n",
    "\n",
    "Before we begin, you need to install Ollama on your system. This is a one-time setup that enables local AI model execution. Choose your platform below:\n",
    "\n",
    "#### ü™ü Windows Installation\n",
    "1. **Download the Installer**\n",
    "   - Visit [ollama.com](https://ollama.com)\n",
    "   - Click \"Download for Windows\"\n",
    "   - Save the `.exe` installer\n",
    "\n",
    "2. **Run the Installer**\n",
    "   - Double-click the downloaded file\n",
    "   - Follow the installation wizard\n",
    "   - Ollama will install as a Windows service\n",
    "\n",
    "3. **Verify Installation**\n",
    "   - Open a new Command Prompt or PowerShell\n",
    "   - Type: `ollama --version`\n",
    "   - You should see the version number\n",
    "\n",
    "#### üçé macOS Installation\n",
    "\n",
    "**Option 1: Using Homebrew (Recommended)**\n",
    "```bash\n",
    "# If you have Homebrew installed:\n",
    "brew install ollama\n",
    "\n",
    "# Start Ollama:\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "**Option 2: Direct Download**\n",
    "1. Visit [ollama.com](https://ollama.com)\n",
    "2. Click \"Download for macOS\"\n",
    "3. Open the downloaded `.dmg` file\n",
    "4. Drag Ollama to your Applications folder\n",
    "5. Launch Ollama from Applications\n",
    "6. You'll see the Ollama icon in your menu bar\n",
    "\n",
    "#### üêß Linux Installation\n",
    "\n",
    "**One-Line Install (Recommended)**\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "This script will:\n",
    "- Download the latest Ollama binary\n",
    "- Install it to `/usr/local/bin`\n",
    "- Set up systemd service (on supported systems)\n",
    "- Start the Ollama service\n",
    "\n",
    "**Manual Installation**\n",
    "```bash\n",
    "# Download the binary\n",
    "sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama\n",
    "\n",
    "# Make it executable\n",
    "sudo chmod +x /usr/local/bin/ollama\n",
    "\n",
    "# Start Ollama\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### ‚è±Ô∏è Installation Time\n",
    "- Download: 1-2 minutes (depending on internet speed)\n",
    "- Installation: 1-2 minutes\n",
    "- First model download: 3-5 minutes\n",
    "\n",
    "**Note**: This setup time is NOT included in our 10-minute tutorial!\n",
    "\n",
    "### üêç Installing Python Dependencies\n",
    "\n",
    "Now let's install the Strands SDK with Ollama support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Strands with Ollama support\n",
    "%pip install strands-agents -q\n",
    "%pip install strands-agents[ollama] -q\n",
    "\n",
    "print(\"‚úÖ Strands SDK installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 1: Checking Ollama Installation\n",
    "\n",
    "### Smart Installation Checker\n",
    "This cell will check if Ollama is installed on your system and provide platform-specific installation instructions if needed.\n",
    "\n",
    "### ‚è±Ô∏è Time Note\n",
    "If you need to install Ollama, this is a one-time setup that doesn't count toward our 10-minute tutorial time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def check_ollama_installation():\n",
    "    \"\"\"Check if Ollama is installed and provide installation instructions if not.\"\"\"\n",
    "    try:\n",
    "        # Try to run ollama version command\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, \n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Great! Ollama is installed!\")\n",
    "            print(f\"   Version: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(\"Ollama command failed\")\n",
    "            \n",
    "    except (subprocess.CalledProcessError, FileNotFoundError, Exception):\n",
    "        print(\"‚ùå Ollama is not installed on your system.\")\n",
    "        print(\"\\nüìã Installation Instructions:\\n\")\n",
    "        \n",
    "        system = platform.system()\n",
    "        \n",
    "        if system == \"Darwin\":  # macOS\n",
    "            print(\"üçé macOS detected. You have two options:\\n\")\n",
    "            print(\"Option 1 - Using Homebrew (recommended):\")\n",
    "            print(\"   brew install ollama\\n\")\n",
    "            print(\"Option 2 - Direct download:\")\n",
    "            print(\"   1. Visit https://ollama.com\")\n",
    "            print(\"   2. Download the macOS installer\")\n",
    "            print(\"   3. Run the installer\")\n",
    "            print(\"   4. Start Ollama from your Applications folder\")\n",
    "            \n",
    "        elif system == \"Linux\":\n",
    "            print(\"üêß Linux detected. Install with this command:\\n\")\n",
    "            print(\"   curl -fsSL https://ollama.com/install.sh | sh\\n\")\n",
    "            print(\"After installation, Ollama will run as a service.\")\n",
    "            \n",
    "        elif system == \"Windows\":\n",
    "            print(\"ü™ü Windows detected. Installation steps:\\n\")\n",
    "            print(\"   1. Visit https://ollama.com\")\n",
    "            print(\"   2. Download the Windows installer\")\n",
    "            print(\"   3. Run the downloaded .exe file\")\n",
    "            print(\"   4. Follow the installation wizard\")\n",
    "            print(\"   5. Ollama will run as a Windows service\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unknown system: {system}\")\n",
    "            print(\"   Visit https://ollama.com for installation instructions\")\n",
    "        \n",
    "        print(\"\\n‚è∏Ô∏è  After installing Ollama, restart this notebook and continue!\")\n",
    "        return False\n",
    "\n",
    "# Check installation\n",
    "ollama_installed = check_ollama_installation()\n",
    "\n",
    "if not ollama_installed:\n",
    "    print(\"\\n‚ö° Quick tip: Installation usually takes just 2-3 minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Downloading AI Models\n",
    "\n",
    "### Smart Model Management\n",
    "We'll download three popular models for our demonstrations. Don't worry - if you already have them, we won't download them again!\n",
    "\n",
    "### üìä Models We'll Use:\n",
    "1. **Llama 3.2 (3B)** - Latest from Meta, great all-around model\n",
    "2. **Mistral (7B)** - Excellent for coding and technical tasks\n",
    "3. **Phi-3 Mini (3.8B)** - Microsoft's efficient model, great for quick responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_pull_model(model_name):\n",
    "    \"\"\"Check if a model is installed, pull if not.\"\"\"\n",
    "    try:\n",
    "        # Get list of installed models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True,\n",
    "                              shell=(platform.system() == 'Windows'))\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Error checking models: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "        installed_models = result.stdout.lower()\n",
    "        \n",
    "        # Check if model is already installed\n",
    "        if model_name.lower() in installed_models:\n",
    "            print(f\"‚úÖ {model_name} is already installed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"üì• Downloading {model_name}... (this may take a few minutes)\")\n",
    "            \n",
    "            # Pull the model\n",
    "            result = subprocess.run(['ollama', 'pull', model_name],\n",
    "                                  shell=(platform.system() == 'Windows'))\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ {model_name} downloaded successfully!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to download {model_name}\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Models to use in this tutorial\n",
    "models_to_install = [\n",
    "    \"llama3.2\",     # Meta's latest, 3B parameters\n",
    "    \"mistral\",      # Great for coding, 7B parameters\n",
    "    \"phi3:mini\"     # Microsoft's efficient model, 3.8B parameters\n",
    "]\n",
    "\n",
    "print(\"üöÄ Setting up AI models for local execution...\\n\")\n",
    "\n",
    "# Check and install each model\n",
    "all_models_ready = True\n",
    "for model in models_to_install:\n",
    "    if not check_and_pull_model(model):\n",
    "        all_models_ready = False\n",
    "    print()  # Add spacing\n",
    "\n",
    "if all_models_ready:\n",
    "    print(\"üéâ All models are ready! Let's create some agents!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some models couldn't be installed. You can continue with the available ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 3: Viewing Available Models\n",
    "\n",
    "Let's see what models are available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models\n",
    "print(\"üìã Available Local Models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], \n",
    "                          capture_output=True, text=True,\n",
    "                          shell=(platform.system() == 'Windows'))\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå Could not list models\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüí° Tip: Model sizes shown are compressed. They'll use ~2x RAM when running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Creating Your First Local Agent\n",
    "\n",
    "### From Cloud to Local\n",
    "Creating an agent with Ollama is just as easy as with cloud providers.\n",
    "\n",
    "### üîÑ Provider Comparison\n",
    "```python\n",
    "# Cloud (AWS Bedrock)\n",
    "from strands.models import BedrockModel\n",
    "model = BedrockModel(model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "\n",
    "# Local (Ollama)\n",
    "from strands.models import OllamaModel\n",
    "model = OllamaModel(host=\"http://localhost:11434\", model_id=\"llama3.2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models.ollama import OllamaModel\n",
    "\n",
    "ollama_host = \"http://localhost:11434\"\n",
    "\n",
    "# Create a local agent with Llama 3.2\n",
    "local_agent = Agent(\n",
    "    model=OllamaModel(\n",
    "        host=ollama_host,\n",
    "        model_id=\"llama3.2\"\n",
    "    ),\n",
    "    system_prompt=\"You are a helpful assistant running locally. Be concise and friendly.\"\n",
    ")\n",
    "\n",
    "print(\"üéâ Your first local AI agent is ready!\")\n",
    "print(\"   Model: Llama 3.2 (3B parameters)\")\n",
    "print(\"   Location: Running entirely on your machine\")\n",
    "print(\"   Cost: $0.00\")\n",
    "print(\"   Privacy: 100% - No data leaves your computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 5: Your First Local Conversation\n",
    "\n",
    "Let's test our local agent! Notice how it responds just like cloud-based models, but everything happens on your machine. Note that the first invocation can take a long time (even minutes) and depending on your hardware, subsequent invocations still may take 20 seconds or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the local agent\n",
    "import time\n",
    "\n",
    "question = \"Give me one sentence with advantages of running LLMs locally.\"\n",
    "\n",
    "print(f\"üë§ You: {question}\")\n",
    "print(\"\\nü§ñ Local Llama 3.2 Agent:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "response = local_agent(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n‚è±Ô∏è  Response time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"üí° Note: First response may be slower as the model loads into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Model Switching - Same Code, Different Models\n",
    "\n",
    "### The Power of Strands\n",
    "One of the best features of Strands is how easy it is to switch between models. Let's create agents with different local models and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents with different models\n",
    "models = {\n",
    "    \"llama3.2\": \"Meta's Llama 3.2 - Great all-around model\",\n",
    "    \"mistral\": \"Mistral 7B - Excellent for technical tasks\",\n",
    "    \"phi3:mini\": \"Microsoft Phi-3 - Fast and efficient\"\n",
    "}\n",
    "\n",
    "agents = {}\n",
    "for model_id, description in models.items():\n",
    "    try:\n",
    "        agents[model_id] = Agent(\n",
    "            model=OllamaModel(host=ollama_host, model_id=model_id),\n",
    "            system_prompt=\"You are a helpful AI assistant. Be concise.\"\n",
    "        )\n",
    "        print(f\"‚úÖ Created agent with {model_id}\")\n",
    "        print(f\"   {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not create agent with {model_id}: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Successfully created {len(agents)} different local agents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 7: Model Comparison - Side by Side\n",
    "\n",
    "### Real-World Testing\n",
    "Let's ask the same question to different models and compare their responses. This helps you choose the right model for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with the same question\n",
    "test_question = \"Write a Python HelloWorld program.\"\n",
    "\n",
    "print(f\"üî¨ Test Question: {test_question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, agent in agents.items():\n",
    "    print(f\"\\nü§ñ {model_name.upper()} Response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = agent(test_question)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            \"time\": end_time - start_time,\n",
    "            \"length\": len(str(response))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Time: {results[model_name]['time']:.2f}s\")\n",
    "        print(f\"üìè Length: {results[model_name]['length']} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"   {model}: {metrics['time']:.2f}s response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 8: Practical Use Cases\n",
    "\n",
    "### When to Use Local Models\n",
    "Let's explore scenarios where local models shine and create specialized agents for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specialized local agents for different use cases\n",
    "\n",
    "# 1. Privacy-First Assistant (e.g., for personal notes, health data)\n",
    "privacy_agent = Agent(\n",
    "    model=OllamaModel(host=ollama_host, model_id=\"llama3.2\"),\n",
    "    system_prompt=\"\"\"You are a private assistant for personal and sensitive information. \n",
    "    Remind users that all data stays local and private.\"\"\"\n",
    ")\n",
    "\n",
    "# 2. Offline Code Assistant\n",
    "code_agent = Agent(\n",
    "    model=OllamaModel(host=ollama_host, model_id=\"mistral\"),\n",
    "    system_prompt=\"\"\"You are a coding assistant that works offline. \n",
    "    Provide clear, concise code examples and explanations.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. Quick Response Agent (for real-time applications)\n",
    "quick_agent = Agent(\n",
    "    model=OllamaModel(host=ollama_host, model_id=\"phi3:mini\"),\n",
    "    system_prompt=\"\"\"You are optimized for quick responses. \n",
    "    Keep answers brief and to the point.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"üéØ Specialized Local Agents Created:\\n\")\n",
    "print(\"1Ô∏è‚É£ Privacy-First Assistant (Llama 3.2)\")\n",
    "print(\"   Perfect for: Personal journals, health data, financial planning\")\n",
    "print(\"\\n2Ô∏è‚É£ Offline Code Assistant (Mistral)\")\n",
    "print(\"   Perfect for: Development in secure environments, air-gapped systems\")\n",
    "print(\"\\n3Ô∏è‚É£ Quick Response Agent (Phi-3)\")\n",
    "print(\"   Perfect for: Real-time chat, quick lookups, rapid prototyping\")\n",
    "\n",
    "# Demo: Privacy-first use case\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîí Demo: Privacy-First Assistant\")\n",
    "private_query = \"I want to analyze my personal health metrics. Is my data safe?\"\n",
    "print(f\"\\nüë§ You: {private_query}\")\n",
    "print(\"\\nü§ñ Privacy Agent:\")\n",
    "response = privacy_agent(private_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Step 9: Best Practices and Tips\n",
    "\n",
    "### Making the Most of Local Models\n",
    "Here are key recommendations for using local models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì BEST PRACTICES FOR LOCAL AI MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick reference guide\n",
    "best_practices = {\n",
    "    \"üéØ Model Selection\": [\n",
    "        \"Llama 3.2 (3B): Best general-purpose model\",\n",
    "        \"Mistral (7B): Best for coding and technical tasks\",\n",
    "        \"Phi-3 Mini: Fastest responses, good for real-time apps\",\n",
    "        \"Llama 3.1 (8B): Best quality if you have 16GB+ RAM\"\n",
    "    ],\n",
    "    \"üíª Hardware Requirements\": [\n",
    "        \"Minimum: 8GB RAM for 3B models\",\n",
    "        \"Recommended: 16GB RAM for 7B models\",\n",
    "        \"Optimal: 32GB RAM for multiple models\",\n",
    "        \"GPU: Optional but 2-3x faster if available\"\n",
    "    ],\n",
    "    \"‚ö° Performance Tips\": [\n",
    "        \"Keep models loaded with 'ollama serve'\",\n",
    "        \"Use smaller models for quick tasks\",\n",
    "        \"Batch similar requests together\",\n",
    "        \"Consider quantized versions for speed\"\n",
    "    ],\n",
    "    \"üîß Development Workflow\": [\n",
    "        \"Develop with local models (free & fast)\",\n",
    "        \"Test edge cases without API limits\",\n",
    "        \"Switch to cloud for production if needed\",\n",
    "        \"Use same code for local and cloud models\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tips in best_practices.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for tip in tips:\n",
    "        print(f\"   ‚Ä¢ {tip}\")\n",
    "\n",
    "# When to use what\n",
    "print(\"\\n\\nüìã QUICK DECISION GUIDE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Use LOCAL models when:\")\n",
    "print(\"   ‚Ä¢ Working with sensitive/private data\")\n",
    "print(\"   ‚Ä¢ Developing and testing (no API costs)\")\n",
    "print(\"   ‚Ä¢ Need offline capability\")\n",
    "print(\"   ‚Ä¢ Want predictable latency\")\n",
    "print(\"   ‚Ä¢ Building privacy-first applications\")\n",
    "\n",
    "print(\"\\n‚òÅÔ∏è  Use CLOUD models when:\")\n",
    "print(\"   ‚Ä¢ Need the absolute best quality\")\n",
    "print(\"   ‚Ä¢ Require specific model features (GPT-4 vision, etc.)\")\n",
    "print(\"   ‚Ä¢ Limited local compute resources\")\n",
    "print(\"   ‚Ä¢ Building for massive scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Step 10: Cost Analysis - Local vs Cloud\n",
    "\n",
    "With LLMs in the cloud you pay per use, i.e., how many tokens do you put into and get out of the LLM. For small models (still bigger than 3B used above), this can cost $0.15 for 1 million tokens. Now there are more powerful, larger models that can charge up to $15 per million tokens, but the models we have been running here and that can be run on commodity hardware is not capable of running these extremely large models. So to keep this an apples-to-apples comparison, let's look at the cheaper cloud models. \n",
    "\n",
    "How much is a million tokens? About 750k words, which equals 2500 single spaced pages or 10 PhD dissertations or the entire works of Shakespeare. In other words: A lot of text for 15 cents.\n",
    "\n",
    "When you run your model locally, all tokens cost $0. But you pay for a.) the (potentially beefy) hardware, b.) the power to run the hardware and c.) potentially software licenses including those for GPUs. Even if you spend just $100 on a PC, your power and licenses, you'd need to generate more than 666 million tokens. That's more than 1.6 million single-spaces pages of text. So in a nutshell, from a cost perspective it almost never makes sense to run this on your own hardware, but there may be other reasons as discussed above.\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "In just 10 minutes (excluding setup), you've:\n",
    "- ‚úÖ Set up Ollama for local AI execution\n",
    "- ‚úÖ Downloaded and configured multiple AI models\n",
    "- ‚úÖ Created agents with different local models\n",
    "- ‚úÖ Compared performance and capabilities\n",
    "- ‚úÖ Built privacy-first AI applications\n",
    "- ‚úÖ Learned when to use local vs cloud models\n",
    "\n",
    "### üöÄ Your Journey Continues\n",
    "\n",
    "You now have the power to:\n",
    "- Build AI applications with zero API costs\n",
    "- Keep sensitive data completely private\n",
    "- Develop offline-capable AI systems\n",
    "- Switch seamlessly between local and cloud models\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "Ready to dive deeper? Check out:\n",
    "1. **Video 4.5**: Advanced Ollama Configuration\n",
    "2. **Video 5**: Understanding the Agent Loop\n",
    "3. **Video 6**: Streaming and Real-Time Responses\n",
    "\n",
    "### üí° Remember\n",
    "\n",
    "With Ollama and Strands, you have:\n",
    "- **Freedom**: No API rate limits or costs\n",
    "- **Privacy**: Your data stays yours\n",
    "- **Flexibility**: Same code works everywhere\n",
    "- **Power**: State-of-the-art models on your machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
