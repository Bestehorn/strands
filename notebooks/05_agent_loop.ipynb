{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Agent Loop\n",
    "\n",
    "**Deep Dive into How Agents Think and Act**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to an in-depth exploration of the **Strands Agent Loop**! This notebook reveals the inner workings of how agents process information, make decisions, and generate responses. By the end of this tutorial, you'll understand the elegant architecture that powers intelligent agent behavior.\n",
    "\n",
    "### 🎯 What You'll Learn\n",
    "\n",
    "In this technical deep-dive, you will:\n",
    "- Understand the core components of the agent loop\n",
    "- Trace how messages flow through the system\n",
    "- See how agents decide when to use tools\n",
    "- Learn about recursive reasoning and multi-step workflows\n",
    "- Debug and monitor agent decision-making\n",
    "- Build intuition for creating more sophisticated agents\n",
    "\n",
    "### 🔄 What is the Agent Loop?\n",
    "\n",
    "The **Agent Loop** is the heart of every Strands agent. It's a sophisticated cycle that:\n",
    "1. **Receives** user input\n",
    "2. **Processes** it with an AI model\n",
    "3. **Decides** on actions (including tool use)\n",
    "4. **Executes** those actions\n",
    "5. **Reasons** about results\n",
    "6. **Generates** responses\n",
    "\n",
    "This loop can iterate multiple times in a single interaction, enabling complex reasoning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Step 1: Setup and Imports\n",
    "\n",
    "### Installing Dependencies\n",
    "We'll need the core Strands SDK and some example tools to demonstrate the agent loop in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Setup complete! Ready to explore the agent loop.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install strands-agents strands-agents-tools -q\n",
    "\n",
    "# Import necessary modules\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from strands import Agent, tool\n",
    "from strands_tools import calculator, current_time\n",
    "\n",
    "print(\"✅ Setup complete! Ready to explore the agent loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 2: Enabling Debug Logging\n",
    "\n",
    "### See What's Happening Under the Hood\n",
    "To understand the agent loop, we need to see what's happening internally. Let's enable debug logging to trace the agent's thought process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 No API key provided, using traditional AWS authentication\n",
      "   Checking for AWS credentials...\n",
      "✅ AWS credentials found!\n",
      "\n",
      "============================================================\n",
      "✅ Authentication Method: AWS Profile\n",
      "   Using: AWS credentials file\n",
      "   Region: us-west-2 (Profile: default)\n",
      "💡 Ready to create agents!\n",
      "============================================================\n",
      "\n",
      "🔍 Debug logging enabled!\n",
      "   You'll now see detailed information about the agent's internal processes.\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EASIEST OPTION: Paste your Bedrock API Key here\n",
    "# Get your API key from: https://console.aws.amazon.com/bedrock/ -> API keys\n",
    "API_KEY = \"\"  # Paste your API key between the quotes\n",
    "\n",
    "# Configuration constants (you can modify these if needed)\n",
    "REGION_NAME = \"us-west-2\"\n",
    "AWS_PROFILE = \"default\"\n",
    "\n",
    "# Set up authentication using our utility function\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))  # Add the src directory to path for imports\n",
    "from auth_utils import setup_bedrock_auth, display_auth_status\n",
    "\n",
    "auth_status = setup_bedrock_auth(\n",
    "    api_key=API_KEY,\n",
    "    region_name=REGION_NAME,\n",
    "    aws_profile=AWS_PROFILE\n",
    ")\n",
    "\n",
    "# Display the authentication status\n",
    "display_auth_status(auth_status)\n",
    "\n",
    "# Configure logging to see agent internals\n",
    "logging.getLogger(\"strands\").setLevel(logging.DEBUG)\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(name)s | %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "print(\"\\n🔍 Debug logging enabled!\")\n",
    "print(\"   You'll now see detailed information about the agent's internal processes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 3: The Tool-Using Agent Loop\n",
    "\n",
    "### Multi-Step Reasoning\n",
    "Let's see how the agent loop handles tool usage. When an agent has tools available, the loop becomes more complex:\n",
    "\n",
    "1. **User Input** → 2. **Model Processing** → 3. **Tool Request** → 4. **Tool Execution** → 5. **Result Processing** → 6. **Final Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658869A90>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 TOOL-USING AGENT LOOP DEMONSTRATION\n",
      "==================================================\n",
      "\n",
      "📍 Watch how the agent loop handles tool usage:\n",
      "\n",
      "I'll help you with both of these questions using the tools I have available.\n",
      "\n",
      "First, let me calculate 25 * 48:\n",
      "Tool #2: calculator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.tools.executor | tool_count=<1>, tool_executor=<ThreadPoolExecutorWrapper> | executing tools in parallel\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_7E2s_XYMRtyriIGttwOp9w', 'name': 'calculator', 'input': {'expression': '25 * 48', 'mode': 'evaluate'}}> | invoking\n",
      "DEBUG | strands.tools.executor | tool_count=<1> | submitted tasks to parallel executor\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Calculation Result</span><span style=\"color: #000080; text-decoration-color: #000080\"> ───────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╭───────────┬─────────────────────╮                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Operation </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Evaluate Expression </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Input     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 25 * 48             </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Result    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 1200                </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╰───────────┴─────────────────────╯                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mCalculation Result\u001b[0m\u001b[34m \u001b[0m\u001b[34m──────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╭───────────┬─────────────────────╮                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mOperation\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mEvaluate Expression\u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mInput    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m25 * 48            \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mResult   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1200               \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╰───────────┴─────────────────────╯                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658869A90>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, let me check the current time:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_4bVkaMOqQDC-YGYU_O5HJg', 'name': 'current_time', 'input': {}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658869A90>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: current_time\n",
      "25 × 48 = 1200\n",
      "\n",
      "The current time is 2025-09-10T08:08:26.908302+00:00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<6>, message_count=<40> | skipping context reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(UTC time format).⏱️  Total Processing Time: 9.89 seconds\n",
      "\n",
      "📊 Agent Loop Analysis:\n",
      "==================================================\n",
      "\n",
      "🔄 Loop Step 1 - Role: user\n",
      "   💬 Text: What is 25 * 48? Also, what time is it?...\n",
      "\n",
      "🔄 Loop Step 2 - Role: assistant\n",
      "   💬 Text: I'll help you with both of these questions using the tools I have available.\n",
      "\n",
      "First, let me calculat...\n",
      "   🔧 Tool Request: calculator\n",
      "      Input: {'expression': '25 * 48', 'mode': 'evaluate'}\n",
      "\n",
      "🔄 Loop Step 3 - Role: user\n",
      "   ✅ Tool Result: Result: 1200\n",
      "\n",
      "🔄 Loop Step 4 - Role: assistant\n",
      "   💬 Text: Now, let me check the current time:...\n",
      "   🔧 Tool Request: current_time\n",
      "      Input: {}\n",
      "\n",
      "🔄 Loop Step 5 - Role: user\n",
      "   ✅ Tool Result: 2025-09-10T08:08:26.908302+00:00\n",
      "\n",
      "🔄 Loop Step 6 - Role: assistant\n",
      "   💬 Text: 25 × 48 = 1200\n",
      "\n",
      "The current time is 2025-09-10T08:08:26.908302+00:00 (UTC time format)....\n"
     ]
    }
   ],
   "source": [
    "# Create an agent with tools\n",
    "tool_agent = Agent(\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[calculator, current_time],\n",
    "    system_prompt=\"You are a helpful assistant with access to tools. Use them when needed.\"\n",
    ")\n",
    "\n",
    "print(\"🔧 TOOL-USING AGENT LOOP DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n📍 Watch how the agent loop handles tool usage:\\n\")\n",
    "\n",
    "# Send a message that requires tool use\n",
    "start_time = time.time()\n",
    "response = tool_agent(\"What is 25 * 48? Also, what time is it?\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"⏱️  Total Processing Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Analyze the conversation history\n",
    "print(\"\\n📊 Agent Loop Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for i, msg in enumerate(tool_agent.messages):\n",
    "    role = msg.get('role', 'unknown')\n",
    "    content = msg.get('content', [])\n",
    "    \n",
    "    print(f\"\\n🔄 Loop Step {i+1} - Role: {role}\")\n",
    "    \n",
    "    for item in content:\n",
    "        if 'text' in item:\n",
    "            print(f\"   💬 Text: {item['text'][:100]}...\")\n",
    "        elif 'toolUse' in item:\n",
    "            tool_use = item['toolUse']\n",
    "            print(f\"   🔧 Tool Request: {tool_use.get('name')}\")\n",
    "            print(f\"      Input: {tool_use.get('input')}\")\n",
    "        elif 'toolResult' in item:\n",
    "            tool_result = item['toolResult']\n",
    "            print(f\"   ✅ Tool Result: {tool_result.get('content', [{}])[0].get('text', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 Step 4: Recursive Agent Loops\n",
    "\n",
    "### Complex Multi-Step Reasoning\n",
    "The agent loop can recursively call itself when multiple tool uses are needed. Let's create a scenario that demonstrates this recursive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<get_word_count> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<get_word_count>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<get_character_count> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<get_character_count>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<calculate_reading_time> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<calculate_reading_time>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<get_word_count> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<get_character_count> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<calculate_reading_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<3> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<get_word_count> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<get_character_count> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<calculate_reading_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<3> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658976C40>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 RECURSIVE AGENT LOOP DEMONSTRATION\n",
      "==================================================\n",
      "\n",
      "📍 Watch how the agent makes multiple tool calls:\n",
      "\n",
      "I'll analyze the provided text using the available tools to give you statistics.\n",
      "Tool #4: get_word_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.tools.executor | tool_count=<1>, tool_executor=<ThreadPoolExecutorWrapper> | executing tools in parallel\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_7zfcPb82RhW1DSqXIFKyiw', 'name': 'get_word_count', 'input': {'text': 'The agent loop is a fascinating concept in AI. It allows agents to reason \\nstep by step, use tools when needed, and build complex responses through iterative processing.'}}> | invoking\n",
      "DEBUG | strands.tools.executor | tool_count=<1> | submitted tasks to parallel executor\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658976C40>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #5: get_character_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_Ali9tpVXRSWQUm4xXij62g', 'name': 'get_character_count', 'input': {'text': 'The agent loop is a fascinating concept in AI. It allows agents to reason \\nstep by step, use tools when needed, and build complex responses through iterative processing.'}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658976C40>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #6: calculate_reading_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_NG__JdPtRxaPM208tqyYjA', 'name': 'calculate_reading_time', 'input': {'word_count': 28}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658976C40>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Text Analysis Statistics\n",
      "\n",
      "Here are all the statistics for the provided text:\n",
      "\n",
      "1. **Word Count**: 28 words\n",
      "2. **Character Count**: 169 characters\n",
      "3. **Estimated Reading Time**: 0.14 minutes (about 8.4 seconds)\n",
      "\n",
      "The text is a brief description of the agent loop concept in AI, explaining how it enables agents to work through problems methodically and utilize tools as"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<8>, message_count=<40> | skipping context reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " needed.\n",
      "📊 Loop Statistics:\n",
      "   Total messages: 8\n",
      "   Tool uses: 3\n",
      "   Loop iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# Create custom tools to demonstrate recursive loops\n",
    "@tool\n",
    "def get_word_count(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "@tool\n",
    "def get_character_count(text: str) -> int:\n",
    "    \"\"\"Count the number of characters in a text.\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "@tool\n",
    "def calculate_reading_time(word_count: int) -> float:\n",
    "    \"\"\"Calculate estimated reading time in minutes (200 words per minute).\"\"\"\n",
    "    return round(word_count / 200, 2)\n",
    "\n",
    "# Create an agent with these tools\n",
    "recursive_agent = Agent(\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[get_word_count, get_character_count, calculate_reading_time],\n",
    "    system_prompt=\"You are a text analysis assistant. Analyze text thoroughly using available tools.\"\n",
    ")\n",
    "\n",
    "print(\"🌀 RECURSIVE AGENT LOOP DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n📍 Watch how the agent makes multiple tool calls:\\n\")\n",
    "\n",
    "# Text to analyze\n",
    "sample_text = \"\"\"The agent loop is a fascinating concept in AI. It allows agents to reason \n",
    "step by step, use tools when needed, and build complex responses through iterative processing.\"\"\"\n",
    "\n",
    "# Send the analysis request\n",
    "response = recursive_agent(f\"Analyze this text and give me all statistics: '{sample_text}'\")\n",
    "\n",
    "# Count the number of tool uses\n",
    "tool_uses = 0\n",
    "for msg in recursive_agent.messages:\n",
    "    for item in msg.get('content', []):\n",
    "        if 'toolUse' in item:\n",
    "            tool_uses += 1\n",
    "\n",
    "print(f\"\\n📊 Loop Statistics:\")\n",
    "print(f\"   Total messages: {len(recursive_agent.messages)}\")\n",
    "print(f\"   Tool uses: {tool_uses}\")\n",
    "print(f\"   Loop iterations: {(len(recursive_agent.messages) - 1) // 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 5: Tracing Agent Decisions\n",
    "\n",
    "### Understanding Why Agents Make Choices\n",
    "Let's create a custom callback handler to trace exactly what happens at each step of the agent loop. This gives us deep insights into the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<calculator> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<current_time> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658CCD0F0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 TRACING AGENT DECISIONS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.tools.executor | tool_count=<1>, tool_executor=<ThreadPoolExecutorWrapper> | executing tools in parallel\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_e6wtu5ggQr6J76OQoKvsig', 'name': 'calculator', 'input': {'expression': '156 * 89', 'mode': 'evaluate'}}> | invoking\n",
      "DEBUG | strands.tools.executor | tool_count=<1> | submitted tasks to parallel executor\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Calculation Result</span><span style=\"color: #000080; text-decoration-color: #000080\"> ───────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╭───────────┬─────────────────────╮                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Operation </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Evaluate Expression </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Input     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 156 * 89            </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Result    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 13884               </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╰───────────┴─────────────────────╯                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mCalculation Result\u001b[0m\u001b[34m \u001b[0m\u001b[34m──────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╭───────────┬─────────────────────╮                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mOperation\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mEvaluate Expression\u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mInput    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m156 * 89           \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mResult   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m13884              \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╰───────────┴─────────────────────╯                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658CCD0F0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n",
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_FCeQbmcYTIKi_C-SeJLNKw', 'name': 'current_time', 'input': {}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658CCD0F0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n",
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_eg5RYTDaTSWsrzTb_UeezQ', 'name': 'calculator', 'input': {'mode': 'evaluate', 'expression': '13884 * 1'}}> | invoking\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Calculation Result</span><span style=\"color: #000080; text-decoration-color: #000080\"> ───────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╭───────────┬─────────────────────╮                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Operation </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Evaluate Expression </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Input     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 13884 * 1           </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  │<span style=\"color: #008080; text-decoration-color: #008080\"> Result    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 13884               </span>│                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  ╰───────────┴─────────────────────╯                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mCalculation Result\u001b[0m\u001b[34m \u001b[0m\u001b[34m──────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╭───────────┬─────────────────────╮                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mOperation\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mEvaluate Expression\u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mInput    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m13884 * 1          \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  │\u001b[36m \u001b[0m\u001b[36mResult   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m13884              \u001b[0m\u001b[32m \u001b[0m│                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  ╰───────────┴─────────────────────╯                                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x0000023658CCD0F0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n",
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<8>, message_count=<40> | skipping context reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 AGENT LOOP TRACE:\n",
      "==================================================\n",
      "\n",
      "⏱️  +0.000s - Step 1\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +0.003s - Step 2\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +0.003s - Step 3\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 4\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 5\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 6\n",
      "   Type: text_generation\n",
      "   Generated: I\n",
      "\n",
      "⏱️  +2.807s - Step 7\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 8\n",
      "   Type: text_generation\n",
      "   Generated: 'll solve this step by\n",
      "\n",
      "⏱️  +2.807s - Step 9\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 10\n",
      "   Type: text_generation\n",
      "   Generated:  step using the available tools.\n",
      "\n",
      "⏱️  +2.807s - Step 11\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 12\n",
      "   Type: text_generation\n",
      "   Generated: \n",
      "\n",
      "First, let's calculate\n",
      "\n",
      "⏱️  +2.807s - Step 13\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +2.807s - Step 14\n",
      "   Type: text_generation\n",
      "   Generated:  156 * 89:\n",
      "\n",
      "⏱️  +3.218s - Step 15\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +3.218s - Step 16\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +3.218s - Step 17\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +3.218s - Step 18\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.575s - Step 19\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.575s - Step 20\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.578s - Step 21\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.578s - Step 22\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.581s - Step 23\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.581s - Step 24\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.585s - Step 25\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.585s - Step 26\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.588s - Step 27\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.588s - Step 28\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +4.603s - Step 29\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.603s - Step 30\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.603s - Step 31\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.604s - Step 32\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.628s - Step 33\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.628s - Step 34\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.628s - Step 35\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +4.628s - Step 36\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.609s - Step 37\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.610s - Step 38\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.610s - Step 39\n",
      "   Type: text_generation\n",
      "   Generated: Now\n",
      "\n",
      "⏱️  +14.610s - Step 40\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.610s - Step 41\n",
      "   Type: text_generation\n",
      "   Generated: , let me check the current time:\n",
      "\n",
      "⏱️  +14.704s - Step 42\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.704s - Step 43\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.704s - Step 44\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.704s - Step 45\n",
      "   Type: tool_decision\n",
      "   Tool: current_time\n",
      "\n",
      "⏱️  +14.907s - Step 46\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.908s - Step 47\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.908s - Step 48\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.909s - Step 49\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.910s - Step 50\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.910s - Step 51\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.910s - Step 52\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +14.910s - Step 53\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.541s - Step 54\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.541s - Step 55\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.541s - Step 56\n",
      "   Type: text_generation\n",
      "   Generated: Finally\n",
      "\n",
      "⏱️  +16.541s - Step 57\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.541s - Step 58\n",
      "   Type: text_generation\n",
      "   Generated: , let's calculate how many seconds are in\n",
      "\n",
      "⏱️  +16.541s - Step 59\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.541s - Step 60\n",
      "   Type: text_generation\n",
      "   Generated:  13884:\n",
      "\n",
      "⏱️  +16.687s - Step 61\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.687s - Step 62\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.687s - Step 63\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +16.687s - Step 64\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.710s - Step 65\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.710s - Step 66\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.710s - Step 67\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.710s - Step 68\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.711s - Step 69\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.711s - Step 70\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.711s - Step 71\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.711s - Step 72\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.711s - Step 73\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.711s - Step 74\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.711s - Step 75\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.711s - Step 76\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.712s - Step 77\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.712s - Step 78\n",
      "   Type: tool_decision\n",
      "   Tool: calculator\n",
      "\n",
      "⏱️  +17.716s - Step 79\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.716s - Step 80\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.716s - Step 81\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.717s - Step 82\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.723s - Step 83\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.723s - Step 84\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.723s - Step 85\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +17.723s - Step 86\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +19.580s - Step 87\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +19.580s - Step 88\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +19.580s - Step 89\n",
      "   Type: text_generation\n",
      "   Generated: To\n",
      "\n",
      "⏱️  +19.852s - Step 90\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +19.852s - Step 91\n",
      "   Type: text_generation\n",
      "   Generated:  summarize:\n",
      "1. \n",
      "\n",
      "⏱️  +20.239s - Step 92\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.239s - Step 93\n",
      "   Type: text_generation\n",
      "   Generated: 156 * 89 = \n",
      "\n",
      "⏱️  +20.240s - Step 94\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.240s - Step 95\n",
      "   Type: text_generation\n",
      "   Generated: 13,884\n",
      "2. The current time is\n",
      "\n",
      "⏱️  +20.307s - Step 96\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.307s - Step 97\n",
      "   Type: text_generation\n",
      "   Generated:  2025-09-10T\n",
      "\n",
      "⏱️  +20.415s - Step 98\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.415s - Step 99\n",
      "   Type: text_generation\n",
      "   Generated: 08:14:06.964114\n",
      "\n",
      "⏱️  +20.598s - Step 100\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.598s - Step 101\n",
      "   Type: text_generation\n",
      "   Generated: +00:00 (UTC)\n",
      "3\n",
      "\n",
      "⏱️  +20.801s - Step 102\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.801s - Step 103\n",
      "   Type: text_generation\n",
      "   Generated: . There are 13,884 \n",
      "\n",
      "⏱️  +20.838s - Step 104\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.838s - Step 105\n",
      "   Type: text_generation\n",
      "   Generated: seconds in the result of the first calculation\n",
      "\n",
      "⏱️  +20.847s - Step 106\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.847s - Step 107\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.847s - Step 108\n",
      "   Type: unknown\n",
      "\n",
      "⏱️  +20.848s - Step 109\n",
      "   Type: unknown\n",
      "\n",
      "\n",
      "🤖 Final Response: To summarize:\n",
      "1. 156 * 89 = 13,884\n",
      "2. The current time is 2025-09-10T08:14:06.964114+00:00 (UTC)\n",
      "3. There are 13,884 seconds in the result of the first calculation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a detailed callback handler\n",
    "class AgentLoopTracer:\n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        self.current_step += 1\n",
    "        event = {\n",
    "            'step': self.current_step,\n",
    "            'timestamp': time.time(),\n",
    "            'type': 'unknown'\n",
    "        }\n",
    "        \n",
    "        if 'data' in kwargs:\n",
    "            event['type'] = 'text_generation'\n",
    "            event['data'] = kwargs['data'][:50] + '...' if len(kwargs['data']) > 50 else kwargs['data']\n",
    "        elif 'current_tool_use' in kwargs:\n",
    "            event['type'] = 'tool_decision'\n",
    "            event['tool'] = kwargs['current_tool_use'].get('name')\n",
    "        elif 'event_type' in kwargs:\n",
    "            event['type'] = kwargs['event_type']\n",
    "            \n",
    "        self.events.append(event)\n",
    "    \n",
    "    def print_trace(self):\n",
    "        print(\"\\n🔍 AGENT LOOP TRACE:\")\n",
    "        print(\"=\" * 50)\n",
    "        start_time = self.events[0]['timestamp'] if self.events else 0\n",
    "        \n",
    "        for event in self.events:\n",
    "            elapsed = event['timestamp'] - start_time\n",
    "            print(f\"\\n⏱️  +{elapsed:.3f}s - Step {event['step']}\")\n",
    "            print(f\"   Type: {event['type']}\")\n",
    "            \n",
    "            if event['type'] == 'text_generation':\n",
    "                print(f\"   Generated: {event.get('data', 'N/A')}\")\n",
    "            elif event['type'] == 'tool_decision':\n",
    "                print(f\"   Tool: {event.get('tool', 'N/A')}\")\n",
    "\n",
    "# Create tracer and agent\n",
    "tracer = AgentLoopTracer()\n",
    "traced_agent = Agent(\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[calculator, current_time],\n",
    "    callback_handler=tracer\n",
    ")\n",
    "\n",
    "print(\"🎯 TRACING AGENT DECISIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Execute a complex request\n",
    "response = traced_agent(\n",
    "    \"Calculate 156 * 89, then tell me what time it is. \"\n",
    "    \"Finally, calculate how many seconds are in the result of the first calculation.\"\n",
    ")\n",
    "\n",
    "# Print the trace\n",
    "tracer.print_trace()\n",
    "\n",
    "print(f\"\\n\\n🤖 Final Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Step 6: Parallel Tool Execution\n",
    "\n",
    "### Optimizing the Agent Loop\n",
    "Strands agents can execute multiple tools in parallel when they're independent. This optimization in the agent loop significantly improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.watcher | tool directory watching initialized\n",
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x000002209B926CF0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PARALLEL VS SEQUENTIAL EXECUTION\n",
      "==================================================\n",
      "\n",
      "1️⃣ Sequential Execution (max_parallel_tools=1):\n",
      "I'll make both tool calls at once for you.\n",
      "Tool #1: slow_calculation\n",
      "\n",
      "Tool #2: slow_multiplication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_THokoCxRQTiHiP3v2lZJIg', 'name': 'slow_calculation', 'input': {'x': 10, 'y': 20}}> | invoking\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_k1_7RoUFRDWBnJpnJnbW1Q', 'name': 'slow_multiplication', 'input': {'x': 5, 'y': 6}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x000002209B926CF0>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've completed both calculations for you:\n",
      "\n",
      "1. slow_calculation with x=10 and y=20 resulted in: 30\n",
      "2. slow_multiplication with x=5 and y=6 resulted in: 30\n",
      "\n",
      "Both operations were performed simultaneously to save time."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<4>, message_count=<40> | skipping context reduction\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_calculation> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<slow_multiplication> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x000002209BB5A490>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Response: I've completed both calculations for you:\n",
      "\n",
      "1. slow_calculation with x=10 and y=20 resulted in: 30\n",
      "2. slow_multiplication with x=5 and y=6 resulted in: 30\n",
      "\n",
      "Both operations were performed simultaneously to save time.\n",
      "\n",
      "   ⏱️  Time: 15.78 seconds\n",
      "\n",
      "2️⃣ Parallel Execution (max_parallel_tools=4):\n",
      "I'll make both tool calls at once for you.\n",
      "Tool #3: slow_calculation\n",
      "\n",
      "Tool #4: slow_multiplication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.tools.executor | tool_count=<2>, tool_executor=<ThreadPoolExecutorWrapper> | executing tools in parallel\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_CqmTgbDpSqCZQN8v8p2GcQ', 'name': 'slow_calculation', 'input': {'x': 10, 'y': 20}}> | invoking\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_3qm3F2tdQJ-hD7qDGfkwfQ', 'name': 'slow_multiplication', 'input': {'x': 5, 'y': 6}}> | invoking\n",
      "DEBUG | strands.tools.executor | tool_count=<2> | submitted tasks to parallel executor\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x000002209BB5A490>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've completed both calculations as requested:\n",
      "\n",
      "1. slow_calculation(x=10, y=20) = 30\n",
      "2. slow_multiplication(x=5, y=6) = 30\n",
      "\n",
      "Both operations returned the same result of 30, though they used different mathematical operations to get there (likely addition for"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<4>, message_count=<40> | skipping context reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the first and multiplication for the second).   Response: I've completed both calculations as requested:\n",
      "\n",
      "1. slow_calculation(x=10, y=20) = 30\n",
      "2. slow_multiplication(x=5, y=6) = 30\n",
      "\n",
      "Both operations returned the same result of 30, though they used different mathematical operations to get there (likely addition for the first and multiplication for the second).\n",
      "\n",
      "   ⏱️  Time: 10.92 seconds\n",
      "\n",
      "⚡ Performance Improvement: 1.4x faster with parallel execution!\n"
     ]
    }
   ],
   "source": [
    "# Create tools that simulate longer operations\n",
    "@tool\n",
    "def slow_calculation(x: int, y: int) -> int:\n",
    "    \"\"\"Perform a slow calculation (simulated).\"\"\"\n",
    "    time.sleep(5)  # Simulate processing time\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def slow_multiplication(x: int, y: int) -> int:\n",
    "    \"\"\"Perform a slow multiplication (simulated).\"\"\"\n",
    "    time.sleep(5)  # Simulate processing time\n",
    "    return x * y\n",
    "# Create a common system prompt that encourages parallel execution\n",
    "system_prompt=\"When given multiple independent tasks, always make ALL tool calls in a single response to enable parallel execution.\"\n",
    "\n",
    "# Create agents with different parallel settings\n",
    "sequential_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[slow_calculation, slow_multiplication],\n",
    "    max_parallel_tools=1  # Sequential execution\n",
    ")\n",
    "\n",
    "parallel_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[slow_calculation, slow_multiplication],\n",
    "    max_parallel_tools=4  # Parallel execution\n",
    ")\n",
    "\n",
    "print(\"🔄 PARALLEL VS SEQUENTIAL EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test sequential execution\n",
    "print(\"\\n1️⃣ Sequential Execution (max_parallel_tools=1):\")\n",
    "prompt = \"I need you to make TWO tool calls in your response: call slow_calculation with x=10, y=20 AND call slow_multiplication with x=5, y=6. Make both calls at once.\"\n",
    "start_time = time.time()\n",
    "seq_response = sequential_agent(\n",
    "    prompt=prompt\n",
    ")\n",
    "seq_time = time.time() - start_time\n",
    "print(f\"   Response: {seq_response}\")\n",
    "print(f\"   ⏱️  Time: {seq_time:.2f} seconds\")\n",
    "\n",
    "# Test parallel execution\n",
    "print(\"\\n2️⃣ Parallel Execution (max_parallel_tools=4):\")\n",
    "start_time = time.time()\n",
    "par_response = parallel_agent(\n",
    "    prompt=prompt\n",
    ")\n",
    "par_time = time.time() - start_time\n",
    "print(f\"   Response: {par_response}\")\n",
    "print(f\"   ⏱️  Time: {par_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n⚡ Performance Improvement: {seq_time/par_time:.1f}x faster with parallel execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 Step 7: Custom Agent Loop Components\n",
    "\n",
    "### Building Your Own Loop Extensions\n",
    "The agent loop is extensible. Let's create a custom component that adds timing information to each tool execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.models.bedrock | config=<{'model_id': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}> | initializing\n",
      "DEBUG | strands.tools.registry | tool_name=<fibonacci> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<fibonacci>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tool_name=<factorial> | registering function tool\n",
      "DEBUG | strands.tools.registry | tool_name=<factorial>, tool_type=<function>, is_dynamic=<False> | registering tool\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | tool_modules=<[]> | discovered\n",
      "DEBUG | strands.tools.registry | tool_count=<0>, success_count=<0> | finished loading tools\n",
      "DEBUG | strands.tools.registry | tools_dir=<d:\\Code Workspace\\strands\\notebooks\\tools> | tools directory not found\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<fibonacci> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<factorial> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.tools.registry | getting tool configurations\n",
      "DEBUG | strands.tools.registry | tool_name=<fibonacci> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_name=<factorial> | loaded tool config\n",
      "DEBUG | strands.tools.registry | tool_count=<2> | tools configured\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x00000236592DDD00>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 CUSTOM AGENT LOOP COMPONENTS\n",
      "==================================================\n",
      "I'll calculate the 10th Fibonacci number and the factorial of 7 for you, and then explain what these sequences represent.\n",
      "Tool #11: fibonacci\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.tools.executor | tool_count=<1>, tool_executor=<ThreadPoolExecutorWrapper> | executing tools in parallel\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_GbF6W31PSpCN3hm70FX8lw', 'name': 'fibonacci', 'input': {'n': 10}}> | invoking\n",
      "DEBUG | strands.tools.executor | tool_count=<1> | submitted tasks to parallel executor\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x00000236592DDD00>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #12: factorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.handlers.tool_handler | tool=<{'toolUseId': 'tooluse_8bJH-_K2SEGSUgfw0jzN0Q', 'name': 'factorial', 'input': {'n': 7}}> | invoking\n",
      "DEBUG | strands.event_loop.streaming | model=<<strands.models.bedrock.BedrockModel object at 0x00000236592DDD00>> | streaming messages\n",
      "DEBUG | strands.types.models.model | formatting request\n",
      "DEBUG | strands.types.models.model | invoking model\n",
      "DEBUG | strands.types.models.model | got response from model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let me explain what these sequences represent:\n",
      "\n",
      "### Fibonacci Sequence\n",
      "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. So the sequence begins: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ...\n",
      "\n",
      "The 10th Fibonacci number is 55, as we calculated above.\n",
      "\n",
      "The sequence is named after the Italian mathematician Leonardo of Pisa (known as Fibonacci), who introduced it to Western mathematics in his 1202 book \"Liber Abaci.\" It appears frequently in nature, such as in the arrangement of leaves on a stem, the spiral pattern of shells, and the branching of trees.\n",
      "\n",
      "### Factorial\n",
      "The factorial of a non-negative integer n (denoted as n!) is the product of all positive integers less than or equal to n. So 7! = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5040.\n",
      "\n",
      "Factorials grow very quickly and are important in many areas of mathematics, particularly in combinatorics and probability. They represent the number of ways to arrange n distinct objects in a specific order. For example, 7! represents the number of different ways you can arrange 7 distinct items in a row.\n",
      "\n",
      "Factorials are used in calculating permutations, combinations, and in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG | strands.types.models.model | finished streaming response from model\n",
      "DEBUG | strands.agent.conversation_manager.sliding_window_conversation_manager | window_size=<6>, message_count=<40> | skipping context reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " areas like probability theory, statistics, and calculus.\n",
      "📊 Agent Loop Execution Analysis:\n",
      "   Tool invocations: 2\n",
      "   Total loop iterations: 6\n",
      "   Final response length: 1254 characters\n",
      "\n",
      "⏱️  Custom Timing Data:\n",
      "   Fibonacci execution times: [5.7220458984375e-06]\n",
      "   Factorial execution times: [5.9604644775390625e-06]\n"
     ]
    }
   ],
   "source": [
    "# Create a custom tool wrapper that tracks execution timing\n",
    "class TimedTool:\n",
    "    def __init__(self, name, func):\n",
    "        self.name = name\n",
    "        self.func = func\n",
    "        self.execution_times = []\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = self.func(*args, **kwargs)\n",
    "        duration = time.time() - start\n",
    "        self.execution_times.append(duration)\n",
    "        return f\"{result} (took {duration:.3f}s)\"\n",
    "\n",
    "# Create base mathematical functions\n",
    "def fibonacci_func(n: int) -> int:\n",
    "    \"\"\"Calculate the nth Fibonacci number.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    a, b = 0, 1\n",
    "    for _ in range(2, n + 1):\n",
    "        a, b = b, a + b\n",
    "    return b\n",
    "\n",
    "def factorial_func(n: int) -> int:\n",
    "    \"\"\"Calculate the factorial of n.\"\"\"\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    result = 1\n",
    "    for i in range(2, n + 1):\n",
    "        result *= i\n",
    "    return result\n",
    "\n",
    "# Wrap tools with TimedTool to add timing functionality\n",
    "timed_fibonacci = TimedTool(\"fibonacci\", fibonacci_func)\n",
    "timed_factorial = TimedTool(\"factorial\", factorial_func)\n",
    "\n",
    "# Convert to Strands tools\n",
    "@tool\n",
    "def fibonacci(n: int) -> str:\n",
    "    \"\"\"Calculate the nth Fibonacci number with timing.\"\"\"\n",
    "    return timed_fibonacci(n)\n",
    "\n",
    "@tool\n",
    "def factorial(n: int) -> str:\n",
    "    \"\"\"Calculate the factorial of n with timing.\"\"\"\n",
    "    return timed_factorial(n)\n",
    "\n",
    "# Create agent with timed custom tools\n",
    "math_agent = Agent(\n",
    "    model=auth_status.strands_bedrock_model,\n",
    "    tools=[fibonacci, factorial],\n",
    "    system_prompt=\"You are a mathematics assistant. Show your calculations with timing.\"\n",
    ")\n",
    "\n",
    "print(\"🎨 CUSTOM AGENT LOOP COMPONENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Complex mathematical request\n",
    "response = math_agent(\n",
    "    \"Calculate the 10th Fibonacci number and the factorial of 7. \"\n",
    "    \"Then explain what these sequences represent.\"\n",
    ")\n",
    "\n",
    "# Analyze the loop execution\n",
    "print(\"\\n📊 Agent Loop Execution Analysis:\")\n",
    "tool_count = sum(1 for msg in math_agent.messages \n",
    "                 for item in msg.get('content', []) \n",
    "                 if 'toolUse' in item)\n",
    "print(f\"   Tool invocations: {tool_count}\")\n",
    "print(f\"   Total loop iterations: {len(math_agent.messages)}\")\n",
    "print(f\"   Final response length: {len(str(response))} characters\")\n",
    "\n",
    "# Show execution times collected by our custom TimedTool\n",
    "print(f\"\\n⏱️  Custom Timing Data:\")\n",
    "print(f\"   Fibonacci execution times: {timed_fibonacci.execution_times}\")\n",
    "print(f\"   Factorial execution times: {timed_factorial.execution_times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary: The Agent Loop Architecture\n",
    "\n",
    "### 🏗️ Key Components We've Explored\n",
    "\n",
    "1. **Message Processing Pipeline**\n",
    "   - User input → Model processing → Response generation\n",
    "   - Automatic conversation history management\n",
    "\n",
    "2. **Tool Integration Flow**\n",
    "   - Model decides when to use tools\n",
    "   - Tool execution (sequential or parallel)\n",
    "   - Result processing and reasoning\n",
    "   - Recursive loops for complex tasks\n",
    "\n",
    "3. **Decision Making Process**\n",
    "   - AI model analyzes user intent\n",
    "   - Determines appropriate tools to use\n",
    "   - Chains multiple operations together\n",
    "   - Synthesizes final response\n",
    "\n",
    "4. **Performance Optimizations**\n",
    "   - Parallel tool execution\n",
    "   - Efficient message handling\n",
    "   - Custom extensions for monitoring\n",
    "\n",
    "### 🚀 What You've Learned\n",
    "\n",
    "You now understand:\n",
    "- ✅ How agents process information step-by-step\n",
    "- ✅ The recursive nature of complex reasoning\n",
    "- ✅ Tool execution patterns and optimization\n",
    "- ✅ Debugging and monitoring techniques\n",
    "- ✅ Custom agent loop extensions\n",
    "\n",
    "### 💡 Key Insights\n",
    "\n",
    "1. **The Loop is Recursive**: Agents can call themselves multiple times to complete complex tasks\n",
    "2. **Tools Enable Intelligence**: The combination of LLMs and tools creates powerful capabilities\n",
    "3. **Parallelism Matters**: Independent operations can run simultaneously for better performance\n",
    "4. **Observability is Crucial**: Understanding the loop helps debug and improve agents\n",
    "5. **Extensibility is Key**: Custom components can enhance the agent loop for specific needs\n",
    "\n",
    "### 🎯 Next Steps\n",
    "\n",
    "With this deep understanding of the agent loop, you're ready to:\n",
    "- Build more sophisticated agents with complex workflows\n",
    "- Debug agent behavior effectively\n",
    "- Optimize performance for production use\n",
    "- Create custom extensions to the agent loop\n",
    "\n",
    "### 📚 Further Reading\n",
    "\n",
    "- [Agent Loop Documentation](https://strandsagents.com/0.1.x/user-guide/concepts/agents/agent-loop/)\n",
    "- [Tool Development Guide](https://strandsagents.com/0.1.x/user-guide/concepts/tools/)\n",
    "- [Streaming and Callbacks](https://strandsagents.com/0.1.x/user-guide/concepts/streaming/)\n",
    "\n",
    "### 🎊 Congratulations!\n",
    "\n",
    "You've mastered the inner workings of the Strands agent loop! This knowledge is fundamental for building advanced AI systems. In the next videos, we'll explore streaming responses, state management, and multi-agent systems.\n",
    "\n",
    "Remember: The agent loop is the engine that powers intelligent behavior. Understanding it deeply makes you a more effective AI developer! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
