{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Agentic RAG\n",
    "\n",
    "**Professional Document Processing with AI Agents**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the advanced world of **Retrieval-Augmented Generation**! This notebook builds upon the basics to show you how to create production-ready RAG systems that can handle real-world documents like PDFs, Word files, and more. By the end of this 10-minute tutorial, you'll have a sophisticated document processing pipeline.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this advanced tutorial, you will:\n",
    "- Process multiple document formats (PDF, DOCX, TXT, MD)\n",
    "- Implement smart text chunking strategies\n",
    "- Use ChromaDB for persistent storage\n",
    "- Add metadata filtering and advanced search\n",
    "- Build a document analysis agent\n",
    "- Optimize performance for production use\n",
    "\n",
    "### üìã Prerequisites\n",
    "\n",
    "This tutorial assumes you've completed the \"Agentic RAG Basics\" notebook and understand:\n",
    "- Basic RAG concepts\n",
    "- Vector embeddings\n",
    "- Tool creation for agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Installing Advanced Dependencies\n",
    "\n",
    "### Additional Packages\n",
    "We'll need extra libraries for document processing and persistent storage.\n",
    "\n",
    "### üìö New Dependencies\n",
    "- **chromadb**: Persistent vector database with metadata filtering\n",
    "- **PyPDF2**: PDF text extraction\n",
    "- **python-docx**: Word document processing\n",
    "- **tiktoken**: Advanced text tokenization for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.33.1 requires opentelemetry-exporter-otlp-proto-common==1.33.1, but you have opentelemetry-exporter-otlp-proto-common 1.34.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.33.1 requires opentelemetry-proto==1.33.1, but you have opentelemetry-proto 1.34.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.33.1 requires opentelemetry-sdk~=1.33.1, but you have opentelemetry-sdk 1.34.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All advanced packages installed successfully!\n",
      "   Ready for professional document processing! üöÄ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.34.1 requires opentelemetry-exporter-otlp-proto-common==1.34.1, but you have opentelemetry-exporter-otlp-proto-common 1.33.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.34.1 requires opentelemetry-proto==1.34.1, but you have opentelemetry-proto 1.33.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.34.1 requires opentelemetry-sdk~=1.34.1, but you have opentelemetry-sdk 1.33.1 which is incompatible.\n",
      "opentelemetry-instrumentation 0.55b1 requires opentelemetry-semantic-conventions==0.55b1, but you have opentelemetry-semantic-conventions 0.54b1 which is incompatible.\n",
      "opentelemetry-instrumentation-asgi 0.55b1 requires opentelemetry-semantic-conventions==0.55b1, but you have opentelemetry-semantic-conventions 0.54b1 which is incompatible.\n",
      "opentelemetry-instrumentation-fastapi 0.55b1 requires opentelemetry-semantic-conventions==0.55b1, but you have opentelemetry-semantic-conventions 0.54b1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install chromadb PyPDF2 python-docx tiktoken -q\n",
    "\n",
    "# Also ensure we have the basics from the previous notebook\n",
    "%pip install sentence-transformers strands-agents reportlab -q\n",
    "\n",
    "print(\"‚úÖ All advanced packages installed successfully!\")\n",
    "print(\"   Ready for professional document processing! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Setting Up AWS Bedrock & ChromaDB\n",
    "\n",
    "### Hybrid Architecture\n",
    "We'll combine:\n",
    "- **AWS Bedrock**: For powerful Claude LLM\n",
    "- **ChromaDB**: For local, persistent vector storage\n",
    "\n",
    "This gives us the best of both worlds - powerful AI with local data privacy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AWS Bedrock configured successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code Workspace\\strands\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Code Workspace\\strands\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bestem\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created new collection: strands_documents\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup AWS Bedrock\n",
    "session = boto3.Session(profile_name='default')\n",
    "try:\n",
    "    bedrock_model = BedrockModel(\n",
    "        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        boto_session=session\n",
    "    )\n",
    "    print(\"‚úÖ AWS Bedrock configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Setup ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"strands_documents\"\n",
    "try:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(f\"‚úÖ Created new collection: {collection_name}\")\n",
    "except:\n",
    "    collection = chroma_client.get_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(f\"‚úÖ Using existing collection with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Step 3: Advanced Document Processing\n",
    "\n",
    "### Multi-Format Document Handler\n",
    "Let's create a processor that handles PDFs, Word docs, and text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Document processor ready!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from docx import Document as DocxDocument\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class AdvancedDocumentProcessor:\n",
    "    \"\"\"Process various document types.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text from PDF.\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\"source_type\": \"pdf\", \"page_count\": 0}\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                metadata[\"page_count\"] = len(pdf_reader.pages)\n",
    "                \n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += f\"\\n\\n[Page {page_num + 1}]\\n{page_text}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF: {e}\")\n",
    "            \n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_docx(docx_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text from Word document.\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\"source_type\": \"docx\", \"paragraph_count\": 0}\n",
    "        \n",
    "        try:\n",
    "            doc = DocxDocument(docx_path)\n",
    "            paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "            text = \"\\n\\n\".join(paragraphs)\n",
    "            metadata[\"paragraph_count\"] = len(paragraphs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DOCX: {e}\")\n",
    "            \n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Extract text from any supported file.\"\"\"\n",
    "        path = Path(file_path)\n",
    "        suffix = path.suffix.lower()\n",
    "        \n",
    "        if suffix == '.pdf':\n",
    "            return AdvancedDocumentProcessor.extract_text_from_pdf(file_path)\n",
    "        elif suffix in ['.docx', '.doc']:\n",
    "            return AdvancedDocumentProcessor.extract_text_from_docx(file_path)\n",
    "        elif suffix in ['.txt', '.md']:\n",
    "            text = path.read_text(encoding='utf-8')\n",
    "            return text, {\"source_type\": \"text\", \"char_count\": len(text)}\n",
    "        else:\n",
    "            return \"\", {\"error\": f\"Unsupported file type: {suffix}\"}\n",
    "\n",
    "processor = AdvancedDocumentProcessor()\n",
    "print(\"üìÑ Document processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 4: Smart Text Chunking\n",
    "\n",
    "### Intelligent Document Splitting\n",
    "Large documents need to be chunked intelligently for optimal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Smart chunker ready!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "class SmartTextChunker:\n",
    "    \"\"\"Intelligent text chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def split_text_into_chunks(self, text: str) -> List[Dict[str, any]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = len(self.encoding.encode(para))\n",
    "            current_tokens = len(self.encoding.encode(current_chunk))\n",
    "            \n",
    "            if current_tokens + para_tokens > self.chunk_size and current_chunk:\n",
    "                # Save current chunk\n",
    "                chunks.append({\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"chunk_index\": chunk_index,\n",
    "                    \"token_count\": current_tokens\n",
    "                })\n",
    "                chunk_index += 1\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                words = current_chunk.split()[-self.chunk_overlap:]\n",
    "                current_chunk = \" \".join(words) + \"\\n\\n\" + para\n",
    "            else:\n",
    "                current_chunk += (\"\\n\\n\" if current_chunk else \"\") + para\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"text\": current_chunk.strip(),\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"token_count\": len(self.encoding.encode(current_chunk))\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "chunker = SmartTextChunker()\n",
    "print(\"‚úÇÔ∏è Smart chunker ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Step 5: Document Generation\n",
    "\n",
    "### Creating Test Documents\n",
    "Let's generate sample documents for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating advanced documents...\n",
      "‚úÖ Created 6 documents\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "try:\n",
    "    from rag_document_generator import create_advanced_documents\n",
    "    print(\"üìù Creating advanced documents...\")\n",
    "    advanced_files = create_advanced_documents()\n",
    "    print(f\"‚úÖ Created {len(advanced_files)} documents\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Document generator not found. Creating simple test files...\")\n",
    "    \n",
    "    # Create simple test documents\n",
    "    import os\n",
    "    os.makedirs('../rag_docs', exist_ok=True)\n",
    "    \n",
    "    with open('../rag_docs/test_document.txt', 'w') as f:\n",
    "        f.write(\"This is a test document for the RAG system.\\n\\n\"\n",
    "                \"It contains information about Strands agents and RAG.\")\n",
    "    \n",
    "    print(\"‚úÖ Created test document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Document Ingestion Pipeline\n",
    "\n",
    "### Complete Processing Pipeline\n",
    "Now let's create a pipeline that processes documents and adds them to ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "class DocumentIngestionPipeline:\n",
    "    \"\"\"Pipeline for document ingestion.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection, processor, chunker):\n",
    "        self.collection = collection\n",
    "        self.processor = processor\n",
    "        self.chunker = chunker\n",
    "    \n",
    "    def ingest_document(self, file_path: str, metadata: Dict = None) -> Dict:\n",
    "        \"\"\"Process and ingest a document.\"\"\"\n",
    "        print(f\"\\nüìÑ Processing: {Path(file_path).name}\")\n",
    "        \n",
    "        # Extract text\n",
    "        text, doc_metadata = self.processor.extract_text_from_file(file_path)\n",
    "        \n",
    "        if not text or \"error\" in doc_metadata:\n",
    "            print(f\"   ‚ùå Error: {doc_metadata.get('error', 'Unknown')}\")\n",
    "            return {\"status\": \"error\"}\n",
    "        \n",
    "        # Add metadata\n",
    "        doc_metadata[\"filename\"] = Path(file_path).name\n",
    "        doc_metadata[\"ingestion_date\"] = datetime.now().isoformat()\n",
    "        if metadata:\n",
    "            doc_metadata.update(metadata)\n",
    "        \n",
    "        # Chunk text\n",
    "        chunks = self.chunker.split_text_into_chunks(text)\n",
    "        print(f\"   üìä Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        chunk_ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = hashlib.md5(f\"{file_path}_{i}\".encode()).hexdigest()\n",
    "            chunk_ids.append(chunk_id)\n",
    "            documents.append(chunk[\"text\"])\n",
    "            \n",
    "            chunk_metadata = doc_metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_index\": chunk[\"chunk_index\"],\n",
    "                \"chunk_total\": len(chunks)\n",
    "            })\n",
    "            metadatas.append(chunk_metadata)\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=chunk_ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            print(f\"   ‚úÖ Successfully ingested\")\n",
    "            return {\"status\": \"success\", \"chunks\": len(chunks)}\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            return {\"status\": \"error\"}\n",
    "\n",
    "pipeline = DocumentIngestionPipeline(collection, processor, chunker)\n",
    "print(\"üîÑ Pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 7: Ingesting Documents\n",
    "\n",
    "### Processing Our Document Collection\n",
    "Let's ingest all documents from the rag_docs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting document ingestion...\n",
      "============================================================\n",
      "\n",
      "‚úÖ Total documents processed: 0\n",
      "üìä Total chunks in database: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Starting document ingestion...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rag_docs_path = Path('../rag_docs')\n",
    "if not rag_docs_path.exists():\n",
    "    print(\"‚ùå rag_docs directory not found!\")\n",
    "else:\n",
    "    # Process all documents\n",
    "    document_types = [\n",
    "        (\"*.pdf\", {\"category\": \"reference\"}),\n",
    "        (\"*.docx\", {\"category\": \"patterns\"}),\n",
    "        (\"*.txt\", {\"category\": \"guides\"}),\n",
    "        (\"*.md\", {\"category\": \"documentation\"})\n",
    "    ]\n",
    "    \n",
    "    total_processed = 0\n",
    "    \n",
    "    for pattern, metadata in document_types:\n",
    "        files = list(rag_docs_path.rglob(pattern))\n",
    "        if files:\n",
    "            print(f\"\\nüìÇ Processing {len(files)} {pattern} files...\")\n",
    "            for file in files:\n",
    "                result = pipeline.ingest_document(str(file), metadata)\n",
    "                if result[\"status\"] == \"success\":\n",
    "                    total_processed += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total documents processed: {total_processed}\")\n",
    "    print(f\"üìä Total chunks in database: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 8: Advanced RAG Tools\n",
    "\n",
    "### Creating Sophisticated Search Tools\n",
    "Let's build tools with metadata filtering and advanced search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Advanced RAG tools created!\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_documents(query: str, category: str = None, limit: int = 5) -> str:\n",
    "    \"\"\"Search documents with optional category filtering.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        category: Optional category filter (reference, patterns, guides, documentation)\n",
    "        limit: Number of results to return\n",
    "    \"\"\"\n",
    "    # Build where clause for filtering\n",
    "    where_clause = {\"category\": category} if category else None\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=limit,\n",
    "        where=where_clause\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "        formatted_results.append(\n",
    "            f\"[Result {i+1}]\\n\"\n",
    "            f\"Source: {metadata.get('filename', 'Unknown')}\\n\"\n",
    "            f\"Category: {metadata.get('category', 'Unknown')}\\n\"\n",
    "            f\"Chunk: {metadata.get('chunk_index', '?')}/{metadata.get('chunk_total', '?')}\\n\"\n",
    "            f\"Content: {doc[:200]}...\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "@tool\n",
    "def list_document_categories() -> str:\n",
    "    \"\"\"List all document categories in the knowledge base.\"\"\"\n",
    "    all_metadata = collection.get()[\"metadatas\"]\n",
    "    categories = set()\n",
    "    \n",
    "    for metadata in all_metadata:\n",
    "        if \"category\" in metadata:\n",
    "            categories.add(metadata[\"category\"])\n",
    "    \n",
    "    if categories:\n",
    "        return \"Available categories:\\n\" + \"\\n\".join(f\"- {cat}\" for cat in sorted(categories))\n",
    "    else:\n",
    "        return \"No categories found in knowledge base.\"\n",
    "\n",
    "@tool\n",
    "def get_document_stats() -> str:\n",
    "    \"\"\"Get statistics about the document knowledge base.\"\"\"\n",
    "    total_chunks = collection.count()\n",
    "    all_metadata = collection.get()[\"metadatas\"]\n",
    "    \n",
    "    # Count unique documents\n",
    "    unique_docs = set(m.get(\"filename\", \"Unknown\") for m in all_metadata)\n",
    "    \n",
    "    # Count by category\n",
    "    category_counts = {}\n",
    "    for metadata in all_metadata:\n",
    "        cat = metadata.get(\"category\", \"uncategorized\")\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    stats = f\"üìä Knowledge Base Statistics:\\n\"\n",
    "    stats += f\"Total chunks: {total_chunks}\\n\"\n",
    "    stats += f\"Unique documents: {len(unique_docs)}\\n\"\n",
    "    stats += f\"\\nChunks by category:\\n\"\n",
    "    for cat, count in sorted(category_counts.items()):\n",
    "        stats += f\"  - {cat}: {count}\\n\"\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"üîß Advanced RAG tools created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 9: Creating the Document Analysis Agent\n",
    "\n",
    "### Professional Document Assistant\n",
    "Let's create an agent that can analyze documents and answer complex questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Document analysis agent ready!\n",
      "   Model: Claude 3.7 Sonnet\n",
      "   Knowledge Base: ChromaDB with advanced search\n",
      "   Tools: search_documents, list_document_categories, get_document_stats\n"
     ]
    }
   ],
   "source": [
    "# Create the document analysis agent\n",
    "document_agent = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"\"\"You are a professional document analysis assistant with access to a comprehensive knowledge base.\n",
    "    \n",
    "    YOUR CAPABILITIES:\n",
    "    1. Search documents by content and category\n",
    "    2. Analyze document statistics\n",
    "    3. Provide detailed summaries and insights\n",
    "    4. Cross-reference information from multiple sources\n",
    "    \n",
    "    GUIDELINES:\n",
    "    - Always search the knowledge base before answering\n",
    "    - Use category filtering when relevant\n",
    "    - Cite your sources with document names\n",
    "    - If information is not found, acknowledge this clearly\n",
    "    - Provide comprehensive, well-structured responses\n",
    "    \n",
    "    Remember: You have access to reference documents, design patterns, guides, and documentation.\"\"\",\n",
    "    tools=[search_documents, list_document_categories, get_document_stats]\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Document analysis agent ready!\")\n",
    "print(\"   Model: Claude 3.7 Sonnet\")\n",
    "print(\"   Knowledge Base: ChromaDB with advanced search\")\n",
    "print(\"   Tools: search_documents, list_document_categories, get_document_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 10: Testing the Advanced RAG System\n",
    "\n",
    "### Professional Document Analysis\n",
    "Let's test our agent with complex queries that require document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 1: Knowledge Base Overview\n",
      "==================================================\n",
      "To answer your question about the types of documents I have access to, let me check the document categories available in the knowledge base.\n",
      "Tool #1: list_document_categories\n",
      "Let me get more statistics about the knowledge base to provide you with a better overview.\n",
      "Tool #2: get_document_stats\n",
      "I apologize, but it appears that the knowledge base is currently empty. There are no documents or categories available at the moment. \n",
      "\n",
      "This means I don't currently have access to any specific document collections to search through or provide information from. If you have specific questions about topics or information you're looking for, I can still try to help based on my general knowledge, but I won't be able to reference specific documents from a knowledge base.\n",
      "\n",
      "Is there something specific you'd like to know about or a particular topic you'd like me to assist with?ü§ñ Agent: I apologize, but it appears that the knowledge base is currently empty. There are no documents or categories available at the moment. \n",
      "\n",
      "This means I don't currently have access to any specific document collections to search through or provide information from. If you have specific questions about topics or information you're looking for, I can still try to help based on my general knowledge, but I won't be able to reference specific documents from a knowledge base.\n",
      "\n",
      "Is there something specific you'd like to know about or a particular topic you'd like me to assist with?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Get overview of knowledge base\n",
    "print(\"üìä Test 1: Knowledge Base Overview\")\n",
    "print(\"=\" * 50)\n",
    "response = document_agent(\"What types of documents do you have access to? Give me an overview.\")\n",
    "print(f\"ü§ñ Agent: {response}\")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Category-specific search\n",
    "print(\"üîç Test 2: Category-Specific Search\")\n",
    "print(\"=\" * 50)\n",
    "response = document_agent(\"Search for information about agent design patterns in the patterns category.\")\n",
    "print(f\"ü§ñ Agent: {response}\")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Cross-reference information\n",
    "print(\"üîó Test 3: Cross-Reference Analysis\")\n",
    "print(\"=\" * 50)\n",
    "response = document_agent(\n",
    "    \"Compare the information about Strands agents across different document types. \"\n",
    "    \"What are the key themes that appear in multiple documents?\"\n",
    ")\n",
    "print(f\"ü§ñ Agent: {response}\")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "\n",
    "In this advanced tutorial, you've:\n",
    "- ‚úÖ Built a multi-format document processor (PDF, DOCX, TXT, MD)\n",
    "- ‚úÖ Implemented smart text chunking with token awareness\n",
    "- ‚úÖ Set up ChromaDB for persistent vector storage\n",
    "- ‚úÖ Created a document ingestion pipeline\n",
    "- ‚úÖ Built advanced search tools with metadata filtering\n",
    "- ‚úÖ Developed a professional document analysis agent\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "Now that you've mastered advanced RAG, you can:\n",
    "1. **Scale Your System**: Add more documents and document types\n",
    "2. **Implement Caching**: Speed up frequent queries\n",
    "3. **Add Re-ranking**: Improve search result quality\n",
    "4. **Build Specialized Agents**: Create domain-specific assistants\n",
    "5. **Deploy to Production**: Use cloud storage and scaling\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **ChromaDB**: Provides persistence and metadata filtering\n",
    "2. **Smart Chunking**: Respects document structure and token limits\n",
    "3. **Metadata**: Enables powerful filtering and organization\n",
    "4. **Pipeline Architecture**: Makes scaling easy\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Tiktoken Documentation](https://github.com/openai/tiktoken)\n",
    "- [PyPDF2 Documentation](https://pypdf2.readthedocs.io/)\n",
    "\n",
    "### üåü Challenge Yourself\n",
    "\n",
    "Try enhancing your RAG system by:\n",
    "- Adding support for Excel files and CSVs\n",
    "- Implementing semantic chunking based on headings\n",
    "- Creating a web interface for document upload\n",
    "- Building specialized agents for different document categories\n",
    "\n",
    "Happy building with advanced RAG! üöÄü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
