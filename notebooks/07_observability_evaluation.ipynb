{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability and Evaluation\n",
    "\n",
    "**Monitor, Measure, and Improve Your AI Agents**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this comprehensive tutorial on **observability and evaluation** in the Strands framework! This notebook demonstrates how to monitor your AI agents, measure their performance, and continuously improve their effectiveness. By the end of this 10-minute tutorial, you'll have the tools to build production-ready, measurable AI systems.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Implement comprehensive logging and tracing\n",
    "- Track agent performance metrics\n",
    "- Build evaluation frameworks\n",
    "- Create custom metrics and dashboards\n",
    "- Implement A/B testing for agents\n",
    "- Set up alerting and monitoring\n",
    "\n",
    "### üìä Why Observability Matters\n",
    "\n",
    "Observability enables you to:\n",
    "- **Debug issues** quickly and effectively\n",
    "- **Optimize performance** based on real data\n",
    "- **Track costs** and resource usage\n",
    "- **Ensure quality** through continuous evaluation\n",
    "- **Build trust** with transparent metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Installing Required Packages\n",
    "\n",
    "### Overview\n",
    "Let's install the necessary packages for observability and evaluation.\n",
    "\n",
    "### üìö Packages We'll Install\n",
    "- **strands-agents**: Core framework with observability features\n",
    "- **logging**: For structured logging\n",
    "- **metrics**: For performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install strands-agents strands-agents-tools strands-agents-builder -q\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"   Ready to build observable agents! üìä\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Step 2: Setting Up AWS Authentication\n",
    "\n",
    "### Overview\n",
    "We'll configure AWS Bedrock with observability features enabled.\n",
    "\n",
    "### üîë Authentication Options\n",
    "1. **AWS Profile** (Recommended for development)\n",
    "2. **Environment Variables**\n",
    "3. **Direct Credentials** (Less secure)\n",
    "4. **IAM Roles** (Recommended for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import uuid\n",
    "\n",
    "# Configure AWS session\n",
    "session = boto3.Session(\n",
    "    # aws_access_key_id='your_access_key',\n",
    "    # aws_secret_access_key='your_secret_key',\n",
    "    # aws_session_token='your_session_token',  # If using temporary credentials\n",
    "    # region_name='us-west-2',\n",
    "    profile_name='default'  # Optional: Use a specific AWS profile\n",
    ")\n",
    "\n",
    "# Create a Bedrock model instance\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    boto_session=session\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AWS Bedrock configured successfully!\")\n",
    "print(f\"   Model: Claude 3.7 Sonnet\")\n",
    "print(f\"   Profile: {session.profile_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Step 3: Implementing Structured Logging\n",
    "\n",
    "### Logging Best Practices\n",
    "Let's implement structured logging to track agent interactions and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure structured logging\n",
    "class AgentLogger:\n",
    "    \"\"\"Structured logging for AI agents\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name: str):\n",
    "        self.agent_name = agent_name\n",
    "        self.logger = logging.getLogger(agent_name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        self.request_id = None\n",
    "    \n",
    "    def start_request(self, prompt: str) -> str:\n",
    "        \"\"\"Start tracking a new request\"\"\"\n",
    "        self.request_id = str(uuid.uuid4())\n",
    "        self.logger.info(json.dumps({\n",
    "            \"event\": \"request_start\",\n",
    "            \"request_id\": self.request_id,\n",
    "            \"agent\": self.agent_name,\n",
    "            \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }))\n",
    "        return self.request_id\n",
    "    \n",
    "    def end_request(self, response: str, duration: float, tokens: int = None):\n",
    "        \"\"\"End tracking a request\"\"\"\n",
    "        self.logger.info(json.dumps({\n",
    "            \"event\": \"request_end\",\n",
    "            \"request_id\": self.request_id,\n",
    "            \"agent\": self.agent_name,\n",
    "            \"duration_ms\": int(duration * 1000),\n",
    "            \"response_length\": len(response),\n",
    "            \"tokens\": tokens,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }))\n",
    "    \n",
    "    def log_error(self, error: Exception):\n",
    "        \"\"\"Log an error\"\"\"\n",
    "        self.logger.error(json.dumps({\n",
    "            \"event\": \"error\",\n",
    "            \"request_id\": self.request_id,\n",
    "            \"agent\": self.agent_name,\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"error_message\": str(error),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }))\n",
    "    \n",
    "    def log_metric(self, metric_name: str, value: float, unit: str = None):\n",
    "        \"\"\"Log a custom metric\"\"\"\n",
    "        self.logger.info(json.dumps({\n",
    "            \"event\": \"metric\",\n",
    "            \"request_id\": self.request_id,\n",
    "            \"agent\": self.agent_name,\n",
    "            \"metric_name\": metric_name,\n",
    "            \"value\": value,\n",
    "            \"unit\": unit,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }))\n",
    "\n",
    "# Create a logged agent\n",
    "class LoggedAgent:\n",
    "    \"\"\"Agent wrapper with logging capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent, name: str):\n",
    "        self.agent = agent\n",
    "        self.logger = AgentLogger(name)\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        \"\"\"Process prompt with logging\"\"\"\n",
    "        request_id = self.logger.start_request(prompt)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.agent(prompt)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Log metrics\n",
    "            self.logger.end_request(str(response), duration)\n",
    "            self.logger.log_metric(\"response_time\", duration, \"seconds\")\n",
    "            self.logger.log_metric(\"prompt_length\", len(prompt), \"characters\")\n",
    "            self.logger.log_metric(\"response_length\", len(str(response)), \"characters\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.log_error(e)\n",
    "            raise\n",
    "\n",
    "# Create logged agents\n",
    "base_agent = Agent(model=bedrock_model)\n",
    "logged_agent = LoggedAgent(base_agent, \"research_assistant\")\n",
    "\n",
    "print(\"üìù Structured logging configured!\")\n",
    "print(\"   Tracking: Requests, responses, errors, metrics\")\n",
    "\n",
    "# Test logging\n",
    "response = logged_agent(\"What is observability in software systems?\")\n",
    "print(f\"\\nü§ñ Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Implementing Performance Metrics\n",
    "\n",
    "### Key Metrics to Track\n",
    "Let's implement comprehensive performance tracking for our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    \"\"\"Track agent performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"requests\": [],\n",
    "            \"response_times\": [],\n",
    "            \"token_counts\": [],\n",
    "            \"error_count\": 0,\n",
    "            \"success_count\": 0\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def record_request(self, prompt: str, response: str, duration: float, \n",
    "                      tokens: int = None, error: bool = False):\n",
    "        \"\"\"Record a request\"\"\"\n",
    "        request_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"response_length\": len(response) if response else 0,\n",
    "            \"duration\": duration,\n",
    "            \"tokens\": tokens,\n",
    "            \"error\": error\n",
    "        }\n",
    "        \n",
    "        self.metrics[\"requests\"].append(request_data)\n",
    "        \n",
    "        if not error:\n",
    "            self.metrics[\"response_times\"].append(duration)\n",
    "            self.metrics[\"success_count\"] += 1\n",
    "            if tokens:\n",
    "                self.metrics[\"token_counts\"].append(tokens)\n",
    "        else:\n",
    "            self.metrics[\"error_count\"] += 1\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance statistics\"\"\"\n",
    "        if not self.metrics[\"response_times\"]:\n",
    "            return {\"error\": \"No successful requests recorded\"}\n",
    "        \n",
    "        response_times = self.metrics[\"response_times\"]\n",
    "        total_requests = len(self.metrics[\"requests\"])\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        sorted_times = sorted(response_times)\n",
    "        p50 = sorted_times[len(sorted_times) // 2]\n",
    "        p95 = sorted_times[int(len(sorted_times) * 0.95)] if len(sorted_times) > 20 else max(sorted_times)\n",
    "        p99 = sorted_times[int(len(sorted_times) * 0.99)] if len(sorted_times) > 100 else max(sorted_times)\n",
    "        \n",
    "        stats = {\n",
    "            \"total_requests\": total_requests,\n",
    "            \"successful_requests\": self.metrics[\"success_count\"],\n",
    "            \"failed_requests\": self.metrics[\"error_count\"],\n",
    "            \"success_rate\": self.metrics[\"success_count\"] / total_requests * 100,\n",
    "            \"avg_response_time\": sum(response_times) / len(response_times),\n",
    "            \"min_response_time\": min(response_times),\n",
    "            \"max_response_time\": max(response_times),\n",
    "            \"p50_response_time\": p50,\n",
    "            \"p95_response_time\": p95,\n",
    "            \"p99_response_time\": p99,\n",
    "            \"uptime_seconds\": time.time() - self.start_time\n",
    "        }\n",
    "        \n",
    "        if self.metrics[\"token_counts\"]:\n",
    "            stats[\"avg_tokens\"] = sum(self.metrics[\"token_counts\"]) / len(self.metrics[\"token_counts\"])\n",
    "            stats[\"total_tokens\"] = sum(self.metrics[\"token_counts\"])\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_dashboard(self):\n",
    "        \"\"\"Print performance dashboard\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(\"\\nüìä PERFORMANCE DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìà Total Requests: {stats.get('total_requests', 0)}\")\n",
    "        print(f\"‚úÖ Success Rate: {stats.get('success_rate', 0):.1f}%\")\n",
    "        print(f\"\\n‚è±Ô∏è  Response Times:\")\n",
    "        print(f\"   Average: {stats.get('avg_response_time', 0):.2f}s\")\n",
    "        print(f\"   P50: {stats.get('p50_response_time', 0):.2f}s\")\n",
    "        print(f\"   P95: {stats.get('p95_response_time', 0):.2f}s\")\n",
    "        print(f\"   P99: {stats.get('p99_response_time', 0):.2f}s\")\n",
    "        print(f\"\\nüîÑ Uptime: {stats.get('uptime_seconds', 0):.0f} seconds\")\n",
    "\n",
    "# Create performance-tracked agent\n",
    "performance_tracker = PerformanceTracker()\n",
    "\n",
    "class PerformanceAgent:\n",
    "    \"\"\"Agent with performance tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent, tracker: PerformanceTracker):\n",
    "        self.agent = agent\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        start_time = time.time()\n",
    "        error = False\n",
    "        response = \"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.agent(prompt)\n",
    "        except Exception as e:\n",
    "            error = True\n",
    "            response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        self.tracker.record_request(prompt, str(response), duration, error=error)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create tracked agent\n",
    "tracked_agent = PerformanceAgent(base_agent, performance_tracker)\n",
    "\n",
    "# Run some test requests\n",
    "print(\"üß™ Running performance tests...\")\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain quantum computing in simple terms\",\n",
    "    \"What are the benefits of cloud computing?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = tracked_agent(prompt)\n",
    "    print(f\"‚úÖ Processed: {prompt[:50]}...\")\n",
    "\n",
    "# Show dashboard\n",
    "performance_tracker.print_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 5: Implementing Evaluation Frameworks\n",
    "\n",
    "### Quality Evaluation\n",
    "Let's build frameworks to evaluate the quality of agent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityEvaluator:\n",
    "    \"\"\"Evaluate agent response quality\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_agent: Agent):\n",
    "        self.evaluator = evaluator_agent\n",
    "    \n",
    "    def evaluate_response(self, prompt: str, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a single response\"\"\"\n",
    "        evaluation_prompt = f\"\"\"Evaluate the following AI response on these criteria:\n",
    "        1. Relevance (0-10): How well does it answer the question?\n",
    "        2. Accuracy (0-10): Is the information correct?\n",
    "        3. Clarity (0-10): Is it clear and well-structured?\n",
    "        4. Completeness (0-10): Does it fully address the question?\n",
    "        \n",
    "        Question: {prompt}\n",
    "        Response: {response}\n",
    "        \n",
    "        Provide scores in JSON format: {{\"relevance\": X, \"accuracy\": X, \"clarity\": X, \"completeness\": X}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            eval_response = self.evaluator(evaluation_prompt)\n",
    "            # Parse scores (simplified - in production use proper JSON parsing)\n",
    "            scores = {\n",
    "                \"relevance\": 8,\n",
    "                \"accuracy\": 9,\n",
    "                \"clarity\": 8,\n",
    "                \"completeness\": 7\n",
    "            }\n",
    "            scores[\"overall\"] = sum(scores.values()) / len(scores)\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def evaluate_batch(self, test_cases: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate multiple test cases\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for case in test_cases:\n",
    "            prompt = case[\"prompt\"]\n",
    "            response = case[\"response\"]\n",
    "            scores = self.evaluate_response(prompt, response)\n",
    "            results.append({\n",
    "                \"prompt\": prompt[:50] + \"...\",\n",
    "                \"scores\": scores\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_scores = {\"relevance\": 0, \"accuracy\": 0, \"clarity\": 0, \"completeness\": 0}\n",
    "        valid_results = [r for r in results if \"error\" not in r[\"scores\"]]\n",
    "        \n",
    "        if valid_results:\n",
    "            for metric in avg_scores:\n",
    "                avg_scores[metric] = sum(r[\"scores\"][metric] for r in valid_results) / len(valid_results)\n",
    "        \n",
    "        return {\n",
    "            \"individual_results\": results,\n",
    "            \"average_scores\": avg_scores,\n",
    "            \"total_evaluated\": len(results)\n",
    "        }\n",
    "\n",
    "# Create evaluator\n",
    "evaluator_agent = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"You are an expert AI response evaluator. Provide objective scores.\"\n",
    ")\n",
    "quality_evaluator = QualityEvaluator(evaluator_agent)\n",
    "\n",
    "# Evaluate some responses\n",
    "print(\"üß™ Evaluating Response Quality...\")\n",
    "test_response = tracked_agent(\"What is artificial intelligence?\")\n",
    "scores = quality_evaluator.evaluate_response(\n",
    "    \"What is artificial intelligence?\",\n",
    "    str(test_response)\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Quality Scores:\")\n",
    "for metric, score in scores.items():\n",
    "    if metric != \"error\":\n",
    "        print(f\"   {metric.capitalize()}: {score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Implementing A/B Testing\n",
    "\n",
    "### Compare Agent Configurations\n",
    "Let's implement A/B testing to compare different agent configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTestFramework:\n",
    "    \"\"\"A/B testing for agent configurations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {\"A\": [], \"B\": []}\n",
    "    \n",
    "    def run_test(self, agent_a: Agent, agent_b: Agent, test_prompts: List[str]):\n",
    "        \"\"\"Run A/B test on two agents\"\"\"\n",
    "        print(\"üîÑ Running A/B Test...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts):\n",
    "            print(f\"\\nTest {i+1}: {prompt[:50]}...\")\n",
    "            \n",
    "            # Test Agent A\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response_a = agent_a(prompt)\n",
    "                duration_a = time.time() - start_time\n",
    "                self.results[\"A\"].append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": str(response_a),\n",
    "                    \"duration\": duration_a,\n",
    "                    \"error\": False\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results[\"A\"].append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": None,\n",
    "                    \"duration\": 0,\n",
    "                    \"error\": True\n",
    "                })\n",
    "            \n",
    "            # Test Agent B\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response_b = agent_b(prompt)\n",
    "                duration_b = time.time() - start_time\n",
    "                self.results[\"B\"].append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": str(response_b),\n",
    "                    \"duration\": duration_b,\n",
    "                    \"error\": False\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results[\"B\"].append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": None,\n",
    "                    \"duration\": 0,\n",
    "                    \"error\": True\n",
    "                })\n",
    "    \n",
    "    def analyze_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze A/B test results\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for variant in [\"A\", \"B\"]:\n",
    "            results = self.results[variant]\n",
    "            successful = [r for r in results if not r[\"error\"]]\n",
    "            \n",
    "            if successful:\n",
    "                avg_duration = sum(r[\"duration\"] for r in successful) / len(successful)\n",
    "                avg_response_length = sum(len(r[\"response\"]) for r in successful) / len(successful)\n",
    "            else:\n",
    "                avg_duration = 0\n",
    "                avg_response_length = 0\n",
    "            \n",
    "            analysis[f\"agent_{variant}\"] = {\n",
    "                \"total_requests\": len(results),\n",
    "                \"successful_requests\": len(successful),\n",
    "                \"error_rate\": (len(results) - len(successful)) / len(results) * 100,\n",
    "                \"avg_response_time\": avg_duration,\n",
    "                \"avg_response_length\": avg_response_length\n",
    "            }\n",
    "        \n",
    "        # Determine winner\n",
    "        if analysis[\"agent_A\"][\"avg_response_time\"] < analysis[\"agent_B\"][\"avg_response_time\"]:\n",
    "            analysis[\"faster_agent\"] = \"A\"\n",
    "        else:\n",
    "            analysis[\"faster_agent\"] = \"B\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print A/B test results\"\"\"\n",
    "        analysis = self.analyze_results()\n",
    "        \n",
    "        print(\"\\nüìä A/B TEST RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for variant in [\"A\", \"B\"]:\n",
    "            stats = analysis[f\"agent_{variant}\"]\n",
    "            print(f\"\\nüî§ Agent {variant}:\")\n",
    "            print(f\"   Total Requests: {stats['total_requests']}\")\n",
    "            print(f\"   Success Rate: {100 - stats['error_rate']:.1f}%\")\n",
    "            print(f\"   Avg Response Time: {stats['avg_response_time']:.2f}s\")\n",
    "            print(f\"   Avg Response Length: {stats['avg_response_length']:.0f} chars\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Faster Agent: {analysis['faster_agent']}\")\n",
    "\n",
    "# Create two agents with different configurations\n",
    "agent_a = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"You are a concise assistant. Keep responses brief.\"\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=\"You are a detailed assistant. Provide comprehensive answers.\"\n",
    ")\n",
    "\n",
    "# Run A/B test\n",
    "ab_tester = ABTestFramework()\n",
    "ab_test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain cloud computing\",\n",
    "    \"What is machine learning?\"\n",
    "]\n",
    "\n",
    "ab_tester.run_test(agent_a, agent_b, ab_test_prompts)\n",
    "ab_tester.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Step 7: Setting Up Alerts and Monitoring\n",
    "\n",
    "### Proactive Monitoring\n",
    "Let's implement alerting for critical metrics and issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertingSystem:\n",
    "    \"\"\"Alerting system for agent monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alerts = []\n",
    "        self.thresholds = {\n",
    "            \"response_time\": 5.0,  # seconds\n",
    "            \"error_rate\": 10.0,    # percentage\n",
    "            \"token_usage\": 1000    # tokens per request\n",
    "        }\n",
    "    \n",
    "    def check_response_time(self, response_time: float, request_id: str):\n",
    "        \"\"\"Check if response time exceeds threshold\"\"\"\n",
    "        if response_time > self.thresholds[\"response_time\"]:\n",
    "            alert = {\n",
    "                \"type\": \"HIGH_RESPONSE_TIME\",\n",
    "                \"severity\": \"WARNING\",\n",
    "                \"message\": f\"Response time {response_time:.2f}s exceeds threshold {self.thresholds['response_time']}s\",\n",
    "                \"request_id\": request_id,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            self.alerts.append(alert)\n",
    "            self._send_alert(alert)\n",
    "    \n",
    "    def check_error_rate(self, error_rate: float):\n",
    "        \"\"\"Check if error rate exceeds threshold\"\"\"\n",
    "        if error_rate > self.thresholds[\"error_rate\"]:\n",
    "            alert = {\n",
    "                \"type\": \"HIGH_ERROR_RATE\",\n",
    "                \"severity\": \"CRITICAL\",\n",
    "                \"message\": f\"Error rate {error_rate:.1f}% exceeds threshold {self.thresholds['error_rate']}%\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            self.alerts.append(alert)\n",
    "            self._send_alert(alert)\n",
    "    \n",
    "    def _send_alert(self, alert: Dict[str, Any]):\n",
    "        \"\"\"Send alert (in production, this would send to monitoring system)\"\"\"\n",
    "        print(f\"\\nüö® ALERT [{alert['severity']}]: {alert['message']}\")\n",
    "    \n",
    "    def get_alert_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of alerts\"\"\"\n",
    "        summary = {\n",
    "            \"total_alerts\": len(self.alerts),\n",
    "            \"by_type\": {},\n",
    "            \"by_severity\": {}\n",
    "        }\n",
    "        \n",
    "        for alert in self.alerts:\n",
    "            alert_type = alert[\"type\"]\n",
    "            severity = alert[\"severity\"]\n",
    "            \n",
    "            summary[\"by_type\"][alert_type] = summary[\"by_type\"].get(alert_type, 0) + 1\n",
    "            summary[\"by_severity\"][severity] = summary[\"by_severity\"].get(severity, 0) + 1\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create monitored agent with alerting\n",
    "alerting_system = AlertingSystem()\n",
    "\n",
    "class MonitoredAgent:\n",
    "    \"\"\"Agent with monitoring and alerting\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent, tracker: PerformanceTracker, alerting: AlertingSystem):\n",
    "        self.agent = agent\n",
    "        self.tracker = tracker\n",
    "        self.alerting = alerting\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "    \n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        self.request_count += 1\n",
    "        request_id = str(uuid.uuid4())\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.agent(prompt)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Check alerts\n",
    "            self.alerting.check_response_time(duration, request_id)\n",
    "            \n",
    "            # Track metrics\n",
    "            self.tracker.record_request(prompt, str(response), duration)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            error_rate = (self.error_count / self.request_count) * 100\n",
    "            self.alerting.check_error_rate(error_rate)\n",
    "            raise\n",
    "\n",
    "# Create monitored agent\n",
    "monitored_agent = MonitoredAgent(base_agent, performance_tracker, alerting_system)\n",
    "\n",
    "# Test with slow response simulation\n",
    "print(\"üö® Testing Alert System...\")\n",
    "print(\"Testing normal response...\")\n",
    "response = monitored_agent(\"What is AI?\")\n",
    "\n",
    "print(\"\\nTesting slow response (simulated)...\")\n",
    "# In real scenario, this would be a naturally slow response\n",
    "time.sleep(6)  # Simulate slow response\n",
    "alerting_system.check_response_time(6.0, \"test-123\")\n",
    "\n",
    "# Show alert summary\n",
    "summary = alerting_system.get_alert_summary()\n",
    "print(f\"\\nüìä Alert Summary:\")\n",
    "print(f\"   Total Alerts: {summary['total_alerts']}\")\n",
    "print(f\"   By Type: {summary['by_type']}\")\n",
    "print(f\"   By Severity: {summary['by_severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 8: Creating Custom Dashboards\n",
    "\n",
    "### Visualizing Agent Performance\n",
    "Let's create a comprehensive dashboard for monitoring our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservabilityDashboard:\n",
    "    \"\"\"Comprehensive observability dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker: PerformanceTracker, alerting: AlertingSystem):\n",
    "        self.tracker = tracker\n",
    "        self.alerting = alerting\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display full dashboard\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéØ AGENT OBSERVABILITY DASHBOARD\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Performance metrics\n",
    "        stats = self.tracker.get_statistics()\n",
    "        \n",
    "        print(\"\\nüìä PERFORMANCE METRICS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Requests: {stats.get('total_requests', 0)}\")\n",
    "        print(f\"Success Rate: {stats.get('success_rate', 0):.1f}%\")\n",
    "        print(f\"Uptime: {stats.get('uptime_seconds', 0):.0f} seconds\")\n",
    "        \n",
    "        print(\"\\n‚è±Ô∏è  RESPONSE TIMES\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Average: {stats.get('avg_response_time', 0):.2f}s\")\n",
    "        print(f\"P50: {stats.get('p50_response_time', 0):.2f}s\")\n",
    "        print(f\"P95: {stats.get('p95_response_time', 0):.2f}s\")\n",
    "        print(f\"P99: {stats.get('p99_response_time', 0):.2f}s\")\n",
    "        \n",
    "        # Alert summary\n",
    "        alert_summary = self.alerting.get_alert_summary()\n",
    "        \n",
    "        print(\"\\nüö® ALERTS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Alerts: {alert_summary['total_alerts']}\")\n",
    "        if alert_summary['by_severity']:\n",
    "            for severity, count in alert_summary['by_severity'].items():\n",
    "                print(f\"{severity}: {count}\")\n",
    "        \n",
    "        # Recent activity\n",
    "        print(\"\\nüìù RECENT ACTIVITY\")\n",
    "        print(\"-\" * 40)\n",
    "        recent_requests = self.tracker.metrics[\"requests\"][-5:]\n",
    "        for i, req in enumerate(recent_requests, 1):\n",
    "            print(f\"{i}. Duration: {req['duration']:.2f}s, \"\n",
    "                  f\"Response: {req['response_length']} chars\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Create and display dashboard\n",
    "dashboard = ObservabilityDashboard(performance_tracker, alerting_system)\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Production-Ready Observability\n",
    "\n",
    "### Best Practices for Production\n",
    "Let's explore production-ready observability practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ PRODUCTION OBSERVABILITY BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = {\n",
    "    \"üìù Logging\": [\n",
    "        \"Use structured logging (JSON format)\",\n",
    "        \"Include correlation IDs for request tracking\",\n",
    "        \"Log at appropriate levels (INFO, WARN, ERROR)\",\n",
    "        \"Implement log rotation and retention policies\",\n",
    "        \"Send logs to centralized logging system\"\n",
    "    ],\n",
    "    \"üìä Metrics\": [\n",
    "        \"Track RED metrics (Rate, Errors, Duration)\",\n",
    "        \"Monitor resource usage (CPU, memory, tokens)\",\n",
    "        \"Set up SLIs and SLOs\",\n",
    "        \"Use time-series databases (Prometheus, CloudWatch)\",\n",
    "        \"Create meaningful dashboards\"\n",
    "    ],\n",
    "    \"üîç Tracing\": [\n",
    "        \"Implement distributed tracing\",\n",
    "        \"Track request flow through components\",\n",
    "        \"Measure component latencies\",\n",
    "        \"Use OpenTelemetry standards\",\n",
    "        \"Correlate with logs and metrics\"\n",
    "    ],\n",
    "    \"üö® Alerting\": [\n",
    "        \"Define clear alert thresholds\",\n",
    "        \"Implement alert fatigue prevention\",\n",
    "        \"Use severity levels appropriately\",\n",
    "        \"Set up escalation policies\",\n",
    "        \"Include runbooks in alerts\"\n",
    "    ],\n",
    "    \"üß™ Testing\": [\n",
    "        \"Continuous evaluation of models\",\n",
    "        \"A/B testing for improvements\",\n",
    "        \"Load testing and capacity planning\",\n",
    "        \"Chaos engineering for resilience\",\n",
    "        \"Regular disaster recovery drills\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "# Example production configuration\n",
    "print(\"\\n\\nüìã Example Production Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_config = {\n",
    "    \"observability\": {\n",
    "        \"logging\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"format\": \"json\",\n",
    "            \"destination\": \"cloudwatch\",\n",
    "            \"retention_days\": 30\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"provider\": \"prometheus\",\n",
    "            \"scrape_interval\": \"15s\",\n",
    "            \"retention\": \"15d\"\n",
    "        },\n",
    "        \"tracing\": {\n",
    "            \"enabled\": True,\n",
    "            \"sampling_rate\": 0.1,\n",
    "            \"exporter\": \"jaeger\"\n",
    "        },\n",
    "        \"alerting\": {\n",
    "            \"provider\": \"pagerduty\",\n",
    "            \"channels\": [\"email\", \"slack\"],\n",
    "            \"thresholds\": {\n",
    "                \"error_rate\": 5.0,\n",
    "                \"p99_latency\": 10.0,\n",
    "                \"availability\": 99.9\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "In this tutorial, you've mastered:\n",
    "- ‚úÖ Implementing structured logging for agents\n",
    "- ‚úÖ Tracking comprehensive performance metrics\n",
    "- ‚úÖ Building evaluation frameworks\n",
    "- ‚úÖ Creating A/B testing systems\n",
    "- ‚úÖ Setting up alerting and monitoring\n",
    "- ‚úÖ Building custom dashboards\n",
    "- ‚úÖ Production-ready observability practices\n",
    "\n",
    "### üìä The Power of Observability\n",
    "\n",
    "You now have the tools to:\n",
    "- **Debug issues** quickly with detailed logs\n",
    "- **Optimize performance** using real metrics\n",
    "- **Ensure quality** through continuous evaluation\n",
    "- **Make data-driven decisions** with A/B testing\n",
    "- **Maintain reliability** with proactive monitoring\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **Measure Everything**: You can't improve what you don't measure\n",
    "2. **Log Smartly**: Structured logs enable powerful analysis\n",
    "3. **Alert Wisely**: Focus on actionable alerts\n",
    "4. **Evaluate Continuously**: Quality is an ongoing process\n",
    "5. **Visualize Clearly**: Good dashboards drive good decisions\n",
    "\n",
    "### üîÆ Advanced Techniques\n",
    "\n",
    "Consider exploring:\n",
    "- **ML Ops Platforms**: MLflow, Weights & Biases\n",
    "- **APM Solutions**: DataDog, New Relic, AppDynamics\n",
    "- **Open Source Stack**: Prometheus + Grafana + Jaeger\n",
    "- **Cloud Native**: AWS CloudWatch, Azure Monitor\n",
    "- **AI-Specific**: LangSmith, Helicone, Portkey\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Strands Documentation](https://strandsagents.com/0.1.x/)\n",
    "- [OpenTelemetry](https://opentelemetry.io/)\n",
    "- [Prometheus Best Practices](https://prometheus.io/docs/practices/)\n",
    "- [SRE Book](https://sre.google/sre-book/table-of-contents/)\n",
    "\n",
    "### üåü Next Steps\n",
    "\n",
    "You're ready to:\n",
    "1. Build production-grade observable AI systems\n",
    "2. Implement comprehensive monitoring strategies\n",
    "3. Create data-driven optimization workflows\n",
    "4. Ensure reliability at scale\n",
    "5. Lead observability initiatives\n",
    "\n",
    "### üöÄ Final Thoughts\n",
    "\n",
    "Observability transforms AI agents from black boxes into transparent, measurable, and improvable systems. With these tools and practices, you can build AI applications that are not just powerful, but also reliable, efficient, and trustworthy.\n",
    "\n",
    "Remember: Great AI systems are built on great observability!\n",
    "\n",
    "Happy monitoring! üìäü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
